{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MATH&ML-9. Математика ансамблевых методов**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Представьте, что вы выполняете важный проект по машинному обучению. Конечно, вы можете делать всё самостоятельно, но если вы дополнительно узнаете мнение коллег, попросите ментора проверить ваши расчёты, найдёте как можно больше информации по вашей теме, то точно получите наилучший результат, ведь вы учтёте не только свои знания и выводы, но и информацию от других компетентных людей.\n",
    "\n",
    "Составление прогнозов в машинном обучении может следовать такой же логике: один алгоритм часто даёт далёкий от желаемой точности прогноз, ведь у каждого метода есть свои ограничения, и в целом создание модели, которая строит очень близкие к реальности предсказания, — достаточно сложная задача. Однако если мы обучим на наших данных несколько моделей и обобщим результаты определённым образом, то сможем получить куда более точный результат.\n",
    "\n",
    "Такой алгоритм решения задач машинного обучения называется **ансамблем моделей**.\n",
    "\n",
    "> **Ансамбль моделей** — это метод, в котором несколько алгоритмов (или вариации одного и того же) обучаются на одних данных, а итоговый прогноз строится на основе всех полученных от моделей прогнозов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c284641ce53a2d60f7bfedb59aa830a3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md9_1_1.png)\n",
    "\n",
    "Вы уже встречались с ансамблями, однако последние модули существенно обогатили ваши знания по математике, и теперь вы сможете изучить математическую основу и тонкости данных методов: особенности настройки параметров, различные библиотеки, нюансы применения тех или иных ансамблей. Это сделает вас более компетентными специалистами, глубоко понимающими суть применяемых методов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.7\n",
    "\n",
    "Найдите минимум функции \\(x^{2}+x y-y+y^{3}\\).\n",
    "\n",
    "В качестве ответа введите координату по оси ординат. Ответ округлите до двух знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точки экстремумов (x, y):  [(-1/3, 2/3), (1/4, -1/2)]\n",
      "Значение функции в первой точке:  -13/27\n",
      "Значение функции во второй точке:  5/16\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "f, x, y = symbols('f x y')\n",
    "\n",
    "f = x**2 + x*y - y + y**3\n",
    "\n",
    "diff_x  = f.diff(x)\n",
    "\n",
    "diff_y  = f.diff(y)\n",
    "\n",
    "extr = solve([diff_x, diff_y], [x, y])\n",
    "\n",
    "print('Точки экстремумов (x, y): ', extr)\n",
    "\n",
    "print('Значение функции в первой точке: ', f.subs([(x, extr[0][0]), (y, extr[0][1])]))\n",
    "\n",
    "print('Значение функции во второй точке: ', f.subs([(x, extr[1][0]), (y, extr[1][1])]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐ Вы справились с тестированием — поздравляем! Теперь вы точно можете быть уверены, что ваша подготовка достаточна для освоения данного модуля. Прежде чем приступить, давайте обозначим основные цели на следующие семь юнитов:\n",
    "\n",
    "1. **Изучить основную терминологию, связанную с ансамблями моделей.**\n",
    "Мы повторим уже знакомые вам основные понятия, которые используются в ансамблях, и познакомимся с рядом новых.\n",
    "2. **Подробно разобрать реализацию разных видов ансамблей с математической и смысловой точек зрения.**\n",
    "Мы намного подробнее, чем раньше, разберём все алгоритмы и изучим их математическую составляющую, чтобы лучше понимать принцип их работы, уметь более тонко их настраивать и за счёт этого добиваться наилучшей эффективности.\n",
    "3. **Научиться решать задачи регрессии и классификации с использованием ансамблей моделей.**\n",
    "Конечно же, полученные знания мы будем использовать для решения настоящих практических задач.\n",
    "4. **Научиться настраивать параметры моделей для повышения прогностической точности.**\n",
    "Мы рассмотрим параметры алгоритмов, которые можно регулировать, и разберёмся, как менять каждый из них для повышения точности предсказания.\n",
    "\n",
    "Важно обозначить, что мы будем рассматривать три вида построения ансамблей: бэггинг, бустинг и стекинг. Для каждого из них мы изучим популярные вариации, программную реализацию и, конечно же, сравним их эффективность при решении задач.\n",
    "\n",
    "В результате освоения этого модуля вы сможете применять ансамблевые методы для решения задач машинного обучения. Вы не просто будете знать плюсы и минусы ансамблевых методов и то, какие из них уместны в том или ином случае, но также будете понимать их суть и математическую составляющую.\n",
    "\n",
    "Итак, вперёд к ансамблям! →"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ансамбли моделей. Бутстреппинг. Бэггинг"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В этом юните вы углубите свои знания ансамблей алгоритмов. Вы познакомитесь с тем, как формируются бутстреп-выборки, а также досконально изучите принцип **бэггинга** — самого простого варианта ансамблей.\n",
    "\n",
    "В основе бэггинга лежит статистический метод, который называется **бутстрепом (bootstrap)**. Идея бутстрепа заключается в генерации выборок размера n из исходного датасета размера N путём случайного выбора элементов с повторениями в каждом из наблюдений.\n",
    "\n",
    "**Рассмотрим идею бутстрепа на элементарном примере.**\n",
    "\n",
    "Пусть у нас есть выборка из 12 клиентов компании: у каждого из них есть свой ID (от 1 до 12) и какие-то характеристики. Мы можем создавать из данной выборки множество различных новых выборок клиентов с новым количеством человек (в данном случае представлены выборки из пяти человек). При этом информацию про одного и того же клиента можно использовать повторно.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/95929bfe488e7dd592014067919c91ad/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md9_2_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это намного проще, чем находить новые выборки. По сути, мы собираем данные лишь единожды, а затем на их основе генерируем много выборок для обучения моделей. Это экономит огромные объёмы ресурсов и времени.\n",
    "\n",
    "При некотором приближении можно считать, что получающиеся выборки являются независимыми и репрезентативными — **это важное допущение**.\n",
    "\n",
    "Выборки можно назвать **независимыми**, если результаты испытаний и измерения, осуществляемые для одной выборки, никак не влияют на результаты, получаемые на другой выборке.\n",
    "\n",
    "**Репрезентативность** заключается в соответствии характеристик выборки всей генеральной совокупности.\n",
    "\n",
    "К примеру, если мы хотим исследовать мнение всех женщин России по какому-то вопросу, то все женщины России — это **генеральная совокупность**.\n",
    "\n",
    "**Репрезентативная выборка** — это такая группа женщин, для которой основные характеристики соответствуют характеристикам для генеральной совокупности. Допустим, если среди всех российских женщин 60 % имеют детей, а 40 % — не имеют, то соотношение в выборке должно быть таким же."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бутстреп-выборки часто используются для оценки различных статистических показателей, например разброса или доверительного интервала.\n",
    "\n",
    "Если вычислять статистические оценки на нескольких независимых выборках, то мы можем, например, оценить их математическое ожидание или разброс. Приведём пример того, как это происходит с точки зрения математики.\n",
    "\n",
    "Допустим, у нас есть некоторая выборка $x=(5,1,3,6,4)$, и мы хотим оценить для неё математическое ожидание. Например, это может быть выборка количества товаров, которые приобретали покупатели нашего магазина, и мы хотим найти ожидаемое количество товаров, которое купит случайный клиент.\n",
    "\n",
    "Конечно, мы без проблем можем его вычислить:\n",
    "\n",
    "$$E(x) =E(5,1,3,6,4) = \\frac{1}{5} \\cdot 5 + \\frac{1}{5} \\cdot1 + \\frac{1}{5} \\cdot3 + \\frac{1}{5} \\cdot6 + \\frac{1}{5} \\cdot4 = 1+0.2 +0.6 + 1.2 + 0.8 = 3.8$$\n",
    "\n",
    "Однако это значение лишь на одной выборке, а мы хотели бы вычислить эту статистическую оценку на нескольких выборках и затем проанализировать разброс оценок.\n",
    "\n",
    "Создаём несколько выборок с помощью бутстрепа и на каждой оцениваем математическое ожидание:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/880fd8813d3d070f2434c836167af820/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md9_2_2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили ряд значений:\n",
    "\n",
    "$4.4, 3.8, 4.8, 4.0, 3.4, 4.2, 5.2$\n",
    "\n",
    "Теперь давайте найдём дисперсию для этого ряда. Мы с вами делали это в модуле по теории вероятностей — самое время вспомнить!\n",
    "\n",
    "### Задание 2.2\n",
    "\n",
    "Вычислите дисперсию для этого ряда. Результат округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.317"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statistics import pvariance\n",
    "\n",
    "pvariance([4.4, 3.8, 4.8, 4.0, 3.4, 4.2, 5.2]).__round__(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы понимаем, что если мы будем создавать различные новые выборки и вычислять для них средние значения, то для полученных значений дисперсия будет равна найденному вами выше значению. **Заметьте: мы узнали это, не собирая никаких новых данных.**\n",
    "\n",
    "Формализуем только что проделанные действия математически:\n",
    "\n",
    "1. Генерируем выборки. Необходимо создавать упорядоченные множества элементов, которые мы выбираем с возвратом из некоторого имеющегося у нас множества:\n",
    "\n",
    "$$\\left\\{X_{1}, \\ldots, X_{N}\\right\\}$$\n",
    "\n",
    "2. Повторяем несколько раз процедуру генерации выборки:\n",
    "\n",
    "$$X_{b}^{*}=\\left(X_{b 1}^{*}, \\ldots, X_{b N}^{*}\\right), \\ где \\ 1 \\leqslant b \\leqslant B$$\n",
    "\n",
    "3. Считаем интересующую нас статистику по каждой выборке:\n",
    "\n",
    "$$T_{1}^{*}=T\\left(X_{1}^{*}\\right), \\ldots, T_{B}^{*}=T\\left(X_{B}^{*}\\right)$$\n",
    "\n",
    "4. Получаем бутстрепную оценку для интересующей нас статистики по этой выборке статистик. Например, для дисперсии она будет вычисляться так:\n",
    "Отлично, мы разобрались с тем, что такое метод бутстрепа. Запомните его идею — совсем скоро она пригодится вам для понимания алгоритма бэггинга.\n",
    "\n",
    "$$\\widehat{D}_{\\text {boot }}=\\frac{1}{B} \\sum_{b=1}^{B} T_{b}^{* 2}-\\left(\\frac{1}{B} \\sum_{b=1}^{B} T_{b}^{*}\\right)^{2}$$\n",
    "\n",
    "Отлично, мы разобрались с тем, что такое метод бутстрепа. Запомните его идею — совсем скоро она пригодится вам для понимания алгоритма бэггинга."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIAS И VARIANCE\n",
    "\n",
    "Прежде чем перейти непосредственно к ансамблям моделей, нам необходимо повторить bias-variance decomposition, или, как его называют по-русски, «разложение ошибки на смещение и разброс». Оно очень полезно для анализа ансамблей моделей.\n",
    "\n",
    "> **Смещение** — это разница между математическим ожиданием для прогноза и реальным значением:\n",
    ">\n",
    "> $$Bias[\\hat{f}(x)] = E[\\hat{f}(x)]-y$$\n",
    "\n",
    "Здесь:\n",
    "\n",
    "* $E[\\hat{f}(x)]$ — математическое ожидание для прогноза,\n",
    "* $y$ — реальное значение функции.\n",
    "\n",
    "**Смысл смещения** — способность получить лучшую среди всех возможных моделей, то есть максимально точные прогнозы."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритмы со стабильно маленьким смещением:**\n",
    "* KNN (n = 1);\n",
    "* метод опорных векторов;\n",
    "* деревья решений с большой глубиной.\n",
    "\n",
    "**Алгоритмы с большим смещением:**\n",
    "* линейная регрессия;\n",
    "* логистическая регрессия;\n",
    "* деревья решений с маленькой глубиной.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также значение смещения часто называют **ошибкой смещения** или **ошибкой из-за смещения**.\n",
    "\n",
    "Если у модели большое смещение, это значит, что ошибка будет достаточно велика из-за слишком сильного упрощения модели.\n",
    "\n",
    "> **Разброс** — это величина разницы в результатах обучения модели на разных выборках:\n",
    ">\n",
    "> $$\\operatorname{Var}[\\hat{f}(x)]=\\mathrm{E}\\left[\\left(\\mathrm{E}[\\hat{f}(x)]-\\hat{f}(x)\\right)^{2}\\right]$$\n",
    "\n",
    "**Примечание**. С математической точки зрения разброс модели определяется как математическое ожидание квадрата разницы ожидаемого прогноза и реализованного прогноза модели.\n",
    "\n",
    "Разброс характеризует устойчивость модели к изменениям в обучающей выборке:\n",
    "\n",
    "* Если результат сильно зависит от того, какие объекты присутствуют в выборке, разброс будет большим.\n",
    "* Если алгоритм работает стабильно вне зависимости от особенностей выборки, разброс будет маленьким.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритмы, показывающие маленький разброс:**\n",
    "* линейная регрессия;\n",
    "* логистическая регрессия;\n",
    "* деревья решений с маленькой глубиной.\n",
    "\n",
    "**Алгоритмы, показывающие большой разброс:**\n",
    "* деревья решений с большой глубиной;\n",
    "* KNN;\n",
    "* метод опорных векторов.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим разложение на смещение и разброс для линейной регрессии.\n",
    "\n",
    "Пусть мы хотим предсказать значение y по значениям вектора $x$. Тогда зависимость $y$ от $x$ можно записать следующим образом:\n",
    "\n",
    "$$y=f(x)+\\varepsilon$$\n",
    "\n",
    "В качестве $f(x)$ здесь выступает истинная зависимость ответов $y$ от характеристик объекта $x$ — мы её не знаем и пытаемся предсказать с помощью модели. Предсказания обозначим как $\\hat{f}(x)$. Символом $\\varepsilon$ обозначается случайная ошибка. Предполагается, что её математическое ожидание равно нулю — это просто шум.\n",
    "\n",
    "Тогда давайте выразим ошибку для какого-то значения $х$. Она будет равняться математическому ожиданию для квадрата разницы между реальным и предсказанным значениями. По сути, это просто среднеквадратичная ошибка, записанная в немного иной форме:\n",
    "\n",
    "$$\\operatorname{Err}(x)=E\\left[(y-\\hat{f}(x))^{2}\\right]$$\n",
    "\n",
    "Также мы можем разложить среднеквадратичную ошибку следующим образом:\n",
    "\n",
    "$$\\operatorname{Err}(x)=(E[\\hat{f}(x)]-y)^{2}+E\\left[(\\hat{f}(x)-E[\\hat{f}(x)])^{2}\\right]+\\sigma_{\\varepsilon}^{2}$$\n",
    "\n",
    "$$\\operatorname{Err}(x)=\\operatorname{Bias}^{2}+Variance+Irreducible Error$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доказательство равенства\n",
    "\n",
    "Для начала представим $y$ как сумму значения функции $f$ и ошибки (вместо $f(x)$ будем далее для краткости писать просто $f$):\n",
    "\n",
    "$$\\mathrm{E}\\left[(y-\\hat{f})^{2}\\right]=\\mathrm{E}\\left[(f+\\varepsilon-\\hat{f})^{2}\\right]$$\n",
    "\n",
    "Теперь в выражение, от которого мы ищем математическое ожидание, добавим математическое ожидание предсказанной функции и вычтем его же (это нужно для того, чтобы далее мы смогли выразить необходимые нам величины). Разумеется, если мы прибавляем какую-то величину, а потом её вычитаем, результат остаётся тем же.\n",
    "\n",
    "$$=\\mathrm{E}\\left[(f+\\varepsilon-\\hat{f}+\\mathrm{E}[\\hat{f}]-\\mathrm{E}[\\hat{f}])^{2}\\right]$$\n",
    "\n",
    "Далее раскроем скобки, то есть возведём в квадрат сумму трёх слагаемых:\n",
    "\n",
    "* $f-\\mathrm{E}[\\hat{f}]$,\n",
    "\n",
    "* $\\varepsilon$,\n",
    "\n",
    "* $\\mathrm{E}[\\hat{f}]-\\hat{f}$.\n",
    "\n",
    "**Примечание**. Для разложения пользуемся формулой из алгебры:\n",
    "\n",
    "$$(a+b+c)^{2}=a^{2}+b^{2}+c^{2}+2 a b+2 a c+2 b c$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем:\n",
    "\n",
    "$=\\mathrm{E}\\left[(f-\\mathrm{E}[\\hat{f}])^{2}\\right]+\\mathrm{E}\\left[\\varepsilon^{2}\\right]+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2 \\mathrm{E}[(f-\\mathrm{E}[\\hat{f}]) \\varepsilon]+2 \\mathrm{E}[\\varepsilon(\\mathrm{E}[\\hat{f}]-\\hat{f})]+2 \\mathrm{E}[(\\mathrm{E}[\\hat{f}]-\\hat{f})(f-\\mathrm{E}[\\hat{f}])]$\n",
    "\n",
    "$=(f-\\mathrm{E}[\\hat{f}])^{2}+\\mathrm{E}\\left[\\varepsilon^{2}\\right]+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]+2(f-\\mathrm{E}[\\hat{f}]) \\mathrm{E}[\\varepsilon]+2 \\mathrm{E}[\\varepsilon] \\mathrm{E}[\\mathrm{E}[\\hat{f}]-\\hat{f}]+2 \\mathrm{E}[\\mathrm{E}[\\hat{f}]-\\hat{f}](f-\\mathrm{E}[\\hat{f}])$\n",
    "\n",
    "Сокращаем одинаковые слагаемые:\n",
    "\n",
    "$=(f-\\mathrm{E}[\\hat{f}])^{2}+\\mathrm{E}\\left[\\varepsilon^{2}\\right]+\\mathrm{E}\\left[(\\mathrm{E}[\\hat{f}]-\\hat{f})^{2}\\right]$\n",
    "\n",
    "Видим, что у нас есть дисперсия ошибки и дисперсия предсказания:\n",
    "\n",
    "$=(f-\\mathrm{E}[\\hat{f}])^{2}+\\operatorname{Var}[\\varepsilon]+\\operatorname{Var}[\\hat{f}]$\n",
    "\n",
    "$=\\operatorname{Bias}[\\hat{f}]^{2}+\\sigma^{2}+\\operatorname{Var}[\\hat{f}]$\n",
    "\n",
    "Итак, мы получили, что наша ошибка — это сумма смещения для квадрата прогноза, разброса и неустранимой случайной ошибки. Теперь мы понимаем, из чего состоит ошибка модели. Такое представление помогает нам исследовать с теоретической точки зрения некоторые алгоритмы машинного обучения и часто используется при изучении ансамблей."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что наша ошибка — это сумма смещения для квадрата прогноза, разброса и неустранимой случайной ошибки. Теперь мы понимаем, из чего состоит ошибка модели. Такое представление помогает нам исследовать с теоретической точки зрения некоторые алгоритмы машинного обучения и часто используется при изучении ансамблей.\n",
    "\n",
    "Рассмотрим иллюстрацию того, как сдвиг и разброс влияют на качество предсказания. На рисунке ниже вы видите цель (красный круг), в которую мы хотим попасть.\n",
    "\n",
    "Есть четыре ситуации:\n",
    "\n",
    "![](https://img.genial.ly/5fdc5ca1853b5759f6e69400/5040dd83-7a8e-444e-8fc0-a3445064b968.png)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В моделях машинного обучения принцип тот же, только в качестве центра мишени выступает минимально возможная ошибка.\n",
    "\n",
    "Когда говорят про разложение на bias и variance, то часто упоминают некую **точку баланса**:\n",
    "\n",
    "* Если модель очень простая, с маленьким количеством параметров, то, скорее всего, у неё будет очень большое смещение, но маленький разброс.\n",
    "* Если модель очень сложная, со множеством параметров, у неё будет большой разброс и маленькое смещение.\n",
    "\n",
    "Схематично эти зависимости можно изобразить следующим образом (это схема не для конкретной модели, а лишь иллюстрация тенденций):\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/eccb230bf722e42b7fbeeadddf60daae/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md9_2_4.png)\n",
    "\n",
    "На графике выше по оси абсцисс отложена сложность модели ($Model \\ Complexity$), а по оси ординат — ошибка ($Error$). Также изображены смещение ($Bias^2$), разброс ($Variance$) и ошибка ($Total \\ Error$ — сумма смещения и разброса).\n",
    "\n",
    "Как вы можете видеть, есть некоторая оптимальная точка, в которой разброс и смещение небольшие, а ошибка минимальна. Именно эта точка нас и интересует."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## БЭГГИНГ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перейдём к понятию **бэггинг**.\n",
    "\n",
    "При построении моделей всегда есть вероятность, что при обучении на других данных получились бы другие результаты. Для того чтобы нивелировать такую вероятность, можно использовать бэггинг.\n",
    "\n",
    "Его идея состоит в том, что мы берём несколько независимых моделей и усредняем полученные по ним результаты. Таким образом мы получаем модель, имеющую меньший разброс, так как при её построении мы учли несколько моделей.\n",
    "\n",
    "Как уже было сказано, в реальности получить много независимых выборок слишком сложно, так как найти столько данных обычно невозможно. Поэтому мы используем бутстреп-выборки, о которых говорили в начале юнита.\n",
    "\n",
    "Важно отметить, что при бэггинге размер каждой бутстреп-выборки должен совпадать с размером исходной выборки.\n",
    "\n",
    "Схематично процесс бэггинга можно представить следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/86233c8d821d894e9008415bda25a791/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md9_2_6.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте сформулируем и объясним эту идею математически.\n",
    "\n",
    "Пусть у нас есть некоторая выборка, и мы с помощью бутстрепа генерируем из неё ещё $B$ выборок:\n",
    "\n",
    "$$X_{1}, ..., X_{B}$$\n",
    "\n",
    "После этого мы определяем много базовых алгоритмов (всего $B$ моделей — по числу выборок) и обучаем каждый базовый алгоритм $a_{i}(x)$ на своей выборке. После этого получаем итоговый результат:\n",
    "\n",
    "$$a(x)=\\frac{1}{B} \\sum_{i=1}^{B} a_{i}(x)$$\n",
    "\n",
    "* Если мы рассматриваем задачу классификации, то, по сути, модели «голосуют» за свой класс.\n",
    "\n",
    "* Если мы рассматриваем задачу регрессии, то результат — просто среднее арифметическое прогнозов по всем моделям.\n",
    "\n",
    "Теперь посмотрим, **насколько применение бэггинга поможет нам улучшить качество модели**.\n",
    "\n",
    "Представим, что мы хотим использовать бэггинг для решения задачи регрессии и берём для этого $K$ базовых алгоритмов:\n",
    "\n",
    "$$a_{1}(x), ..., a_{K}(x)$$\n",
    "\n",
    "Конечно, если для каждого объекта известно значение целевой переменной, мы можем вычислить ошибку для каждой модели:\n",
    "\n",
    "$$\\varepsilon_{i}(x)=a_{i}(x)-y(x), i=1, \\ldots, K$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы можем выразить математическое ожидание квадратичной ошибки (т. е., по сути, среднеквадратичную ошибку):\n",
    "\n",
    "$$\\mathbb{E}_{x}\\left[\\left(a_{i}(x)-y(x)\\right)^{2}\\right]=\\mathbb{E}_{x}\\left[\\varepsilon_{i}^{2}(x)\\right]$$\n",
    "\n",
    "Ошибка здесь обозначается как .\n",
    "\n",
    "Тогда, если мы усредним значение ошибки по всем моделям регрессии, то получим следующее:\n",
    "\n",
    "$$\\mathbb{E}_{1}=\\frac{1}{n} \\mathbb{E}_{x} \\sum_{i=1}^{n} \\varepsilon_{i}^{2}(x)$$\n",
    "\n",
    "Будем считать, что математическое ожидание для всех ошибок равно нулю (то есть нет какого-то систематического смещения ошибок, и они случайные) и что все ошибки независимы друг от друга (то есть их коэффициент корреляции равен нулю):\n",
    "\n",
    "$$\\begin{aligned} \\mathbb{E}_{x} \\varepsilon_{i}(x) &=0 \\\\ \\mathbb{E}_{x} \\varepsilon_{i}(x) \\varepsilon_{j}(x) &=0, i \\neq j \\end{aligned}$$\n",
    "\n",
    "Теперь определим регрессионную функцию, которая будет брать ответы от всех обученных нами регрессионных моделей и просто усреднять их:\n",
    "\n",
    "$$f(x)=\\frac{1}{K} \\sum_{i=1}^{K} a_{i}(x)$$\n",
    "\n",
    "Теперь найдём для этой модели среднеквадратичную ошибку. Выразим её через математическое ожидание:\n",
    "\n",
    "$$\\mathbb{E}_{K}=\\mathbb{E}_{x}\\left(\\frac{1}{K} \\sum_{i=1}^{K} a_{i}(x)-y(x)\\right)^{2}$$\n",
    "\n",
    "Упростим часть внутри скобок, воспользовавшись введённым ранее утверждением о том, что $\\mathbb{E}_{x}\\left[\\left(a_{i}(x)-y(x)\\right)^{2}\\right]=\\mathbb{E}_{x}\\left[\\varepsilon_{i}^{2}(x)\\right]$, и получим следующее:\n",
    "\n",
    "$$\\mathbb{E}_{x}\\left[\\left(a_{i}(x)-y(x)\\right)^{2}\\right]=\\mathbb{E}_{x}\\left[\\varepsilon_{i}^{2}(x)\\right]$$\n",
    "\n",
    "$$=\\mathbb{E}_{x}\\left(\\frac{1}{K} \\sum_{i=1}^{K} \\varepsilon_{i}\\right)^{2}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После этого вынесем за скобку $\\frac{1}{K}$, а также возведём внутреннюю часть скобки в квадрат:\n",
    "\n",
    "$$=\\frac{1}{K^{2}} \\mathbb{E}_{x}\\left(\\sum_{i=1}^{K} \\varepsilon_{i}^{2}(x)+\\sum_{i \\neq j} \\varepsilon_{i}(x) \\varepsilon_{j}(x)\\right)$$\n",
    "\n",
    "Ранее мы уже выяснили, что $\\sum_{i \\neq j} \\varepsilon_{i}(x) \\varepsilon_{j}(x)$ равняется нулю, так как все ошибки независимы. Сокращаем выражение и получаем в итоге:\n",
    "\n",
    "$$\\mathbb{E}_{K}=\\frac{1}{K}\\mathbb{E}_{1}$$\n",
    "\n",
    "Получается, что путём усреднения предсказаний линейных регрессий мы смогли уменьшить среднеквадратичную ошибку в $K$ раз.\n",
    "\n",
    "Однако тут важно отметить, что при решении прикладных задач эффект будет не таким выраженным, так как здесь мы использовали предположение о полной независимости ошибок, а в реальной жизни такое случается редко.\n",
    "\n",
    "Также, чтобы иметь полное представление о характеристиках рассматриваемого алгоритма, давайте вспомним про разложение ошибки на смещение и разброс.\n",
    "\n",
    "Доказано, что бэггинг не ухудшает показатель смещения модели, то есть смещение у ансамбля ровно такое же, как и у одного базового алгоритма.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Доказательство\n",
    "\n",
    "Выберем из нашей выборки $X$ бутстрепом $K$ раз выборку длиной $N$. Получим выборки $X_{1}, X_{2}, ..., X_{K}$. Обучим базовые модели $a(x)$ на данных подвыборках. Первую модель $a_{1}(x) = a(x,X_{1})$  обучим на первой выборке бутстрепа, вторую $a_{2}(x) = a(x,X_{2})$ — на второй и так далее. Выполнив данную процедуру $n$ раз, мы получим предсказание как усреднение по всем обученным на подвыборках моделях.\n",
    "\n",
    "$$f(x,X) = \\frac{1}{K} \\sum_{i=1}^{K} (f_i (x))$$\n",
    "\n",
    "Теперь рассмотрим изменение смещения (bias) и разброса (variance) ансамблирования по отношению к базовым моделям.\n",
    "\n",
    "> **Смещение (bias)** есть не что иное, как математическое ожидание разности между истинными ответами y и предсказаниями ансамбля: \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/0010efcbd8914782f96e32442230250f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-51.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c742c46100695e837ba46566856184cb/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-52.png)\n",
    "\n",
    "**Вывод**: смещение ансамбля равно смещению базовой модели ансамбля!\n",
    "\n",
    "> Разброс (variance, обозначим далее как ) — это дисперсия ответов алгоритма:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3d239a00c5d9d71aec09651f109a150f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-53.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c704097ba958bf98bb20e2aa7bbac971/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-54.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bc1b9ab29110bd4e40c3d2b1f947fbf1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-55.png)\n",
    "\n",
    "При условии некоррелированности базовых моделей, которая достигается за счёт обучения на бутстрепе, последнее слагаемое равно нулю. Итого:\n",
    "\n",
    "$$var(f(x,X)) = \\frac{1}{K^2} \\sum_{i=1}^{K} E \\left [(a_i (x,X)- E \\left [a_i (x,X) \\right ] )\\right ]^2 = \\frac{1}{K^2} \\sum_{i=1}^{K} var(a(x, X_i))$$\n",
    "\n",
    "Тогда, зная, что модели не коррелированы, получаем:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7c8d37f4978d49117ffa159ebaf2c527/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-57.png)\n",
    "\n",
    "**Вывод**: разброс ансамбля уменьшается в $K$ раз по сравнению с разбросом базовой модели!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем случае разброс бэггинга будет выражаться следующим образом:\n",
    "\n",
    "$$\\frac{1}{K}\\left(\\operatorname{Var} a_{n}(x)\\right)+\\operatorname{Cov}\\left(a_{n}(x), a_{m}(x)\\right)$$\n",
    "\n",
    "В данном выражении через $a_{n}(x)$ обозначен один из базовых алгоритмов, а за $a_{m}(x)$ — другой базовый алгоритм.\n",
    "\n",
    "Из этого следует, что если модели (в данной формуле — базовые модели $a_{n}(x)$ и $a_{m}(x)$) независимы, то разброс для ансамбля типа бэггинг будет в $K$ раз меньше, чем разброс у отдельной модели.\n",
    "\n",
    "**Резюмируем:**\n",
    "\n",
    "* Бэггинг даёт уменьшение ошибки в $K$ раз по сравнению с одиночной моделью.\n",
    "* Бэггинг не уменьшает смещение по сравнению с одиночной моделью.\n",
    "* Бэггинг уменьшает разброс в $K$ раз по сравнению с одиночной моделью.\n",
    "\n",
    "Важно отметить, что эти утверждения выведены и доказаны теоретически и будут выполняться на практике только в том случае, если между ошибками нулевая корреляция.\n",
    "\n",
    "Как видим, бэггинг — очень эффективный и полезный алгоритм, так что есть смысл попрактиковаться с ним."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.7\n",
    "\n",
    "Объёмная и содержательная практика у нас ещё впереди, но в качестве разминки давайте поработаем с уже известным вам датасетом о вине, который можно скачать [здесь](https://lms.skillfactory.ru/assets/courseware/v1/805b5c231251e174abb4fdbbd391adc3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/wineQualityReds.zip).\n",
    "\n",
    "Ранее вы обучали на данных только один алгоритм, а теперь мы попробуем сравнить несколько.\n",
    "\n",
    "* Подготовьте данные к классификации. Условно разделите вино на хорошее и плохое. Хорошим вином будем называть то, параметр quality которого — 6 и более.\n",
    "* Сравните несколько методов классификации: логистическую регрессию, дерево решений и бэггинг. Это позволит вам увидеть, как меняется качество в зависимости от выбора того или иного алгоритма.\n",
    "* Разделите выборку на обучающую и тестовую в соотношении 70/30, в качестве значения параметра random_state возьмите число 42.\n",
    "* Для начала обучите два классификатора: логистическую регрессию (с параметрами по умолчанию) и дерево решений (random_state = 42, максимальная глубина — 10).\n",
    "\n",
    "1. Введите значение F1-score для классификатора, который показал наилучшее значение. Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>fixed.acidity</th>\n",
       "      <th>volatile.acidity</th>\n",
       "      <th>citric.acid</th>\n",
       "      <th>residual.sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free.sulfur.dioxide</th>\n",
       "      <th>total.sulfur.dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  fixed.acidity  volatile.acidity  citric.acid  residual.sugar  \\\n",
       "0           1            7.4              0.70         0.00             1.9   \n",
       "1           2            7.8              0.88         0.00             2.6   \n",
       "2           3            7.8              0.76         0.04             2.3   \n",
       "3           4           11.2              0.28         0.56             1.9   \n",
       "4           5            7.4              0.70         0.00             1.9   \n",
       "\n",
       "   chlorides  free.sulfur.dioxide  total.sulfur.dioxide  density    pH  \\\n",
       "0      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "1      0.098                 25.0                  67.0   0.9968  3.20   \n",
       "2      0.092                 15.0                  54.0   0.9970  3.26   \n",
       "3      0.075                 17.0                  60.0   0.9980  3.16   \n",
       "4      0.076                 11.0                  34.0   0.9978  3.51   \n",
       "\n",
       "   sulphates  alcohol  quality  \n",
       "0       0.56      9.4        0  \n",
       "1       0.68      9.8        0  \n",
       "2       0.65      9.8        0  \n",
       "3       0.58      9.8        1  \n",
       "4       0.56      9.4        0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "data = pd.read_csv('data/wineQualityReds.zip')\n",
    "\n",
    "data['quality'] = data['quality'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[data.columns[:-1]]\n",
    "y = data[data.columns[-1]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression F1 score: 0.739\n",
      "DecisionTreeClassifier F1 score: 0.760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Home\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "lr = linear_model.LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "print('LogisticRegression F1 score: {:.3f}'.format(metrics.f1_score(y_test, y_pred)))\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=10\n",
    ")\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "print('DecisionTreeClassifier F1 score: {:.3f}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.8\n",
    "\n",
    "Обучите модель с использованием бэггинга (класс BaggingClassifier с random_state=42).\n",
    "\n",
    "Возьмите из предыдущего задания алгоритм, показавший наилучшее качество, и укажите для него новое количество моделей — 1500. Вычислите новое значение F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaggingClassifier F1 score: 0.824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(\n",
    "    base_estimator=dt,\n",
    "    n_estimators=1500,\n",
    "    random_state=42    \n",
    ")\n",
    "\n",
    "bc.fit(X_train, y_train)\n",
    "y_pred = bc.predict(X_test)\n",
    "print('BaggingClassifier F1 score: {:.3f}'.format(metrics.f1_score(y_test, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Случайный лес"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950b5653ccfc34417735dd321d006fd482b31f7611416c3d8236dc5b17587d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
