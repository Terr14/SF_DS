{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MATH&ML-2. Линейная алгебра в контексте линейных методов. Часть II**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Рады приветствовать вас во втором модуле, посвящённом линейной алгебре!\n",
    "\n",
    "В предыдущем модуле мы познакомились с базовыми понятиями линейной алгебры. По сути, мы изучили основы матричного языка и теперь можем на нем говорить. \n",
    "\n",
    "В этом модуле мы перейдём к применению линейной алгебры в машинном обучении и рассмотрим **алгоритмы анализа данных** с разных сторон. В основном мы, конечно, будем говорить о модели линейной регрессии и её модификациях.\n",
    "\n",
    "Мы будем использовать весь математический аппарат, который изучили в прошлом модуле. Темы предстоят очень интересные, но в то же время непростые. Для того чтобы усвоить их, необходимо владеть всеми навыками, приобретёнными в предыдущем модуле — от базовых операций над векторами до знания принципов решения СЛАУ. Давайте проверим, насколько вы готовы ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.1\n",
    "\n",
    "Найдите скалярное произведение векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (-1, 2, \\ -7, 9)^T$\n",
    "\n",
    "$\\vec{v_{2}}  = (2, 8, 2, \\ -1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([-1, 2, -7, 9])@np.array([2, 8, 2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [3, 5]])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 1], [1, 2]])@np.array([[1, 1], [1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.4\n",
    "\n",
    "Задана матрица\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c1579bdd2592b86d75c9413c69d0627d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_1_1.png)\n",
    "\n",
    "Найдите матрицу Грама $(A^{T}A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A^T)*A: \n",
      " [[2 3]\n",
      " [3 5]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 1], [1, 2]])\n",
    "print('(A^T)*A: \\n', A.T@A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.6\n",
    "\n",
    "Вычислите обратную матрицу $A^{-1}$, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e9a41d606d7dcf555779b45d871b27ce/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_1_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7., -2.],\n",
       "       [-3.,  1.]])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 7]])\n",
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.7\n",
    "\n",
    "Найдите ранг матрицы системы, составленной из векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (2, 10, \\ -2)^T$\n",
    "\n",
    "$\\vec{v_{2}}  = (3, 2, \\ -2)^T$\n",
    "\n",
    "$\\vec{v_{3}}  = (8, 14, \\ -6)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([2, 10, -2])\n",
    "v2 = np.array([3, 2, -2])\n",
    "v3 = np.array([8, 14, -6])\n",
    "\n",
    "A = np.array([v1, v2, v3]).T\n",
    "np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В первой части** мы будем говорить о **классической модели линейной регрессии**. Для этого мы вернёмся к неоднородным системам линейных алгебраических уравнений, которые мы затронули в прошлом модуле, и посмотрим, как они связаны с **методом наименьших квадратов** (МНК, или OLS, Ordinary Least Squares). Затем мы с математической точки зрения посмотрим на проблемы, которые возникают при его использовании, например мультиколлинеарность или чересчур большое количество факторов.\n",
    "\n",
    "**Во второй части** мы перейдём к модификациям модели линейной регрессии и посмотрим, как линейная алгебра работает в **полиномиальной регрессии**, а также поговорим о том, как работают методы регуляризации на математическом уровне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце каждого из блоков нас ожидает небольшая **практическая задача** на применение регрессионных моделей.\n",
    "\n",
    "Сразу отметим, что **мы не будем рассматривать следующие вопросы**:\n",
    "\n",
    "* Вероятностные предпосылки использования модели линейной регрессии, необходимые для её валидности (данные предпосылки регламентирует теорема Маркова-Гаусса).\n",
    "* Оценки качества полученной регрессионной модели и оценки статистической значимости её коэффициентов.\n",
    "* Статистические методы предварительной обработки данных.\n",
    "\n",
    "Для изучения этих вопросов понадобятся знания в области теории вероятности и статистики. Мы обязательно обсудим их в следующих модулях. А пока нас интересуют подробности построения вычислительных алгоритмов с точки зрения линейной алгебры.\n",
    "\n",
    "* Также мы не будем затрагивать тему корректной валидации моделей и разделять выборку на тренировочную/тестовую/валидационную с целью экономии времени и сил. Надеемся, что вы помните, как правильно оценивать качество получаемых моделей, сможете проделать это самостоятельно.\n",
    "\n",
    "Таким образом, мы будем рассматривать **только математическую составляющую** линейных (и полиномиальных) моделей регрессии, не концентрируясь на сторонних аспектах машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ЦЕЛИ ДАННОГО МОДУЛЯ:\n",
    "\n",
    "* познакомиться с неоднородными СЛАУ и случаями их решений;\n",
    "* изучить математическую формализацию метода наименьших квадратов;\n",
    "* научиться строить модель линейной регрессии с помощью МНК;\n",
    "* понять, какие проблемы возникают в МНК с математической точки зрения;\n",
    "* познакомиться с математической формализацией полиномиальной регрессии;\n",
    "* рассмотреть методы регуляризации и принципы их работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Неоднородные СЛАУ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы начнём с **алгоритма классической линейной регрессии по методу наименьших квадратов** (OLS, Ordinary Least Squares). Данный алгоритм является базовым, но, тем не менее, весьма непрост для восприятия, поэтому данная сложносочинённая задача будет разделена на две части:\n",
    "\n",
    "* В этом юните мы обсудим случаи и алгоритм решения неоднородных СЛАУ.\n",
    "* В следующем юните подведём под эту задачу контекст задачи регрессии.\n",
    "\n",
    "Для начала давайте вспомним, что такое **неоднородные СЛАУ**.\n",
    "\n",
    "**Примечание**. Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений** (**СЛАУ**) и в общем случае записывается как:\n",
    "\n",
    "$$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$$\n",
    "\n",
    "где\n",
    "\n",
    "* $n$ — количество уравнений;\n",
    "* $m$ — количество переменных;\n",
    "* $x_i$ — неизвестные переменные системы;\n",
    "* $a_{ij}$ — коэффициенты системы;\n",
    "* $b_i$ — свободные члены системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 ($b_1=b_2=⋯=b_n=0$):\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$$\n",
    "\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$$\n",
    "\n",
    "Пример неоднородной СЛАУ:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Вспомним, что СЛАУ можно записать в матричном виде:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/35808bc9ca767f3c996106d8887c1108/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_3.png)\n",
    "\n",
    "где $A$ — матрица системы, $\\omega$ — вектор неизвестных коэффициентов, а $b$ — вектор свободных членов. \n",
    "\n",
    "Давайте введём новое для нас определение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Р**асширенной матрицей системы $A|b$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов:\n",
    ">\n",
    "> $$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$$\n",
    "\n",
    "Расширенная матрица системы — это обычная матрица. Черта, отделяющая коэффициенты $a_{ij}$ от свободных членов $b_i$ — чисто символическая. \n",
    "\n",
    "Над расширенной матрицей неоднородной СЛАУ можно производить те же самые действия, что и над обычной, а именно:\n",
    "\n",
    "* складывать/вычитать между собой строки/столбцы матрицы;\n",
    "* умножать строки/столбцы на константу;\n",
    "* менять строки/столбцы местами.\n",
    "\n",
    "Приведём пример расширенной матрицы системы. Пусть исходная система будет следующей:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f928ba511cc7a2ac62881707fdfeea66/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_6.png)\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f534ccbdd7c85b138e033bbb178a520b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "еперь, когда мы вспомнили все определения и познакомились с термином расширенной матрицы, мы можем переходить к решению неоднородных СЛАУ.\n",
    "\n",
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "* «Идеальная пара»\n",
    "\n",
    "    Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "* «В активном поиске»\n",
    "\n",
    "    Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "* «Всё сложно»\n",
    "\n",
    "    Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений.\n",
    "\n",
    "**Примечание**. В данной классификации неоднородных СЛАУ допущено упрощение в терминологии. На самом деле неопределённые системы — это те, в которых независимых уравнений меньше, чем неизвестных. Они могут иметь бесконечно много решений (быть совместными) или ни одного решения (быть несовместными, если уравнения противоречат друг другу).\n",
    "\n",
    "На практике, например в обучении регрессий, этот случай практически не встречается.\n",
    "\n",
    "Что касается переопределённых систем, то в них, помимо несовместности (отсутствия решений), количество независимых уравнений превышает количество неизвестных — это тот самый случай, что мы видим в регрессионном анализе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы рассмотрим каждый из случаев на примере.\n",
    "\n",
    "## СЛУЧАЙ «ИДЕАЛЬНАЯ ПАРА»\n",
    "\n",
    "> Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**.\n",
    "\n",
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — **теорема Кронекера — Капелли** (также её называют **критерием совместности системы**).\n",
    "\n",
    "> **Теорема Кронекера — Капелли:**\n",
    "> \n",
    "> Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы $A$ **равен** рангу расширенной матрицы системы $(A| \\vec{b})$ и равен количеству независимых переменных $m$:\n",
    "> \n",
    "> $$rk(A) = rk(\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$$\n",
    ">\n",
    "> Причём решение системы будет равно:\n",
    "> \n",
    "> $$\\vec{w} = A^{-1} \\vec{b}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Здесь значок $\\exists !$ переводится как «существует и причём единственное».\n",
    "\n",
    "Сложно и непонятно? Давайте разберёмся, как работает эта теорема, на примерах ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Дана СЛАУ:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/cadd1684cd0b6d152ce5a1a784fc8973/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_8.png)\n",
    "\n",
    "*где $w_1$ и $w_2$ — неизвестные переменные.*\n",
    "\n",
    "При решении системы «в лоб» получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/37c7d45c0a88651cd57dd05b085adb78/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_9.png)\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5191ad698399cde0da309f8deea15cb6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_10.png)\n",
    "\n",
    "На языке линейной алгебры это означает что вектор $(1,2)^T$ линейно выражается через векторы коэффициентов системы $(1,1)^T$ и $(1,2)^T$.\n",
    "\n",
    "В матричном виде система запишется, как:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a1e86f40406ca7904279ac19a60844b0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование уравнений будем таким же, как и при преобразовании расширенной матрицы системы $(A|\\vec{b})$, вычитая сначала первую строку из второй, а затем — результат из первой, получим то же решение, что и решение «в лоб».\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d0df10f19deeb809ba7cbf27264f85a9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_12.png)\n",
    "\n",
    "Других решений у системы нет. \n",
    "\n",
    "Посмотрим на ранги матрицы $A$ и расширенной матрицы $(A|\\vec{b})$ (количество ступеней в ступенчатых матрицах):\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/da90d08d05f922039ad20d58aa5aff45/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_13.png)\n",
    "\n",
    "Они совпадают и равны количеству неизвестных, а это и гарантирует существование и единственность решения. То есть в общем случае, чтобы узнать, сколько решений существует у системы, её необязательно было бы решать — достаточно было бы найти ранги матриц $A$ и $A|b$.\n",
    "\n",
    "Тут возникает вопрос: «Можно ли найти решение одной формулой?»\n",
    "\n",
    "Для удобства перепишем систему без стрелок:\n",
    "\n",
    "$Aw = b$\n",
    "\n",
    "Так как матрица квадратная и невырожденная, у неё обязательно есть обратная матрица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножим на $A^{-1}$ слева обе части уравнения. Стоит напомнить, что произведение матриц не перестановочно, поэтому есть разница, с какой стороны умножать.\n",
    "\n",
    "$$A^{-1} Aw = A^{-1}b$$\n",
    "\n",
    "$$w = A^{-1} b$$\n",
    "\n",
    "\n",
    "**Важно**! Отсюда явно видны **ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "Убедимся в правильности формулы. Найдём произведение матрицы $A^{-1}$ и вектора-столбца $b$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/207d07ac7518f16e548924a9bc2e3382/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_14.png)\n",
    "\n",
    "Меняем местами главную диагональ и меняем знаки у побочной. Как видим, результат совпал с прошлым методом.\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ **равен** рангу расширенной матрицы $(A|b)$ и **равен** количеству переменных ($rk(A)=rk(\\vec{b})=m$), то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет **единственное** решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.4\n",
    "\n",
    "*Решите систему линейных уравнений:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bb410f21af5bc73df165cf79d3623d37/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_23.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x y] =  [-2.  4.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4, 7], [5, 10]])\n",
    "b = np.array([20, 30])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print('[x y] = ', A_inv@b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «В АКТИВНОМ ПОИСКЕ»\n",
    "\n",
    "А что, если система не удовлетворяет теореме Кронекера — Капелли? То есть ранг матрицы системы равен расширенному рангу матрицы, но не равен количеству неизвестных. Неужели тогда система нерешаема?\n",
    "\n",
    "На этот вопрос отвечает первое следствие из теоремы ↓\n",
    "\n",
    "> **Следствие №1 из теоремы Кронекера — Капелли:**\n",
    "> \n",
    "> Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "> \n",
    "> $$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Решим систему уравнений:*\n",
    "\n",
    "$w_1 + w_2 + w_3 = 10$\n",
    "\n",
    "Да, уравнение одно, но формально оно является неоднородной СЛАУ.\n",
    "\n",
    "Итак, мы имеем одно уравнение на три неизвестных, значит две координаты из трёх вектора $w$ мы можем задать как угодно. Например, зададим вторую и третью как $\\alpha$ и $\\beta$. Тогда первая будет равна $10 - \\alpha - \\beta$.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/426d121f84b92a0d96b78bd777ebd44d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_30.png)\n",
    "\n",
    "Вместо переменных $\\alpha$ и $\\beta$ мы можем подставлять любые числа и всегда будем получать равенство. \n",
    "\n",
    "Составим расширенную матрицу:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/81cc95b153a93a58dc3c517470779345/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_31.png)\n",
    "\n",
    "Её ранг, как и ранг $A$, равен 1, что меньше числа неизвестных $m=3$:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) = 1 < 3$\n",
    "\n",
    "Такая ситуация, по следствию из теоремы Кронекера — Капелли, говорит о существовании и не единственности решения, то есть решений бесконечно много.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных ($rk(A) = rk(A | \\vec{b}) < m$), значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ВСЁ СЛОЖНО»\n",
    "\n",
    "А теперь посмотрим на самый интересный для нас случай. Его формально регламентирует второе следствие из теоремы Кронекера — Капелли.\n",
    "\n",
    "> Следствие №2 из теоремы Кронекера — Капелли:\n",
    "> \n",
    "> Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "> \n",
    "> $$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Решим систему уравнений:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/abcb754bb9f9181e93b95b49e77ff25c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_42.png)\n",
    "\n",
    "Посмотрим на первое и третье уравнение — очевидно, что такая система не имеет решений, так как данные уравнения противоречат друг другу.\n",
    "\n",
    "Но давайте обоснуем это математически. Для этого запишем расширенную матрицу системы:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/9f9d4dbee7564d18fa820c0a54a3785e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_43.png)\n",
    "\n",
    "Посчитаем ранги матриц $A$ и $A|\\vec[b$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2774b7f6264958ea7c7d6f23df959a77/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, $rk(A)=2$, в то время как $rk(A|\\vec(b))=3$. Это и есть **критерий переопределённости** системы уравнений: ранг матрицы системы меньше ранга расширенной матрицы системы.\n",
    "\n",
    "→ Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее.\n",
    "\n",
    "?\n",
    "\n",
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/4267dd365e0b367f66729d1cf0eb4b5e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_45.png)\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b}=A\\hat{w}$. Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}$.\n",
    "\n",
    "**Примечание**. Здесь мы снова опустили стрелки у векторов $b$, $\\hat{b}$ и $\\hat{w}$ для наглядности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a9f15e57bf47fd7db7b10f9986d878a1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_46.png)\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/cc797ce965e4cfa42340afcc97494476/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_47.png)\n",
    "\n",
    "→ Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "Для первого случая будем иметь:\n",
    "\n",
    "$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$\n",
    "\n",
    "Для второго случая:\n",
    "\n",
    "$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$\n",
    "\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$\\left\\|e \\right\\| \\rightarrow min$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к задаче поиска оптимальных приближений вектора $\\hat{w}$.\n",
    "\n",
    "**Примечание**. Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван методом наименьших квадратов (МНК). В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "Сейчас мы почувствуем себя настоящими математиками и попробуем решить эту задачу самостоятельно с помощью простой геометрии и знакомых нам операций над матрицами.\n",
    "\n",
    "Вспомним, что на языке линейной алгебры неразрешимость системы\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/02f4b4ff440bbe548de99fd21d758ef2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_48.png)\n",
    "\n",
    "означает, что попытка выразить вектор $(1, 2, 12)^T$ через $(1, 1, 1)^T$ и $(1,2,1)^T$ не будет успешной, так как они линейно независимы.\n",
    "\n",
    "**Геометрически** это означает, что вектор свободных коэффициентов  (коричневый) не лежит в одной плоскости со столбцами матрицы  (синие векторы)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2448deb8bce151ef64ddb640a1fa57db/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_49.png)\n",
    "\n",
    "→ Напомним, что подобную задачу мы решали в предыдущем модуле по линейной алгебре, в юните «Практика: векторы». Вы можете вернуться в предыдущий модуль и освежить в памяти решение задачи.\n",
    "\n",
    "Идея состояла в том, что наилучшим приближением для коричневого вектора будет ортогональная проекция на синюю плоскость — голубой вектор. Так происходит потому, что наименьший по длине вектор ошибок — красный — должен быть перпендикулярен к синей плоскости:\n",
    "\n",
    "$$e=b=\\hat{b}$$\n",
    "\n",
    "В прошлом модуле мы производили расчёты интуитивно, а теперь настала пора вывести формулу.\n",
    "\n",
    "Давайте умножим наши уравнения слева на $A^T$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f6cba5210f0383927c24de2da993db79/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея заключается в следующем: справа мы найдём скалярное произведение столбцов матрицы $A$ на вектор $b$, а слева — произведение столбцов $A$ на приближённый вектор $\\hat{b}$ (по сути, на голубую проекцию).\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/04f3d1836f3f2e04db6db6dad688862a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_71.png)\n",
    "\n",
    "Упростим уравнение, перемножив всё, что содержит только числа. В левой части умножим $A^T$ на $A$, в правой — умножим $A^T$ на $b$. Тогда слева получим матрицу 2×2 — это не что иное, как матрица Грама столбцов $A$.\n",
    "\n",
    "Столбцы $A$ линейно независимы, а это значит, что, по свойству матрицы Грама, $A^T \\cdot A$  — невырожденная квадратная матрица (её определитель не равен нулю, и для неё существует обратная матрица). Получившаяся система — один в один случай «идеальная пара», а это значит, что теперь мы можем её решить.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/227fe95c6711c25be991e9c81e9c0d93/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_51.png)\n",
    "\n",
    "?\n",
    "Но ведь мы не могли решить изначальную задачу, так как она была переопределена, а эту — можем. Как так получилось?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы потребовали, чтобы у приближения $\\hat{b}$ были с векторами $(1,1,1)^T$ и $(1,2,1)^T$ такие же скалярные произведения, как у $b$. Это и означает что $\\hat{b}$ — ортогональная проекция на нашу синюю плоскость, в которой лежат столбцы матрицы $A$, и в этой плоскости мы можем найти коэффициенты.\n",
    "\n",
    "Мы с вами отлично умеем решать системы типа «Идеальная пара». Для этого нам нужно найти обратную матрицу  и умножить на неё слева всё уравнение. Так мы и получим наше приближение:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/926b933be4ea80370273f21776528ca2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_52.png)\n",
    "\n",
    "Находим определитель матрицы $(A^TA)$:\n",
    "\n",
    "$det(A^T A) = 3 \\cdot 6 - 4 \\cdot 4 = 2$\n",
    "\n",
    "Находим обратную матрицу (A^TA)^{-1}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/0a77b6e3a1603d68db6cda0ebca99758/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_53.png)\n",
    "\n",
    "Умножаем всё уравнение на обратную матрицу слева:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/72ad70cfed4736faeba2e57ace2fa47d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, вот он — долгожданный приближённый вектор $\\hat{w}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/456b762ff60c55233bd071d5da87ebfd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_55.png)\n",
    "\n",
    "Ещё раз посмотрим на финальную формулу:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d22d5f53531f1b98a5c2608008f670dd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐ Пришло время открытий!\n",
    "\n",
    "Только что мы геометрическим образом вывели формулу оценки решения методом наименьших квадратов (МНК или OLS, Ordinary Least Squares).\n",
    "\n",
    "**Примечание**. Стоит отметить, что полученная матричная формула не зависит от размерностей и конкретных значений, а значит применима не только в нашем локальном случае, но и в общем.\n",
    "\n",
    "Нам осталось выполнить проверку полученных результатов, чтобы убедиться в верности решения.\n",
    "\n",
    "Вычислим голубой вектор $\\hat{b}$. Для этого возьмём линейную комбинацию столбцов матрицы $А$ с найденными нами коэффициентами $\\hat{w_1}$ и $\\hat{w_2}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/18d29c611a2228d28dfd178039e199b0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_57.png)\n",
    "\n",
    "Вычислим вектор ошибок $e$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1674251d755c8b031c983072f6557e00/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_58.png)\n",
    "\n",
    "Убедимся, что данный вектор действительно ортогонален столбцам матрицы $А$. Для этого найдём их скалярные произведения:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/583371dd14e680cce62a7c551fe859ee/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_59.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dffc86afc7e86faa61d314c567027151/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярные произведения равны 0, а это означает, что вектор ошибок $e$ действительно ортогонален всей синей плоскости, а голубой вектор $b$ приближённого значения является ортогональной проекцией коричневого вектора $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Примечание**. Прежде чем перейти к выводам, стоит отметить, что обычно OLS-оценку выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "> \n",
    "> $$\\left\\|\\vec{e} \\right\\| \\rightarrow min$$\n",
    "> \n",
    "> $$\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min$$\n",
    "> \n",
    "> $$\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$$\n",
    "> \n",
    "> Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический. Мы вернёмся к этому способу, когда будем обсуждать оптимизацию функции многих переменных в разделе по математическому анализу.\n",
    "> \n",
    "> Наконец, мы может подвести итоги для случая «Всё сложно»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов ($OLS-оценка - \\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора  на столбцы матрицы $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.9\n",
    "\n",
    "Вычислите вектор ошибок для приближённого решения системы , если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e5455f82f2c719a89defd6fbf935bf99/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_61.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, -1,  0])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,-5],[2,1],[1,1]])\n",
    "b = np.array([1,2,2])\n",
    "w_hat = np.array([1,1])\n",
    "e = b - A@w_hat\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.11\n",
    "\n",
    "*Найдите $OLS$-оценку для коэффициентов $w_1$, $w_2$ СЛАУ:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/aabe72a5dc4493bf7f00fce4be816414/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_63.png)\n",
    "\n",
    "*Для этого выполните задания под цифрами 1-4 ниже.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A : \n",
      " [[ 1  2]\n",
      " [-3  1]\n",
      " [ 1  2]\n",
      " [ 1 -1]] \n",
      " b :  [1 4 5 0]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2],[-3,1],[1,2],[1,-1]])\n",
    "b = np.array([1,4,5,0])\n",
    "print(' A : \\n',A, '\\n', 'b : ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вычислите матрицу Грама столбцов $A:A^{T}A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0],\n",
       "       [ 0, 10]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите матрицу $(A^{T}A)^{-1}$. Она имеет вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08333333, 0.        ],\n",
       "       [0.        , 0.1       ]])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(A.T@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите $A^{T} \\vec{b}$. Введите координаты полученного вектора через запятую, без пробелов. Пример ввода ответа: 1,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6, 16])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вычислите вектор оценок коэффициентов $\\vec{v}$.\n",
    "\n",
    "Примечание: для корректного ответа не округляйте $(A^TA)^{-1}$.\n",
    "\n",
    "Чему равен полученный вектор коэффициентов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  1.6])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat = np.linalg.inv(A.T@A)@A.T@b\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Линейная регрессия по методу наименьших квадратов\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В видео эксперт называет задачу регрессии **задачей прогнозирования**. Ранее, в модуле «ML-1. Теория машинного обучения», под задачей прогнозирования мы понимали отдельную ветвь задачи регрессии — предсказание будущего поведения временного ряда.\n",
    "\n",
    "В этом нет ничего страшного: такая взаимозаменяемость терминов является довольно распространённой. В таком случае для указания контекста предсказания именно временного ряда говорят «прогнозирование временных рядов».\n",
    "\n",
    "✍ С алгоритмом МНК мы познакомились. Теперь можем перейти к задаче регрессии. Начнём с её постановки.\n",
    "\n",
    "В задаче регрессии обычно есть **целевая переменная**, которую мы хотим предсказать. Её, как правило, обозначают буквой $y$. Помимо целевой переменной, есть **признаки** (их также называют **факторами** или **регрессорами**). Пусть их будет $k$ штук:\n",
    "\n",
    "$y$ — целевая переменная\n",
    "\n",
    "$x_1,x_2,...,x_k$ — признаки/факторы/регрессоры\n",
    "\n",
    "Поставить задачу — значит ответить на два вопроса:\n",
    "\n",
    "* Что у нас есть?\n",
    "* Что мы хотим получить?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответим на них ↓\n",
    "\n",
    "В задаче регрессии есть $N$ (как правило, их действительно много) наблюдений. Это наша обучающая выборка или датасет, представленный в виде таблицы. В столбцах таблицы располагаются векторы признаков $x_i$.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1b83b31d5ee587f525ad4571e378672d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_1.png)\n",
    "\n",
    "То есть и целевая переменная, и признаки представлены векторами из векторного пространства $\\mathbb{R}^N$ — каждого вектора $N$ координат.\n",
    "\n",
    "В качестве регрессионной модели мы будем использовать модель линейной регрессии. Мы предполагаем, что связь между целевой переменной и признаками линейная. Это означает, что:\n",
    "\n",
    "$$y=w_0+w_1x_1+w_2x_2+…+w_kx_k,$$\n",
    "\n",
    "или\n",
    "\n",
    "$$y=(\\vec{w}, \\vec{x})$$\n",
    "\n",
    "Здесь $\\vec{w}=(w_0,w_1,…,w_k)^T$ обозначают веса (коэффициенты уравнения линейной регрессии), а $\\vec{x}=(1,x_1, x_2,…, x_k)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Наличие коэффициента  говорит о том, что мы строим регрессию с константой, или, как ещё иногда говорят, с **интерсептом**.\n",
    "\n",
    "Пока что коэффициенты $w$ нам неизвестны. Как же их найти?\n",
    "\n",
    "Для этого у нас есть $N$ наблюдений — обучающий набор данных.\n",
    "\n",
    "Давайте попробуем подобрать такие веса $w$, чтобы для каждого наблюдения наше равенство было выполнено. Таким образом, получается $N$ уравнений на $k+1$ неизвестную.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1533aeb55381269febe2c3f0fb58761b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_2.png)\n",
    "\n",
    "Или в привычном виде систем уравнений:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/53a3b8d36387e6e7c1428fd6091d7003/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_3.png)\n",
    "\n",
    "Говоря на языке машинного обучения, мы хотим обучить такую модель, которая описывала бы зависимость целевой переменной от факторов на обучающей выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как правило, $N$ гораздо больше $k$ (количество строк с данными в таблице намного больше количества столбцов) и система переопределена, значит точного решения нет. Поэтому можно найти только приближённое.\n",
    "\n",
    "**Примечание**. Полученной СЛАУ можно дать геометрическую интерпретацию. Если представить каждое наблюдение в виде точки на графике (см. ниже), то уравнение линейной регрессии будет задавать прямую (если фактор один) или гиперплоскость (если факторов  штук). Приравняв уравнение прямой к целевому признаку, мы потребовали, чтобы эта прямая проходила через все точки в нашем наборе данных. Конечно же, это условие не может быть выполнено полностью, так как в данных всегда присутствует какой-то шум, и идеальной прямой (гиперплоскости) не получится, но зато можно построить приближённое решение.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f40756bb7b93baf3e769707d9e920fd2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обратите внимание**, что у нас появился новый вектор из единиц. Он здесь из-за того, что мы взяли модель с интерсептом. Можно считать, что это новый регрессор-константа. Данная константа тянется из уравнения прямой, которое мы разбирали в модуле «ML-2. Обучение с учителем: регрессия».\n",
    "\n",
    "Мы уже умеем решать переопределённые системы, для этого мы должны составить матрицу системы A, записав в столбцы все наши регрессоры, включая регрессор константу:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a5c552ec735bafce40afda45c3f68817/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_5.png)\n",
    "\n",
    "**Примечание**. В контексте задач машинного обучения матрица $A$ называется **матрицей наблюдений**: по строкам отложены наблюдения (объекты), а по столбцам — характеризующие их признаки. В модулях по машинному обучению мы в основном обозначали её за $X$. Здесь же мы будем придерживаться традиций линейной алгебры и обозначать матрицу за $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Обратите внимание, что индексация матрицы $A$ отличается от привычной нам индексации матрицы. Например, здесь $x_{12}$ — второе наблюдение первого регрессора. Это чистая формальность. Если обозначать за первый индекс номер наблюдения, а за второй индекс — номер регрессора, мы получим привычную нам нумерацию элементов матрицы (строка-столбец).\n",
    "\n",
    "Осталось записать финальную формулу OLS-оценки для коэффициентов:\n",
    "\n",
    "$$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$$\n",
    "\n",
    "→ Казалось бы, задача решена, однако это совсем не так, ведь мы искали коэффициенты не просто так, а чтобы сделать прогноз — предсказание на новых данных.\n",
    "\n",
    "Допустим, у нас есть новое наблюдение по регрессорам, которое характеризуется признаками $\\vec{x}_{NEW} = (x_{1, NEW}, x_{2, NEW}, ..., x_{k, NEW})^T$. Тогда, предсказание будет строиться следующим образом:\n",
    "\n",
    "$$\\vec{y}_{NEW} = \\vec{w}_0 + \\vec{w}_1 x_{1, NEW} + ... + \\vec{w}_k x_{k, NEW}$$\n",
    "\n",
    "$$или$$\n",
    "\n",
    "$$\\vec{y}_{NEW} = (\\hat{\\vec{w}}, \\vec{x}_{NEW})$$\n",
    "\n",
    "Теперь перейдём от формул к **практике** и решим задачу в контексте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим классический датасет для обучения линейной регрессии — Boston Housing. В нём собраны усреднённые данные по стоимости недвижимости в 506 районах Бостона. Ниже вы видите фрагмент датасета.\n",
    "\n",
    "**Примечание**. С данным датасетом мы знакомились, когда говорили о модели линейной регрессии в модуле [«ML-2. Обучение с учителем: регрессия»](https://lms.skillfactory.ru/courses/course-v1:SkillFactory+DST-3.0+28FEB2021/jump_to_id/4ffbb89c532a4b78a12a3db7253b9a1e#bostondata).\n",
    "\n",
    "Целевой переменной будет PRICE — это, в некотором смысле, типичная (медианная) стоимость дома в районе.\n",
    "\n",
    "Для примера возьмём в качестве регрессоров уровень преступности (CRIM) и среднее количество комнат в доме (RM).\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/855e8663c63edefc01a4646114266f4c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем нашу модель:\n",
    "\n",
    "$y=w_0+w_1 \\cdot x_1+w_2 \\cdot x_2$\n",
    "\n",
    "Для наглядности обозначим:\n",
    "\n",
    "$y=w_0+w_1 \\cdot CRIM+w_2 \\cdot RM$\n",
    "\n",
    "Составим матрицу регрессоров:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/fd804b8ab6a52c535c3bae003090bd1c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_7.png)\n",
    "\n",
    "В нашем случае $N=506$, а $k=2$. Размерность матрицы $A$ будет равна $dim A =(506, 3)$. Далее мы применяем формулу для вычисления оценок коэффициентов:\n",
    "\n",
    "$\\hat{\\vec{w}} = (A^T A)^{-1} A^T \\vec{y}$\n",
    "\n",
    "Вычисления к этой задаче мы сделаем в Python ниже, а пока приведём конечный результат. Если сократить запись до двух знаков после точки, получим следующие коэффициенты:\n",
    "\n",
    "$\\hat{\\vec{w}} = (-29.3, \\ -0.26, \\ 8.4)^T$\n",
    "\n",
    "То есть:\n",
    "\n",
    "$\\hat{w}_0 = -29.3$\n",
    "\n",
    "$\\hat{w}_1 = -0.26$\n",
    "\n",
    "$\\hat{w}_2 = 8.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем переписать нашу модель для прогноза:\n",
    "\n",
    "$\\hat{y} = -29.3 - 0.26 \\cdot CRIM + 8.4 \\cdot RM$\n",
    "\n",
    "Теперь, если у нас появятся новые наблюдения, то есть ещё один небольшой район с уровнем преступности 0.1 на душу населения и средним количеством комнат в доме, равным 8, мы сможем сделать прогноз на типичную стоимость дома в этом районе — 37 тысяч долларов:\n",
    "\n",
    "$CRIM_{NEW} = 0.1$\n",
    "\n",
    "$RM_{NEW} = 8$\n",
    "\n",
    "$\\hat{y}_{NEW} = -29.3 -0.26 \\cdot 0.1 + 8.4 \\cdot 8 \\approx 37$\n",
    "\n",
    "→ Решение на Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Home\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "\n",
    "# загружаем датасет\n",
    "boston = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(\n",
    "    data=boston.data, #данные\n",
    "    columns=boston.feature_names #наименования столбцов\n",
    ")\n",
    "boston_data['PRICE'] = boston.target;\n",
    "boston_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем матрицу $A$ из столбца единиц и факторов $CRIM$ и $RM$, а также вектор целевой переменной $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000e+00 6.3200e-03 6.5750e+00]\n",
      " [1.0000e+00 2.7310e-02 6.4210e+00]\n",
      " [1.0000e+00 2.7290e-02 7.1850e+00]\n",
      " ...\n",
      " [1.0000e+00 6.0760e-02 6.9760e+00]\n",
      " [1.0000e+00 1.0959e-01 6.7940e+00]\n",
      " [1.0000e+00 4.7410e-02 6.0300e+00]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу А и вектор целевой переменной\n",
    "CRIM = boston_data['CRIM']\n",
    "RM = boston_data['RM']\n",
    "A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "y = boston_data[['PRICE']]\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размерность матрицы $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 3)\n"
     ]
    }
   ],
   "source": [
    "# проверим размерность\n",
    "print(A.shape)\n",
    "## (506, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам ничего не мешает вычислить оценку вектора коэффициентов $w$ по выведенной нами формуле МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-29.24471945]\n",
      " [ -0.26491325]\n",
      " [  8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat = np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь составим прогноз нашей модели:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37.85733519]\n"
     ]
    }
   ],
   "source": [
    "# добавились новые данные:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# делаем прогноз типичной стоимости дома\n",
    "PRICE_new = w_hat.iloc[0]+w_hat.iloc[1]*CRIM_new+w_hat.iloc[2]*RM_new\n",
    "print(PRICE_new.values)\n",
    "## [37.85733519]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Согласитесь, такая запись вычисления оценки стоимости слишком длинная и неудобная, особенно если факторов не два, как у нас, а 200. Более короткий способ сделать прогноз — вычислить скалярное произведение вектора признаков и коэффициентов регрессии.\n",
    "\n",
    "Для удобства дальнейшего использования оформим характеристики нового наблюдения в виде матрицы размером $(1,3)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "# короткий способ сделать прогноз\n",
    "new=np.array([[1,CRIM_new,RM_new]])\n",
    "print('prediction:', (new@w_hat).values)\n",
    "## prediction: [[37.85733519]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Обратите внимание, что, решая задачу с помощью Python, мы получили немного другой результат прогноза стоимости. Это связано с тем, что при выполнении ручного расчёта мы округлили значения коэффициентов и получили менее точный результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы уже знаем, что алгоритм построения модели линейной регрессии по МНК реализован в классе LinearRegression, находящемся в модуле sklearn.linear_model. Для вычисления коэффициентов (обучения модели) нам достаточно передать в метод fit() нашу матрицу с наблюдениями и вектор целевой переменной, а для построения прогноза — вызвать метод predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
      "prediction: [[37.85733519]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "new_prediction = model.predict(new)\n",
    "print('prediction:', new_prediction)\n",
    "## w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n",
    "## prediction: [[37.85733519]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Здесь при создании объекта класса LinearRegression мы указали fit_itercept=False, так как в нашей матрице наблюдений $A$ уже присутствует столбец с единицами для умножения на свободный член $w_0$. Его повторное добавление не имеет смысла.\n",
    "\n",
    "Получили те же результаты, что и ранее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ПРОБЛЕМЫ В КЛАССИЧЕСКОЙ МНК-МОДЕЛИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что в уравнении классической OLS-регрессии присутствует очень важный множитель $A^TA$:\n",
    "\n",
    "$$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T}\\vec{y}$$\n",
    "\n",
    "Вы могли заметить, что это матрица Грама значений наших признаков, включая признак-константу.\n",
    "\n",
    "Вспомним свойства этой матрицы: \n",
    "\n",
    "* квадратная (размерности $k+1$ на $k+1$, где $k$ — количество факторов);\n",
    "* симметричная.\n",
    "\n",
    "Как и у любого метода, у классической OLS-регрессии есть свои **ограничения**. Если матрица $A^TA$ вырождена или близка к вырожденной, то хорошего решения у классической модели не получится. Такие данные называют **плохо обусловленными**.\n",
    "\n",
    "*Корректна ли модель классической OLS-регрессии, если*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2555354cab48376e28c9e067824debbd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем матрицу $A$ и вычислим $A^TA$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f730a71498b3946bb7854d86e7278b30/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_12.png)\n",
    "\n",
    "Как видите, две последние строки матрицы $A^TA$ являются пропорциональными. Это говорит о том, что матрица вырождена ($det A^T A =0$) или её ранг ($rkA=2$) меньше количества неизвестных ($3$), а значит обратной матрицы $(A^TA)^{-1}$ к ней не существует. Отсюда следует, что классическая OLS-модель **неприменима для этих данных**.\n",
    "\n",
    "Борьба с вырожденностью матрицы $A^TA$ часто сводится к устранению «плохих» (зависимых) признаков. Для этого анализируют корреляционную матрицу признаков или матрицу их значений. Но иногда проблема может заключаться, например, в том, что один признак измерен в тысячных долях, а другой — в тысячах единиц. Тогда коэффициенты при них могут отличаться в миллион раз, что потенциально может привести к вырожденности матрицы $A^TA$.\n",
    "\n",
    "В устранении этой проблемы может помочь знакомая нам **нормализация/стандартизация данных**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ОСОБЕННОСТИ КЛАССА LINEAR REGRESSION БИБЛИОТЕКИ SKLEARN\n",
    "\n",
    "Давайте посмотрим, что «скажет» Python, если мы попробуем построить модель линейной регрессии на вырожденной матрице наблюдений, используя классическую формулу линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` py\n",
    "# создадим вырожденную матрицу А\n",
    "A = np.array([\n",
    "    [1, 1, 1, 1], \n",
    "    [2, 1, 1, 2], \n",
    "    [-2, -1, -1, -2]]\n",
    ").T\n",
    "y = np.array([1, 2, 5, 1])\n",
    "# вычислим OLS-оценку для коэффициентов\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat) \n",
    "## LinAlgError: Singular matrix\n",
    "```\n",
    "----------------------\n",
    "``` py\n",
    "LinAlgError: Singular matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и ожидалось, мы получили ошибку, говорящую о том, что матрица  — сингулярная (вырожденная), а значит обратить её не получится. Что и требовалось доказать — с математикой всё сходится.\n",
    "\n",
    "⭐ Настало время фокусов!\n",
    "\n",
    "Попробуем обучить модель линейной регрессии LinearRegression из модуля sklearn, используя нашу вырожденную матрицу :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w_hat: [[-29.24471945  -0.26491325   8.39106825]]\n"
     ]
    }
   ],
   "source": [
    "# создаём модель линейной регрессии\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "# вычисляем коэффициенты регрессии\n",
    "model.fit(A, y)\n",
    "print('w_hat:', model.coef_)\n",
    "## w_hat: [ 6.   -1.25  1.25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакой ошибки не возникло! Более того, у нас даже получились вполне адекватные оценки коэффициентов линейной регрессии $\\hat{\\vec{v}}$.\n",
    "\n",
    "Но ведь мы только что использовали формулу для вычисления коэффициентов при расчётах вручную и получали ошибку. Как мы могли получить результат, если матрица $A^TA$ вырожденная? Существование обратной матрицы для неё противоречит законам линейной алгебры. Неужели это очередной случай, когда «мнения» математики и Python расходятся?\n",
    "\n",
    "На самом деле, не совсем. Здесь нет никакой магии, ошибки округления или бага. Просто в реализации линейной регрессии в sklearn предусмотрена **борьба с плохо определёнными (близкими к вырожденным и вырожденными) матрицами**.\n",
    "\n",
    "Для этого используется метод под названием **сингулярное разложение (SVD)**. О нём мы будем говорить отдельно, однако уже сейчас отметим тот факт, что данный метод позволяет всегда получать корректные значения при обращении матриц.\n",
    "\n",
    "Если вы хотите понять, почему так происходит, ознакомьтесь с [этой статьёй](https://towardsdatascience.com/understanding-linear-regression-using-the-singular-value-decomposition-1f37fb10dd33).\n",
    "\n",
    "**Суть метода** заключается в том, что в OLS-формуле мы на самом деле используем не саму матрицу $A$, а её диагональное представление из сингулярного разложения, которое гарантированно является невырожденным. Вот и весь секрет.\n",
    "\n",
    "?\n",
    "Правда, открытым остаётся вопрос: можно ли доверять коэффициентам, полученным таким способом, и интерпретировать их? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В дальнейшем мы увидим, что делать этого лучше не стоит: возможна такая ситуация, при которой коэффициенты при линейно зависимых факторах, которые получаются в результате применения линейной регрессии через сингулярное разложение, могут получиться слишком большими по модулю. Они могут измеряться миллионами, миллиардами и более высокими порядками, что не будет иметь отношения к действительности. Такие коэффициенты не подлежат интерпретации.\n",
    "\n",
    "Заметим, что в случае использования решения через сингулярное разложение для линейно зависимых столбцов коэффициенты будут всегда получаться одинаковыми по модулю, но различными по знаку: $w_1=-1.25$ и $w_2=1.25$. Неудивительно, ведь второй и третий столбцы матрицы $A$ линейно зависимы с коэффициентом — 1.\n",
    "\n",
    "Запишем итоговое уравнение линейной регрессии:\n",
    "\n",
    "$y=w_{0} \\overrightarrow{1}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}=6-1.25 \\cdot \\vec{x}_{1}+1.25 \\cdot \\vec{x}_{2},$\n",
    "\n",
    "поставим столбцы матрицы $A$ в данное уравнение, чтобы получить прогноз:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/cb2ed00bab8bf77d2ae6115d610d59b9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_3_13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. На самом деле сингулярное разложение зашито в функцию np.linalg.lstsq(), которая позволяет в одну строку построить модель линейной регрессии по МНК:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-29.24471945],\n",
       "        [ -0.26491325],\n",
       "        [  8.39106825]]),\n",
       " array([19565.80724199]),\n",
       " 3,\n",
       " array([219.85111475, 127.94826644,   2.38123074]))"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# классическая OLS-регрессия в numpy с возможностью получения решения даже для вырожденных матриц\n",
    "np.linalg.lstsq(A, y, rcond=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция возвращает четыре значения:\n",
    "\n",
    "* вектор рассчитанных коэффициентов линейной регрессии;\n",
    "* сумму квадратов ошибок, MSE (она не считается, если ранг матрицы $A$ меньше числа неизвестных, как в нашем случае);\n",
    "* ранг матрицы $A$;\n",
    "* вектор из сингулярных значений, которые как раз и оберегают нас от ошибки (о них мы поговорим позже).\n",
    "\n",
    "Обратите внимание, что мы получили те же коэффициенты, что и с помощью sklearn. При этом ранг матрицы $A$ равен 2, что меньше количества неизвестных коэффициентов. Это ожидаемо говорит о вырожденности матрицы $A$ и, как следствие, матрицы $A^TA$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмируем ↓\n",
    "\n",
    "* Для поиска коэффициентов модели линейной регрессии используется МНК-оценка: \n",
    "\n",
    "    $$\\vec{w}=\\left(A^{T} A\\right)^{-1} A^{T} \\vec{y}$$\n",
    "\n",
    "* Полученная матричная формула не зависит от размерности матрицы наблюдений $A$ и работает при любом количестве объектов/признаков в данных.\n",
    "\n",
    "* Для реализации обучения модели линейной регрессии по МНК в sklearn используется класс LinearRegression.\n",
    "\n",
    "* Для предотвращения обращения вырожденной матрицы $A$ в LinearRegression вместо самой матрицы используется её сингулярное разложение. Поэтому на практике при построении модели линейной регрессии вместо ручного вычисления обратной матрицы с помощью np.inv() приоритетнее пользоваться именно LinearRegression из sklearn (или np.linalg.lstsq()).\n",
    "\n",
    "    Данный метод оберегает от ошибки только при обращении плохо обусловленных и вырожденных матриц и не гарантирует получение корректных коэффициентов линейной регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Стандартизация векторов и матрица корреляций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В этом юните мы продолжим обсуждать проблемы классической МНК-модели линейной регрессии и способы их решения. Мы поговорим о стандартизации векторов и плавно перейдём к разбору корреляционной матрицы.\n",
    "\n",
    "## СТАНДАРТИЗАЦИЯ ВЕКТОРОВ\n",
    "\n",
    "В модулях по разведывательному анализу данных и машинному обучению мы не раз говорили о преобразованиях признаков путём нормализации и стандартизации. Вспомним, что это такое ↓\n",
    "\n",
    "> * **Нормализация** — это процесс приведения признаков к единому масштабу, например от 0 до 1. Пример — min-max-нормализация:\n",
    ">\n",
    "> $$x_{scaled} =  \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    ">\n",
    "> * **Стандартизация** — это процесс приведения признаков к единому масштабу характеристик распределения — нулевому среднему и единичному стандартному отклонению:\n",
    ">\n",
    "> $$x_{scaled} =  \\frac{x - x_{mean}}{x_{std}}$$\n",
    ">\n",
    "\n",
    "В линейной алгебре под стандартизацией вектора $\\vec{x} \\in R^n$ понимается несколько другая операция, которая проходит в два этапа:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Центрирование вектора** — это операция приведения среднего к 0:\n",
    "\n",
    "    $$\\vec{x}_{cent} = \\vec{x} - \\vec{x}_{mean}$$\n",
    "\n",
    "2. **Нормирование вектора** — это операция приведения диапазона вектора к масштабу от -1 до 1 путём деления центрированного вектора на его длину:\n",
    "\n",
    "    $$\\vec{x}_{st} =  \\frac{\\vec{x}_{cent}}{ \\| \\vec{x}_{cent} \\| },$$\n",
    "\n",
    "где $\\vec{x}_{mean}$ — вектор, составленный из среднего значения вектора $\\vec{x}$, а $||\\vec{x}_{cent}||$ — длина вектора  .\n",
    "\n",
    "В результате стандартизации вектора всегда получается новый вектор, длина которого равна 1:\n",
    "\n",
    "$$\\| \\vec{x}_{st} \\|  = 1$$\n",
    "\n",
    "***Пример № 1***\n",
    "\n",
    "*Необходимо стандартизировать векторы:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/9bc56e79ccfb16a3ecdc1090aeba333a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Центрируем:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/045220fe874b7eb51de1f6149835d000/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_2.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e29ddae31214a2804646e84bca8ec715/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_3.png)\n",
    "\n",
    "Нормируем:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1f9e10f5db14d7a14820c16722a7c10c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_4.png)\n",
    "\n",
    "Как видите, теперь оба признака имеют значения от -1 до 1 и равный порядок, в отличие от исходных признаков.\n",
    "\n",
    "Давайте посмотрим, что произойдёт с матрицей Грама после стандартизации векторов $\\vec{x}_1$ и $\\vec{x}_2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 2***\n",
    "\n",
    "*Найти матрицу для стандартизированных признаков для*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3c5b08994075c0912447cdb75a53b2e9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_5.png)\n",
    "\n",
    "Вычислим попарные скалярные произведения новых признаков:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/01aff524410530b471f92e67481d4989/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_6.png)\n",
    "\n",
    "Как видите, все числа — в диапазоне от -1 до 1. \n",
    "\n",
    "Забегая вперёд, скажем, что это так называемые **выборочные корреляции признаков**, а сама матрица является **матрицей корреляций** или **корреляционной матрицей**. Пока просто запомните, как выглядит эта матрица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот **ещё одна особенность стандартизации** ↓\n",
    "\n",
    "До стандартизации мы прогоняли регрессию $y$ на регрессоры $x_1, x_2, …, x_k$ и константу. Всего получалось $k+1$ коэффициентов:\n",
    "\n",
    "$$\\vec{y}=w_{0}+w_{1} \\vec{x}_{1}+w_{2} \\vec{x}_{2}+\\ldots+w_{k} \\vec{x}_{k}$$\n",
    "\n",
    "После стандартизации мы прогоняем регрессию стандартизованного  на стандартизованные регрессоры **без константы**:\n",
    "\n",
    "$$\\vec{y}=w_{1_{st}} \\vec{x}_{1_{st}}+w_{2_{st}} \\vec{x}_{2_{st}}+\\ldots+w_{k_{st}} \\vec{x}_{k_{st}}$$\n",
    "\n",
    "Математически мы получим одну и ту же регрессию в том смысле, что если пересчитать стандартизированные коэффициенты, мы получим исходные. То же и с прогнозом (пересчёт здесь опустим)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ЧЁМ БОНУСЫ?\n",
    "\n",
    "Математика говорит, что регрессия исходного  на исходные («сырые») признаки c константой точно такая же, как регрессия стандартизированного на стандартизированные признаки без константы. В чём же разница? Математически — ни в чём.\n",
    "\n",
    "На прогноз модели линейной регрессии, построенной по МНК, и её качество стандартизация практически не влияет. Масштабы признаков будут иметь значение только в том случае, если для поиска коэффициентов вы используете численные методы, такие как градиентный спуск (SGDRegressor из sklearn). О нём мы поговорим, когда будем знакомиться с алгоритмом градиентного спуска в модуле по оптимизации.\n",
    "\n",
    "Однако **с точки зрения интерпретации важности коэффициентов разница есть**. Если вы занимаетесь отбором наиболее важных признаков по значению коэффициентов линейной регрессии на нестандартизированных данных, это будет не совсем корректно: один признак может изменяться от 0 до 1, а второй — от -1000 до 1000. Коэффициенты при них также будут различного масштаба. Если же вы посмотрите оценки коэффициентов регрессии после стандартизации, то они будут в едином масштабе, что даст более цельную и объективную картину.\n",
    "\n",
    "Более важный бонус заключается в том, что **после стандартизации матрица Грама признаков** как по волшебству **превращается в корреляционную матрицу**, о которой пойдёт речь далее. Почему это хорошо? На свойства корреляционной матрицы опираются такие алгоритмы, как метод главных компонент и сингулярное разложение, а так как «сырая» и стандартизированная регрессия математически эквивалентны, то имеет смысл исследовать стандартизированную, а результаты обобщить на «сырую»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим всё это на **примере** ↓\n",
    "\n",
    "***Пример № 3***\n",
    "\n",
    "*Вновь рассмотрим данные о стоимости жилья в районах Бостона.*\n",
    "\n",
    "*На этот раз возьмём четыре признака: CHAS, LSTAT, CRIM и RM.*\n",
    "\n",
    "Для начала посмотрим на статистические характеристики с помощью метода describe():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.069170</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>3.613524</td>\n",
       "      <td>6.284634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.253994</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>8.601545</td>\n",
       "      <td>0.702617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>0.006320</td>\n",
       "      <td>3.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>0.082045</td>\n",
       "      <td>5.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>0.256510</td>\n",
       "      <td>6.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>3.677083</td>\n",
       "      <td>6.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>88.976200</td>\n",
       "      <td>8.780000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CHAS       LSTAT        CRIM          RM\n",
       "count  506.000000  506.000000  506.000000  506.000000\n",
       "mean     0.069170   12.653063    3.613524    6.284634\n",
       "std      0.253994    7.141062    8.601545    0.702617\n",
       "min      0.000000    1.730000    0.006320    3.561000\n",
       "25%      0.000000    6.950000    0.082045    5.885500\n",
       "50%      0.000000   11.360000    0.256510    6.208500\n",
       "75%      0.000000   16.955000    3.677083    6.623500\n",
       "max      1.000000   37.970000   88.976200    8.780000"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_data[['CHAS', 'LSTAT', 'CRIM','RM']].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждый из признаков измеряется в различных единицах и изменяется в различных диапазонах: например, CHAS лежит в диапазоне от 0 до 1, а вот CRIM — в диапазоне от 0.006 до 88.976.\n",
    "\n",
    "Рассмотрим модель линейной регрессии по МНК без стандартизации. Помним, что необходимо добавить столбец из единиц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.92052548]\n",
      " [ 3.9975594 ]\n",
      " [-0.58240212]\n",
      " [-0.09739445]\n",
      " [ 5.07554248]]\n"
     ]
    }
   ],
   "source": [
    "# составляем матрицу наблюдений и вектор целевой переменной\n",
    "A = np.column_stack((np.ones(506), boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]))\n",
    "y = boston_data[['PRICE']]\n",
    "# вычисляем OLS-оценку для коэффициентов без стандартизации\n",
    "w_hat=np.linalg.inv(A.T@A)@A.T@y\n",
    "print(w_hat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот наши коэффициенты. Округлим их для наглядности:\n",
    "\n",
    "$\\hat{w}_0 = -1.92$\n",
    "\n",
    "$\\hat{w}_{CHAS} = 4$\n",
    "\n",
    "$\\hat{w}_{LSTAT} = -0.6$\n",
    "\n",
    "$\\hat{w}_{CRIM} = -0.1$\n",
    "\n",
    "$\\hat{w}_{RM} = 5$\n",
    "\n",
    "Давайте вспомним интерпретацию коэффициентов построенной модели линейной регрессии, которую мы изучали в модуле «ML-2. Обучение с учителем: регрессия». Значение коэффициента $\\hat{w}_i$ означает, на сколько в среднем изменится медианная цена (в тысячах долларов) при увеличении $x_i$ на 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если количество низкостатусного населения (LSTAT) увеличится на 1 %, то медианная цена домов в районе (в среднем) упадёт на 0.1 тысяч долларов. А если среднее количество комнат (RM) в районе станет больше на 1, то медианная стоимость домов в районе (в среднем) увеличится на 5 тысяч долларов. \n",
    "\n",
    "Тут в голову может прийти мысль: судя по значению коэффициентов, количество комнат (RM) оказывает на стоимость жилья большее влияние, чем процент низкостатусного населения (LSTAT). Однако **такой вывод будет ошибочным**. Мы не учитываем, что признаки, а значит и коэффициенты линейной регрессии, лежат в разных масштабах. Чтобы говорить о важности влияния признаков на модель, нужно строить её на стандартизированных данных.\n",
    "\n",
    "Помним, что для построения стандартизированной линейной регрессии нам не нужен вектор свободных коэффициентов, а значит и столбец из единиц тоже не понадобится.\n",
    "\n",
    "Сначала центрируем векторы, которые находятся в столбцах матрицы $A$. Для этого вычтем среднее, вычисленное по строкам матрицы $A$ в каждом столбце, с помощью метода mean(). Затем разделим результат на длины центрированных векторов, вычисленных с помощью функции linalg.norm().\n",
    "\n",
    "**Примечание**. Обратите внимание, что для функции linalg.norm() обязательно необходимо указать параметр axis=0, так как по умолчанию норма считается для всей матрицы, а не для каждого столбца в отдельности. С определением нормы матрицы и тем, как она считается, вы можете ознакомиться в [документации к функции norm()](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "      <td>506.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CHAS   LSTAT    CRIM      RM\n",
       "count  506.00  506.00  506.00  506.00\n",
       "mean    -0.00   -0.00    0.00   -0.00\n",
       "std      0.04    0.04    0.04    0.04\n",
       "min     -0.01   -0.07   -0.02   -0.17\n",
       "25%     -0.01   -0.04   -0.02   -0.03\n",
       "50%     -0.01   -0.01   -0.02   -0.00\n",
       "75%     -0.01    0.03    0.00    0.02\n",
       "max      0.16    0.16    0.44    0.16"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# составляем матрицу наблюдений без дополнительного столбца из единиц\n",
    "A = boston_data[['CHAS', 'LSTAT', 'CRIM','RM']]\n",
    "y = boston_data[['PRICE']]\n",
    "# стандартизируем векторы в столбцах матрицы A\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь векторы имеют одинаковые средние значения и стандартные отклонения. Если вычислить длину каждого из векторов, мы увидим, что они будут равны 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(A_st, axis=0))\n",
    "## [1. 1. 1. 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения стандартизированных коэффициентов нам также понадобится стандартизация целевой переменной $y$ по тому же принципу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стандартизируем вектор целевой переменной\n",
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формула для вычисления коэффициента та же, что и раньше, только матрица $A$ теперь заменяется на $A_{st}$, а $y$ — на $y_{st}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11039956]\n",
      " [-0.45220423]\n",
      " [-0.09108766]\n",
      " [ 0.38774848]]\n"
     ]
    }
   ],
   "source": [
    "# вычислим OLS-оценку для стандартизированных коэффициентов\n",
    "w_hat_st=np.linalg.inv(A_st.T@A_st)@A_st.T@y_st\n",
    "print(w_hat_st.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь смотрим на коэффициенты. Помним, что коэффициента $\\hat{w}_0$ у нас больше нет:\n",
    "\n",
    "$\\hat{w}_{CHAS, \\ st} = 0.11$\n",
    "\n",
    "$\\hat{w}_{LSTAT, \\ st} = -0.45$\n",
    "\n",
    "$\\hat{w}_{CRIM, \\ st} = -0.09$\n",
    "\n",
    "$\\hat{w}_{RM, \\ st} = 0.38$\n",
    "\n",
    "Итак, мы видим картину, прямо противоположную той, что видели ранее. Теперь модуль коэффициента $\\left|\\hat{w}_{LSTAT, \\ st} \\right| = 0.45$ будет выше, чем модуль коэффициента $\\left|\\hat{w}_{RM, \\ st} \\right| = 0.38$. Значит, процент низкостатусного населения оказывает большее влияние на значение стоимости жилья, чем количество комнат.\n",
    "\n",
    "Однако теперь интерпретировать сами коэффициенты в тех же измерениях у нас не получится.\n",
    "\n",
    "**Сделаем важный вывод ↓**\n",
    "\n",
    "Для того чтобы проинтерпретировать оценки коэффициентов линейной регрессии (понять, каков будет прирост целевой переменной при изменении фактора на 1 условную единицу), нам достаточно построить линейную регрессию в обычном виде без стандартизации и получить обычный вектор $\\hat{\\vec{w}}$.\n",
    "\n",
    "Однако, чтобы корректно говорить о том, какой фактор оказывает на прогноз большее влияние, необходимо рассматривать стандартизированную оценку вектора коэффициентов $\\hat{\\vec{w}}_{st}$.\n",
    "\n",
    "Давайте поближе взглянем на матрицу Грама для стандартизированных факторов:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAS</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>CRIM</th>\n",
       "      <th>RM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.053929</td>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.091251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.053929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>-0.613808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.055892</td>\n",
       "      <td>0.455621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.219247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>0.091251</td>\n",
       "      <td>-0.613808</td>\n",
       "      <td>-0.219247</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CHAS     LSTAT      CRIM        RM\n",
       "CHAS   1.000000 -0.053929 -0.055892  0.091251\n",
       "LSTAT -0.053929  1.000000  0.455621 -0.613808\n",
       "CRIM  -0.055892  0.455621  1.000000 -0.219247\n",
       "RM     0.091251 -0.613808 -0.219247  1.000000"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# матрица Грама\n",
    "A_st.T @ A_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На самом деле мы с вами только что вычислили **матрицу выборочных корреляций** наших исходных факторов. Мы уже сталкивались с ней много раз в разделах по разведывательному анализу данных и машинному обучению, правда, вычисляли её мы с помощью функции Pandas, а теперь научились делать это вручную.\n",
    "\n",
    "**Примечание**. Матрицу корреляций можно получить только в том случае, если производить стандартизацию признаков как векторы (делить на длину центрированного вектора $\\vec{x}_{st}$). Другие способы стандартизации/нормализации признаков не превращают матрицу Грама в матрицу корреляций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте разберёмся с математическими особенностями корреляционной матрицы.\n",
    "\n",
    "## КОРРЕЛЯЦИОННАЯ МАТРИЦА"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Напомним, что **корреляционная матрица** $C$ — это матрица выборочных корреляций между факторами регрессий.\n",
    "\n",
    "$$C=corr(X)$$\n",
    "\n",
    "Корреляция является одной из важнейших статистических характеристик выборки. Как мы уже знаем из модуля «EDA-2. Математическая статистика в контексте EDA», корреляцию можно измерять различным способами:\n",
    "\n",
    "* корреляцией Пирсона;\n",
    "* корреляцией Спирмена;\n",
    "* корреляцией Кендалла.\n",
    "\n",
    "> В этом модуле мы будем говорить именно о **корреляции Пирсона**. Она измеряет тесноту линейных связей между непрерывными числовыми факторами и может принимать значения от -1 до +1.\n",
    "\n",
    "$$c_{ij} = corr(\\vec{x}_{i}, \\vec{x}_{j})$$\n",
    "\n",
    "Как и любая статистическая величина, корреляция бывает **генеральной** и **выборочной**. Разница очень тонкая, и мы подробнее разберём её в модуле по теории вероятностей.\n",
    "\n",
    "> **Генеральная (истинная) корреляция** — это теоретическая величина, которая отражает общую линейную зависимость между случайными величинами $X-i$ и $X_j$. Забегая вперёд скажем, что данная характеристика является абстрактной и вычисляется для **генеральных совокупностей** — всех возможных реализаций $X-i$ и $X_j$. В природе такой величины не существует, она есть только в теории вероятностей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Выборочная корреляция** — это корреляция, вычисленная на ограниченной выборке. Это уже ближе к нашей теме. Выборочная корреляция отражает линейную взаимосвязь между факторами $x_i$ и $x_j$, реализации которых представлены в выборке.\n",
    "\n",
    "Выборочная корреляция между факторами высчитывается по громоздкой (на первый взгляд) формуле:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d4967fe68e49e8466304c6b2a1e5fc14/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_14.png)\n",
    "\n",
    "Из вычисленных $c_{ij}$ как раз и составляется матрица корреляций $C$. Если факторов $k$ штук, то матрица $C$ будет квадратной размера $dimC=(k \\cdot k)$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7c5025940975cb81148929005d8dfe5e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_15.png)\n",
    "\n",
    "Давайте разберём представленную выше формулу на простом примере. Но сначала нас вновь будут ждать довольно сложные формулы — не пугайтесь.\n",
    "\n",
    "***Пример № 1***\n",
    "\n",
    "*Найти выборочную корреляцию факторов:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/64694adccb1ace615100228420e63845/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на формулу для выборочной корреляции. Чтобы вычислить коэффициент корреляции $c_{12}$, необходимо предварительно вычислить $\\vec{x}_{1_{mean}}$ и $\\vec{x}_{2_{mean}}$ — средние значения координат векторов.\n",
    "\n",
    "Мы уже вычисляли их ранее:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a6655c1b4001ddda5f07fca742039bf3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_17.png)\n",
    "\n",
    "Далее нужно вычислить числитель:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/6ada8b8a6da887327878d01993ed127a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_18.png)\n",
    "\n",
    "Если присмотреться, можно заметить не что иное, как скалярное произведение векторов $\\vec{x}_{1} -\\vec{x}_{1_{mean}})$ и $\\vec{x}_{2} -\\vec{x}_{2_{mean}})$. Считаем:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/78925ad0f36b1a82d15a62f682f0a059/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_19.png)\n",
    "\n",
    "Кажется, мы это уже где-то видели. Да — это векторы $\\vec{x}_{1_{cent}}$ и $\\vec{x}_{2_{cent}}$, которые мы получили, когда стандартизировали векторы. Посчитаем их скалярное произведение:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3b9787f952f2b515ab9d324c90300871/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А что в знаменателе?\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5e3fa77b5e9840899c93b5057c8e4ac4/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_21.png)\n",
    "\n",
    "Это произведение длин векторов $\\vec{x}_{1_{cent}}$ и $\\vec{x}_{2_{cent}}$.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a5644ce19365c79e89b4dfc9ab50ec4d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_22.png)\n",
    "\n",
    "Мы также уже считали их в примере по стандартизации:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/26a98091216e3764b944108cd72430f6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_23.png)\n",
    "\n",
    "Считаем коэффициент корреляции:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/122346e6575847f6e6a5b329909b4f7f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_24.png)\n",
    "\n",
    "Снова знакомые числа. Да — это элемент на побочной диагонали матрицы Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1_{cent}}$ и $\\vec{x}_{2_{cent}}$, а значит:\n",
    "\n",
    "$c_{12} = (\\vec{x}_{1_{st}}, \\vec{x}_{2_{st}})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если посчитать корреляцию в обратном порядке между факторами $c_{21}=(\\vec{x}_{2_{st}}, \\vec{x}_{1_{st}})$, получим то же самое число, ведь скалярное произведение перестановочно: $c_{12}=c_{21}$.\n",
    "\n",
    "**Ещё один очевидный факт** → Корреляция фактора с самим собой всегда равна 1: $c_{ii}=1$, то есть $c_{11}=c_{22}=1$. Так происходит потому, что скалярное произведение вектора с самим собой всегда даёт 1 по свойствам скалярного произведения.\n",
    "\n",
    "Вот мы и нашли нашу матрицу корреляций:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/fe38a8cfceea17039b54d7a668139a79/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_25.png)\n",
    "\n",
    "Она в точности совпадает с матрицей Грама, вычисленной для стандартизированных векторов $\\vec{x}_{1_{st}}$ и $\\vec{x}_{2_{st}}$:\n",
    "\n",
    "?\n",
    "Но «магия» ещё не закончилась. Давайте подумаем: какова геометрическая интерпретация корреляции?\n",
    "\n",
    "Присмотритесь к формуле, вспомните свойства скалярного произведения, а затем загляните в ответ:\n",
    "\n",
    "$$c_{i j}=\\frac{\\left(\\vec{x}_{i_{\\text {cent }}} \\vec{x}_{j_{\\text {cent }}}\\right)}{\\left\\|\\vec{x}_{i_{\\text {cent }}}\\right\\| \\cdot\\left\\|\\vec{x}_{j_{\\text {cent }}}\\right\\|}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это косинус угла между центрированными векторами $\\vec{x}_{i_{cent}}$ и $\\vec{x}_{j_{cent}}$. По свойству скалярного произведения:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b8a0bb72498df265b1163785689d541f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. В NumPy матрица корреляций вычисляется функцией np.corrcoef():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.18898224],\n",
       "       [-0.18898224,  1.        ]])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_1 = np.array([1, 2, 6])\n",
    "x_2 = np.array([3000, 1000, 2000])\n",
    "np.corrcoef(x_1, x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили тот же результат, что и раньше.\n",
    "\n",
    "В Pandas матрица корреляций вычисляется методом corr(), вызванным от имени DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике корреляция с точки зрения линейной алгебры означает следующее:\n",
    "\n",
    "* Если корреляция $c_{ij}=1$, значит векторы $\\vec{x}_{i}$ и $\\vec{x}_{j}$ пропорциональны и сонаправлены.\n",
    "* Если корреляция $c_{ij}=-1$, значит векторы $\\vec{x}_{i}$ и $\\vec{x}_{j}$ пропорциональны и противонаправлены.\n",
    "* Если корреляция $c_{ij}=0$, значит векторы $\\vec{x}_{i}$ и $\\vec{x}_{j}$ ортогональны друг другу и, таким образом, являются линейно независимыми.\n",
    "\n",
    "Во всех остальных случаях между факторами $\\vec{x}_{i}$ и $\\vec{x}_{j}$ существует какая-толинейная взаимосвязь, причём чем ближе модуль коэффициентакорреляции к 1, тем сильнее эта взаимосвязь. Вспомним классификациюсвязей факторов, которую мы рассматривали в модуле «EDA-2.Математическая статистика в контексте EDA»:\n",
    "\n",
    "Сила связи|Значение коэффициента корреляции\n",
    "-|:-:\n",
    "Отсутствие связи или очень слабая связь|0…+/- 0.3\n",
    "Слабая связь|+/- 0.3…+/- 0.5\n",
    "Средняя связь|+/- 0.5…+/- 0.7\n",
    "Сильная связь|+/- 0.7…+/- 0.9\n",
    "Очень сильная или абсолютная связь|+/- 0.9…+/-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод ↓**\n",
    "\n",
    "Таким образом, матрица корреляций — это матрица Грама, составленная для стандартизированных столбцов исходной матрицы наблюдений $A$. Она всегда (в теории) симметричная. На главной диагонали этой матрицы стоят 1, а на местах всех остальных элементов — коэффициенты корреляции между факторами $\\vec{x}_{i}$ и $\\vec{x}_{j}$.\n",
    "\n",
    "Если коэффициент корреляции больше 0, то взаимосвязь между факторами прямая (растёт один — растёт второй), в противном случае — обратная (растёт один — падает второй).\n",
    "\n",
    "Рассмотрим пример ↓\n",
    "\n",
    "***Пример № 2***\n",
    "\n",
    "*Проинтерпретировать выборочные коэффициенты корреляции:*\n",
    "\n",
    "$corr(\\vec{x}, \\vec{u}) = 1$, \n",
    "\n",
    "$corr(\\vec{x}, \\vec{v}) = -1$, \n",
    "\n",
    "$corr(\\vec{x}, \\vec{w}) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Даны коэффициенты корреляции трёх пар факторов, причём это краевые значения. Что они означают?\n",
    "\n",
    "* $corr(\\vec{x}, \\vec{u}) = 1$ означает что $\\vec{x}$ и $\\vec{u}$ линейно выражаются друг через друга и имеют прямую зависимость (когда растёт один фактор, растёт и другой).\n",
    "* $corr(\\vec{x}, \\vec{v}) = -1$ говорит о точно такой же линейной, но обратной взаимосвязи.\n",
    "* $corr(\\vec{x}, \\vec{w}) = 0$ означает, что факторы не связаны, то есть один фактор не чувствителен к изменениям другого.\n",
    "\n",
    "Теперь коэффициенты принимают уже не экстремальные значения:\n",
    "\n",
    "$corr(\\vec{x}, \\vec{u}) = 0.73$, \n",
    "\n",
    "$corr(\\vec{x}, \\vec{v}) = -0.72$,\n",
    "\n",
    "$corr(\\vec{x}, \\vec{w}) = 0.12$ \n",
    "\n",
    "* $corr(\\vec{x}, \\vec{u}) =0.73$ говорит о сильной прямой взаимосвязи. Угол между векторами — острый.\n",
    "* $corr(\\vec{x}, \\vec{v}) =-0.72$ говорит о сильной обратной взаимосвязи. Угол между векторами — тупой.\n",
    "* $corr(\\vec{x}, \\vec{w}) =0.12$ говорит о слабой прямой взаимосвязи. Угол между векторами острый, но близок к 90 градусам."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 3***\n",
    "\n",
    "*Давайте посмотрим на корреляционную матрицу в задаче прогнозирования количества показов квартир агентства недвижимости «Рай в шалаше» в зависимости от разных параметров.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь:\n",
    "\n",
    "* Demo 2 w — количество показов квартир за две недели;\n",
    "* Rub — стоимость аренды в рублях;\n",
    "* Area — площадь квартиры;\n",
    "* Liv.Area — жилая площадь квартиры;\n",
    "* Floor — этаж;\n",
    "* Euro — стоимость аренды в евро;\n",
    "* NLiv.Area — нежилая площадь квартиры.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bb23d029a87a3d7bea81868f09ea72ad/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Матрица получилась размером 7x7, однако её ранг равен 5, а определитель — и вовсе 0. Что это значит? Для начала заметим, что по главной диагонали матрицы стоят единицы — это корреляция каждого фактора с самим собой. Разумеется, матрица симметрична: в первой строке и первом столбце расположены корреляции целевого параметра, то есть количества показов со всеми остальными факторами. Чем эти корреляции больше, тем сильнее взаимосвязь факторов.\n",
    "\n",
    "Подозрительно одинаковыми выглядят корреляции со стоимостью аренды в рублях и евро. Корреляция между ними равна 1. Это логично так как факторы пропорциональны с каким-то коэффициентом. Кроме того, также странно велика корреляция между жилой и общей площадью. Чистой пропорциональности здесь нет, но из предыдущего модуля мы помним, что жилая, нежилая и общая площади линейно зависимы.\n",
    "\n",
    "Обратите внимание, что корреляции с нежилой площадью не так велики. Итого мы нашли два избыточных набора факторов: один набор пропорционален, другой просто линейно зависим. Это случай чистой коллинеарности. Уберём по одному фактору из каждого, и ранг станет максимальным. \n",
    "\n",
    "Нежилая площадь имеет самую маленькую корреляцию с целевым параметром, поэтому мы избавимся от неё.\n",
    "\n",
    "Между рублями и евро нет разницы — оставим рубли, так как они нам привычнее.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/ec9dfbd4d4053051f434998bc27a1e8f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы избавились от нежилой площади и аренды в евро. Ранг стал максимальным (то есть равным 5), чистой коллинеарности больше нет, но определитель всё равно маловат. В чём же дело?\n",
    "\n",
    "Стоимость аренды жилой площади и общей площади сильно коррелируют между собой. Обратите внимание на значения коэффициентов корреляции — они практически равны 1, хотя формально эти факторы линейно независимы. Такие корреляции ощутимо портят картину, что и отражается на определителе.\n",
    "\n",
    "Давайте оставим только жилую площадь, её корреляция с показами максимальна.\n",
    "\n",
    "Корреляции между жилой площадью и этажом уже не такие сильные.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1aaa706a52c33da7f8fd3e77db3ff27d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_4_30.png)\n",
    "\n",
    "Ранг матрицы теперь равен 3 (как ему и положено), а определитель не так близок к нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "* Корреляция — это мера линейной зависимости между признаками.\n",
    "\n",
    "* Чем больше по модулю корреляция между каким-нибудь фактором и целевым признаком, тем лучше:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{y}) \\right| \\rightarrow 1 - хорошо$$\n",
    "\n",
    "* Чем больше по модулю корреляция между факторами, тем хуже:\n",
    "\n",
    "$$\\left|corr(\\vec{x}_{i}, \\vec{x}_{j}) \\right| \\rightarrow 1 - плохо$$\n",
    "\n",
    "* Чем больше линейно зависимых факторов, тем меньше ранг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно выделить **два неприятных случая**:\n",
    "\n",
    "* **Чистая коллинеарность**\n",
    "\n",
    "    Некоторые факторы являются линейно зависимыми между собой. Это влечёт к уменьшению ранга матрицы факторов. Корреляции между зависимыми факторами близки к +1 или -1. Матрица корреляции вырождена.\n",
    "\n",
    "    Такие случаи очень редко встречаются на практике, но если вы таковые заметите, можете смело избавиться от одного из факторов.\n",
    "\n",
    "* **Мультиколлинеарность**\n",
    "\n",
    "    Формально линейной зависимости между факторами нет, и матрица факторов имеет максимальный ранг. Однако корреляции между мультиколлинеарными факторами по-прежнему близки к +1 или -1, и матрица корреляции практически вырождена, несмотря на то что имеет максимальный ранг.\n",
    "\n",
    "Таким образом, чистая коллинеарность провоцирует больше проблем, но её легче заметить. Мультиколлинеарность же может быть скрытой, и заметить её не так просто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### КАК ОБНАРУЖИТЬ МУЛЬТИКОЛЛИНЕАРНОСТЬ?\n",
    "\n",
    "* Иногда видно сразу или заметно по контексту, что некоторые факторы будут коррелировать между собой.\n",
    "* Также можно посмотреть на определитель матрицы корреляции: если он близок к нулю, значит дела обстоят не очень хорошо.\n",
    "* Важным маркером будут странные результаты стандартной регрессионной формулы, например слишком большие по модулю коэффициенты (вспомните модуль «ML-2. Обучение с учителем: регрессия», где у нас получились запредельные коэффициенты при решении задач) или взаимно обратные коэффициенты (как мы видели в примере в предыдущем юните). \n",
    "* И, наконец, исследование спектра матрицы корреляций и числа обусловленности не только позволяет обнаружить мультиколлинераность, но и помогает избавиться от неё.\n",
    "\n",
    "Есть много способов борьбы с мультиколлинеарностью. Мы с вами применили самый наивный — **удаление взаимных факторов «на глаз»**. Увы, это получается не всегда.\n",
    "\n",
    "Два других метода называются по-разному, но по сути делают одно и тоже: это **метод главных компонент** для корреляционной матрицы и **сингулярное разложение матрицы факторов**. О них мы поговорим в следующем модуле. \n",
    "\n",
    "Кроме того, можно воспользоваться знакомыми нам **методами регуляризации**, о которых поговорим уже в этом модуле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В ЧЁМ ПРОБЛЕМА МУЛЬТИКОЛЛИНЕАРНОСТИ ДЛЯ LINEAR REGRESSION?\n",
    "\n",
    "Несмотря на то что мультиколлинеарность делает матрицу корреляций более вырожденной, она не оказывает прямого влияния на точность модели сама по себе. Проблема полной вырожденности матрицы ($A^TA$), как мы уже обсуждали ранее, в sklearn вполне решается с помощью сингулярного разложения. То есть решение можно получить всегда даже при полной коллинеарности и сильной мультиколлинеарности, несмотря на противоречие с теорией линейной алгебры.\n",
    "\n",
    "?\n",
    "Однако сможем ли мы доверять такому решению?\n",
    "\n",
    "Бывают задачи, где важно не просто построить модель, но и проинтерпретировать её результат — коэффициенты линейной регрессии. Типичный пример — задача кредитного скоринга: в ней важно понять, что влияет на вероятность дефолта заёмщика."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема заключается в том, что в случае мультиколлинеарности коэффициенты линейной регрессии становятся неустойчивыми. Например, признак «остаток долга/сумма выдачи» вроде бы должен приводить к уменьшению вероятности дефолта, так как клиенту остаётся выплачивать всё меньшую сумму. Однако мультиколлинеарность приводит к тому, что подобранный в ходе обучения модели коэффициент может сменить знак на противоположный, а признак, с точки зрения модели, может начать говорить об обратном: чем меньше остаётся платить, тем больше вероятность дефолта. Подобный кейс хорошо описан в этой статье — рекомендуем с ней ознакомиться.\n",
    "\n",
    "К тому же, чем больше в данных мультиколлинеарных факторов, тем сильнее увеличивается разброс коэффициентов регрессии. Полная коллинеарность означает, что существует бесконечное количество способов выразить один фактор через линейную комбинацию других. В свою очередь это значит, что есть бесконечное число возможных коэффициентов регрессии $\\vec{w}$, таких, которые дают одни и те же результаты. \n",
    "\n",
    "Чем больше высококоррелированных факторов в данных, тем больше таких линейных комбинаций и тем больше коэффициенты становятся по модулю, что приводит к проблеме под названием «взрывной рост весов», когда коэффициенты регрессии начинают стремиться к бесконечности, что приводит к «поломке» даже устойчивой к вырожденным матрицам модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.7\n",
    "\n",
    "Вычислите коэффициент корреляции между векторами $\\vec{v}=(5, 1, 2)^T$ и $\\vec{u}=(4, 2, 8)^T$.\n",
    "\n",
    "Ответ округлите до двух знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([5, 1, 2])\n",
    "u = np.array([4, 2, 8])\n",
    "\n",
    "v_st = (v - v.mean())/(np.linalg.norm(v - v.mean()))\n",
    "u_st = (u - u.mean())/(np.linalg.norm(u - u.mean()))\n",
    "round(v_st@u_st, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.8\n",
    "\n",
    "Составьте корреляционную матрицу для системы векторов:\n",
    "\n",
    "$\\vec{x}_1=(5.1, 1.8, 2.1, 10.3, 12.1, 12.6)^T$\n",
    "\n",
    "$\\vec{x}_2=(10.2, 3.7, 4.1, 20.5, 24.2, 24.1)^T$\n",
    "\n",
    "$\\vec{x}_3=(2.5, 0.9, 1.1, 5.1, 6.1, 6.3)^T$\n",
    "\n",
    "Для расчёта используйте библиотеку NumPy или Pandas.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999255</td>\n",
       "      <td>0.999837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999837</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  1.000000  0.999255  0.999837\n",
       "1  0.999255  1.000000  0.999066\n",
       "2  0.999837  0.999066  1.000000"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.array([5.1, 1.8, 2.1, 10.3, 12.1, 12.6])\n",
    "x2 = np.array([10.2, 3.7, 4.1, 20.5, 24.2, 24.1])\n",
    "x3 = np.array([2.5, 0.9, 1.1, 5.1, 6.1, 6.3])\n",
    "\n",
    "A = pd.DataFrame([x1, x2, x3]).T\n",
    "A.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Чему равен ранг полученной корреляционной матрицы?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(A.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Чему равен определитель полученной корреляционной матрицы? Ответ округлите до седьмого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-07"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.linalg.det(A.corr()), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Практика: линейная регрессия и метод наименьших квадратов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Настало время попрактиковаться в построении модели линейной регрессии и закрепить знания математического аппарата для работы с ней.\n",
    "\n",
    "Сразу импортируем необходимые библиотеки для работы с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У Василия, основателя компании «Газ-Таз-Ваз-Нефть», дела идут в гору: в этом году он открывает 100 новых скважин по добыче газа. Однако в целях оптимизации расходов и для потенциального повышения дохода Василию необходимо оценить, сколько денег будет приносить ему каждая из скважин, а также понять, какие факторы потенциально сильнейшим образом влияют на объём добычи газа. Для этого Василий решил нанять вас как специалиста по построению моделей машинного обучения.\n",
    "\n",
    "Василий представляет вам набор данных о добыче газа на своих скважинах. Файл с данными вы можете скачать [здесь](https://lms.skillfactory.ru/assets/courseware/v1/11a2ef69ecca8fc5ec1b5c43c8dde935/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/unconv.zip).\n",
    "\n",
    "**Признаки**:\n",
    "\n",
    "* Well — идентификатор скважины;\n",
    "* Por — пористость скважины (%);\n",
    "* Perm — проницаемость скважины;\n",
    "* AI — акустический импеданс ($кг/м^2*10^6$);\n",
    "* Brittle — коэффициент хрупкости скважины (%);\n",
    "* TOC — общий органический углерод (%);\n",
    "* VR — коэффициент отражения витринита (%);\n",
    "* Prod — добыча газа в сутки (млн. кубических футов).\n",
    "\n",
    "**Ваша задача** — построить регрессионную модель, которая прогнозирует выработку газа на скважине (целевой признак — Prod) на основе остальных характеристик скважины, и проинтерпретировать результаты вашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.31</td>\n",
       "      <td>4165.196191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.88</td>\n",
       "      <td>3561.146205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14.02</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.01</td>\n",
       "      <td>72.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4284.348574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.67</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.63</td>\n",
       "      <td>39.81</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.88</td>\n",
       "      <td>5098.680869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17.52</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.18</td>\n",
       "      <td>10.94</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.90</td>\n",
       "      <td>3406.132832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Well    Por  Perm    AI  Brittle   TOC    VR         Prod\n",
       "0     1  12.08  2.92  2.80    81.40  1.16  2.31  4165.196191\n",
       "1     2  12.38  3.53  3.22    46.17  0.89  1.88  3561.146205\n",
       "2     3  14.02  2.59  4.01    72.80  0.89  2.72  4284.348574\n",
       "3     4  17.67  6.75  2.63    39.81  1.08  1.88  5098.680869\n",
       "4     5  17.52  4.57  3.18    10.94  1.51  1.90  3406.132832"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/unconv.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала в качестве модели будем использовать простую линейную регрессию.\n",
    "\n",
    "### Задание 5.1\n",
    "\n",
    "Постройте корреляционную матрицу факторов, включив в неё целевой признак. Ответьте на следующие вопросы:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Well</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.068927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.727426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.079252</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOC</th>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>0.654445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.861910</td>\n",
       "      <td>0.727426</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "Well     1.000000  0.068927  0.077928  0.041483 -0.079252  0.022624 -0.007279   \n",
       "Por      0.068927  1.000000  0.760546 -0.461549 -0.218570  0.711831  0.111860   \n",
       "Perm     0.077928  0.760546  1.000000 -0.239636 -0.124017  0.471746  0.051023   \n",
       "AI       0.041483 -0.461549 -0.239636  1.000000  0.127599 -0.531864  0.499143   \n",
       "Brittle -0.079252 -0.218570 -0.124017  0.127599  1.000000 -0.214282  0.317929   \n",
       "TOC      0.022624  0.711831  0.471746 -0.531864 -0.214282  1.000000  0.299483   \n",
       "VR      -0.007279  0.111860  0.051023  0.499143  0.317929  0.299483  1.000000   \n",
       "Prod     0.026817  0.861910  0.727426 -0.390835  0.237155  0.654445  0.323182   \n",
       "\n",
       "             Prod  \n",
       "Well     0.026817  \n",
       "Por      0.861910  \n",
       "Perm     0.727426  \n",
       "AI      -0.390835  \n",
       "Brittle  0.237155  \n",
       "TOC      0.654445  \n",
       "VR       0.323182  \n",
       "Prod     1.000000  "
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = data.corr()\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Выберите топ-3 факторов, наиболее коррелированных с целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prod       1.000000\n",
       "Por        0.861910\n",
       "Perm       0.727426\n",
       "TOC        0.654445\n",
       "VR         0.323182\n",
       "Brittle    0.237155\n",
       "Well       0.026817\n",
       "AI         0.390835\n",
       "Name: Prod, dtype: float64"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(C.sort_values(by='Prod', ascending=False).iloc[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите ранг полученной матрицы корреляций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите определитель матрицы корреляций. Ответ округлите до четвёртого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.linalg.det(C), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.2\n",
    "\n",
    "Создайте матрицу наблюдений. Обозначьте её за $X$, а вектор правильных ответов — за $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Well', 'Por', 'Perm', 'AI', 'Brittle', 'TOC', 'VR', 'Prod'], dtype='object')"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>2.92</td>\n",
       "      <td>2.80</td>\n",
       "      <td>81.40</td>\n",
       "      <td>1.16</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12.38</td>\n",
       "      <td>3.53</td>\n",
       "      <td>3.22</td>\n",
       "      <td>46.17</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>14.02</td>\n",
       "      <td>2.59</td>\n",
       "      <td>4.01</td>\n",
       "      <td>72.80</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>17.67</td>\n",
       "      <td>6.75</td>\n",
       "      <td>2.63</td>\n",
       "      <td>39.81</td>\n",
       "      <td>1.08</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17.52</td>\n",
       "      <td>4.57</td>\n",
       "      <td>3.18</td>\n",
       "      <td>10.94</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>196</td>\n",
       "      <td>11.95</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2.97</td>\n",
       "      <td>67.18</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>197</td>\n",
       "      <td>17.99</td>\n",
       "      <td>9.87</td>\n",
       "      <td>3.38</td>\n",
       "      <td>44.32</td>\n",
       "      <td>0.98</td>\n",
       "      <td>2.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>198</td>\n",
       "      <td>12.12</td>\n",
       "      <td>2.27</td>\n",
       "      <td>3.52</td>\n",
       "      <td>57.07</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>199</td>\n",
       "      <td>15.55</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.48</td>\n",
       "      <td>58.25</td>\n",
       "      <td>1.89</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>200</td>\n",
       "      <td>20.89</td>\n",
       "      <td>7.54</td>\n",
       "      <td>3.23</td>\n",
       "      <td>46.17</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Well    Por  Perm    AI  Brittle   TOC    VR\n",
       "0       1  12.08  2.92  2.80    81.40  1.16  2.31\n",
       "1       2  12.38  3.53  3.22    46.17  0.89  1.88\n",
       "2       3  14.02  2.59  4.01    72.80  0.89  2.72\n",
       "3       4  17.67  6.75  2.63    39.81  1.08  1.88\n",
       "4       5  17.52  4.57  3.18    10.94  1.51  1.90\n",
       "..    ...    ...   ...   ...      ...   ...   ...\n",
       "195   196  11.95  3.13  2.97    67.18  0.80  2.06\n",
       "196   197  17.99  9.87  3.38    44.32  0.98  2.08\n",
       "197   198  12.12  2.27  3.52    57.07 -0.04  1.73\n",
       "198   199  15.55  4.48  2.48    58.25  1.89  2.35\n",
       "199   200  20.89  7.54  3.23    46.17  1.71  2.27\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop(columns = ['Prod'])\n",
    "y = data['Prod']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07071068, -0.01512243, -0.09243483, ...,  0.14109496,\n",
       "        -0.07011773, -0.06947435],\n",
       "       [-0.07071068, -0.0143976 , -0.0896004 , ...,  0.05760383,\n",
       "        -0.07103216, -0.07100216],\n",
       "       [-0.07071068, -0.01367277, -0.07410552, ...,  0.12071393,\n",
       "        -0.07103216, -0.0680176 ],\n",
       "       ...,\n",
       "       [-0.07071068,  0.12766899, -0.09205691, ...,  0.0834356 ,\n",
       "        -0.07418185, -0.07153512],\n",
       "       [-0.07071068,  0.12839382, -0.05964993, ...,  0.08623207,\n",
       "        -0.06764539, -0.06933223],\n",
       "       [-0.07071068,  0.12911865, -0.00919708, ...,  0.05760383,\n",
       "        -0.06825501, -0.06961647]])"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.column_stack((np.ones(200), X))\n",
    "A_cent = A - A.mean()\n",
    "A_st = A_cent/np.linalg.norm(A_cent, axis=0)\n",
    "A_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     -0.010434\n",
       "1     -0.053598\n",
       "2     -0.001920\n",
       "3      0.056270\n",
       "4     -0.064675\n",
       "         ...   \n",
       "195   -0.033131\n",
       "196    0.092180\n",
       "197   -0.064429\n",
       "198    0.055477\n",
       "199    0.168024\n",
       "Name: Prod, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cent = y - y.mean()\n",
    "y_st = y_cent/np.linalg.norm(y_cent, axis=0)\n",
    "y_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Постройте модель линейной регрессии по методу наименьших квадратов. Для этого используйте матричную формулу NumPy. В качестве ответа укажите полученные оценки коэффициентов модели. **Ответ округлите до целого числа**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1232.,     0.,   230.,   116.,  -365.,    25.,   -78.,   785.])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat = (np.linalg.inv(A.T@A)@A.T@y).round()\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.3\n",
    "\n",
    "Далее потренируемся строить предсказание для наблюдений целевой переменной.\n",
    "\n",
    "1. Постройте прогноз выработки газа для скважины с параметрами, указанными ниже. Чему равна абсолютная ошибка построенного вами прогноза для предложенной скважины (в миллионах кубических футов в день). Ответ округлите до целого числа.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d8b159aa3b085511221f509dd93e977f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_5_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new = np.array([1, 106, 15.32, 3.71, 3.29, 55.99, 1.35, 2.42])\n",
    "y_new = 4748.315024\n",
    "\n",
    "y_pred = x_new @ w_hat\n",
    "\n",
    "round(abs(y_pred - y_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Постройте прогноз выработки газа для всех скважин из обучающего набора данных. Чему равно значение метрики MAPE вашей модели? Ответ приведите в процентах (не указывайте знак процента), округлив его до первого знака после точки-разделителя.\n",
    "\n",
    "    Примечание. Если вы забыли интерпретацию данной метрики и формулу (функцию из sklearn) для её расчёта, вернитесь в модуль [«ML-2. Обучение с учителем: регрессия»](https://lms.skillfactory.ru/courses/course-v1:SkillFactory+DST-3.0+28FEB2021/jump_to_id/60b4775f80fd4e29a38603f226c0608f#MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = A@w_hat\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "round(metrics.mean_absolute_percentage_error(y_pred, y)*100 , 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.4\n",
    "\n",
    "Настало время анализа построенной модели. Посмотрите на коэффициенты и сравните их знаки со значениями выборочных корреляций между целевым признаком и факторами, которые вы нашли ранее.\n",
    "\n",
    "\n",
    "1. Есть ли в вашей модели фактор, при котором коэффициент в модели линейной регрессии противоречит соответствующему коэффициенту корреляции? Например, корреляция говорит, что зависимость между фактором и целью прямая, а модель говорит обратное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>w_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Well</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Por</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Perm</td>\n",
       "      <td>-364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AI</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brittle</td>\n",
       "      <td>-77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TOC</td>\n",
       "      <td>783.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  w_hat\n",
       "1     Well  230.0\n",
       "2      Por  116.0\n",
       "3     Perm -364.0\n",
       "4       AI   25.0\n",
       "5  Brittle  -77.0\n",
       "6      TOC  783.0"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat_df = pd.DataFrame(list(zip((['w_0'] + list(X.columns)), w_hat)), \n",
    "                        columns = ['Feature', 'w_hat'])\n",
    "w_hat_df[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Well       0.026817\n",
       "Por        0.861910\n",
       "Perm       0.727426\n",
       "AI        -0.390835\n",
       "Brittle    0.237155\n",
       "TOC        0.654445\n",
       "VR         0.323182\n",
       "Name: Prod, dtype: float64"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C['Prod'][0:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, есть такой коэффициент $\\hat{w}_{TOC} \\approx -78$. Согласно построенной модели, зависимость между процентом органического углерода и производительностью скважины обратная. Однако, согласно положительному коэффициенту корреляции между этим факторым и целевым признаком, равным 0.65 (а также согласно реальным фактам из сферы добычи газа), зависимость должна быть прямой. Чтобы убедиться в этом, можно построить диаграмму рассеяния, отражающую зависимость между TOC и Prod:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x='TOC', y='Prod', data=data);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наличие сильной мультиколлинеарности в данных, которая «портит» коэффициенты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.5\n",
    "\n",
    "Исключите из данных сильно коррелированные между собой факторы. Под сильной корреляцией в данной задаче будем понимать значения, выше $0.7$. Выбирая, какой из коррелированных факторов оставить, руководствуйтесь коэффициентом корреляции с целевой переменной: оставляйте тот фактор, который больше всего коррелирует с объёмом добычи газа.\n",
    "\n",
    "Также исключите из данных факторы, для которых корреляция с целевой переменной меньше $0.05$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Well</th>\n",
       "      <th>Por</th>\n",
       "      <th>Perm</th>\n",
       "      <th>AI</th>\n",
       "      <th>Brittle</th>\n",
       "      <th>TOC</th>\n",
       "      <th>VR</th>\n",
       "      <th>Prod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Well</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.068927</td>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.079252</td>\n",
       "      <td>0.022624</td>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.026817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Por</th>\n",
       "      <td>0.068927</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.861910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perm</th>\n",
       "      <td>0.077928</td>\n",
       "      <td>0.760546</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.727426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AI</th>\n",
       "      <td>0.041483</td>\n",
       "      <td>-0.461549</td>\n",
       "      <td>-0.239636</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>-0.390835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brittle</th>\n",
       "      <td>-0.079252</td>\n",
       "      <td>-0.218570</td>\n",
       "      <td>-0.124017</td>\n",
       "      <td>0.127599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.237155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOC</th>\n",
       "      <td>0.022624</td>\n",
       "      <td>0.711831</td>\n",
       "      <td>0.471746</td>\n",
       "      <td>-0.531864</td>\n",
       "      <td>-0.214282</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>0.654445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VR</th>\n",
       "      <td>-0.007279</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.051023</td>\n",
       "      <td>0.499143</td>\n",
       "      <td>0.317929</td>\n",
       "      <td>0.299483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod</th>\n",
       "      <td>0.026817</td>\n",
       "      <td>0.861910</td>\n",
       "      <td>0.727426</td>\n",
       "      <td>-0.390835</td>\n",
       "      <td>0.237155</td>\n",
       "      <td>0.654445</td>\n",
       "      <td>0.323182</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Well       Por      Perm        AI   Brittle       TOC        VR  \\\n",
       "Well     1.000000  0.068927  0.077928  0.041483 -0.079252  0.022624 -0.007279   \n",
       "Por      0.068927  1.000000  0.760546 -0.461549 -0.218570  0.711831  0.111860   \n",
       "Perm     0.077928  0.760546  1.000000 -0.239636 -0.124017  0.471746  0.051023   \n",
       "AI       0.041483 -0.461549 -0.239636  1.000000  0.127599 -0.531864  0.499143   \n",
       "Brittle -0.079252 -0.218570 -0.124017  0.127599  1.000000 -0.214282  0.317929   \n",
       "TOC      0.022624  0.711831  0.471746 -0.531864 -0.214282  1.000000  0.299483   \n",
       "VR      -0.007279  0.111860  0.051023  0.499143  0.317929  0.299483  1.000000   \n",
       "Prod     0.026817  0.861910  0.727426 -0.390835  0.237155  0.654445  0.323182   \n",
       "\n",
       "             Prod  \n",
       "Well     0.026817  \n",
       "Por      0.861910  \n",
       "Perm     0.727426  \n",
       "AI      -0.390835  \n",
       "Brittle  0.237155  \n",
       "TOC      0.654445  \n",
       "VR       0.323182  \n",
       "Prod     1.000000  "
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Постройте линейную регрессию на обновлённых после удаления факторов данных по методу наименьших квадратов. Для этого используйте матричную формулу NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>w_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Por</td>\n",
       "      <td>293.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AI</td>\n",
       "      <td>-200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brittle</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VR</td>\n",
       "      <td>517.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature  w_hat\n",
       "1      Por  293.0\n",
       "2       AI -200.0\n",
       "3  Brittle   28.0\n",
       "4       VR  517.0"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_droped = X.drop(columns = ['Well', 'Perm', 'TOC'])\n",
    "\n",
    "A = np.column_stack((np.ones(200), X_droped))\n",
    "\n",
    "w_hat = (np.linalg.inv(A.T@A)@A.T@y).round()\n",
    "\n",
    "w_hat_df = pd.DataFrame(list(zip((['w_0'] + list(X_droped.columns)), w_hat)), \n",
    "                        columns = ['Feature', 'w_hat'])\n",
    "w_hat_df[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Por        0.861910\n",
       "AI        -0.390835\n",
       "Brittle    0.237155\n",
       "VR         0.323182\n",
       "Name: Prod, dtype: float64"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[list(X_droped.columns) + ['Prod']].corr().iloc[:-1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Сделайте прогноз для всего обучающего набора данных и рассчитайте метрику MAPE (Mean Absolute Percentage Error). Результат приведите в процентах (не указывайте знак процента), округлив его до первого знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = A@w_hat\n",
    "\n",
    "round(metrics.mean_absolute_percentage_error(y_pred, y)*100 , 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950b5653ccfc34417735dd321d006fd482b31f7611416c3d8236dc5b17587d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
