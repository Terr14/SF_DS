{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MATH&ML-2. Линейная алгебра в контексте линейных методов. Часть II**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Рады приветствовать вас во втором модуле, посвящённом линейной алгебре!\n",
    "\n",
    "В предыдущем модуле мы познакомились с базовыми понятиями линейной алгебры. По сути, мы изучили основы матричного языка и теперь можем на нем говорить. \n",
    "\n",
    "В этом модуле мы перейдём к применению линейной алгебры в машинном обучении и рассмотрим **алгоритмы анализа данных** с разных сторон. В основном мы, конечно, будем говорить о модели линейной регрессии и её модификациях.\n",
    "\n",
    "Мы будем использовать весь математический аппарат, который изучили в прошлом модуле. Темы предстоят очень интересные, но в то же время непростые. Для того чтобы усвоить их, необходимо владеть всеми навыками, приобретёнными в предыдущем модуле — от базовых операций над векторами до знания принципов решения СЛАУ. Давайте проверим, насколько вы готовы ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.1\n",
    "\n",
    "Найдите скалярное произведение векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (-1, 2, \\ -7, 9)^T$\n",
    "\n",
    "$\\vec{v_{2}}  = (2, 8, 2, \\ -1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([-1, 2, -7, 9])@np.array([2, 8, 2, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [3, 5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1, 1], [1, 2]])@np.array([[1, 1], [1, 2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.4\n",
    "\n",
    "Задана матрица\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c1579bdd2592b86d75c9413c69d0627d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_1_1.png)\n",
    "\n",
    "Найдите матрицу Грама $(A^{T}A)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A^T)*A: \n",
      " [[2 3]\n",
      " [3 5]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 1], [1, 2]])\n",
    "print('(A^T)*A: \\n', A.T@A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.6\n",
    "\n",
    "Вычислите обратную матрицу $A^{-1}$, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e9a41d606d7dcf555779b45d871b27ce/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_1_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7., -2.],\n",
       "       [-3.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1, 2], [3, 7]])\n",
    "np.linalg.inv(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.7\n",
    "\n",
    "Найдите ранг матрицы системы, составленной из векторов:\n",
    "\n",
    "$\\vec{v_{1}}  = (2, 10, \\ -2)^T$\n",
    "\n",
    "$\\vec{v_{2}}  = (3, 2, \\ -2)^T$\n",
    "\n",
    "$\\vec{v_{3}}  = (8, 14, \\ -6)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = np.array([2, 10, -2])\n",
    "v2 = np.array([3, 2, -2])\n",
    "v3 = np.array([8, 14, -6])\n",
    "\n",
    "A = np.array([v1, v2, v3]).T\n",
    "np.linalg.matrix_rank(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В первой части** мы будем говорить о **классической модели линейной регрессии**. Для этого мы вернёмся к неоднородным системам линейных алгебраических уравнений, которые мы затронули в прошлом модуле, и посмотрим, как они связаны с **методом наименьших квадратов** (МНК, или OLS, Ordinary Least Squares). Затем мы с математической точки зрения посмотрим на проблемы, которые возникают при его использовании, например мультиколлинеарность или чересчур большое количество факторов.\n",
    "\n",
    "**Во второй части** мы перейдём к модификациям модели линейной регрессии и посмотрим, как линейная алгебра работает в **полиномиальной регрессии**, а также поговорим о том, как работают методы регуляризации на математическом уровне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конце каждого из блоков нас ожидает небольшая **практическая задача** на применение регрессионных моделей.\n",
    "\n",
    "Сразу отметим, что **мы не будем рассматривать следующие вопросы**:\n",
    "\n",
    "* Вероятностные предпосылки использования модели линейной регрессии, необходимые для её валидности (данные предпосылки регламентирует теорема Маркова-Гаусса).\n",
    "* Оценки качества полученной регрессионной модели и оценки статистической значимости её коэффициентов.\n",
    "* Статистические методы предварительной обработки данных.\n",
    "\n",
    "Для изучения этих вопросов понадобятся знания в области теории вероятности и статистики. Мы обязательно обсудим их в следующих модулях. А пока нас интересуют подробности построения вычислительных алгоритмов с точки зрения линейной алгебры.\n",
    "\n",
    "* Также мы не будем затрагивать тему корректной валидации моделей и разделять выборку на тренировочную/тестовую/валидационную с целью экономии времени и сил. Надеемся, что вы помните, как правильно оценивать качество получаемых моделей, сможете проделать это самостоятельно.\n",
    "\n",
    "Таким образом, мы будем рассматривать **только математическую составляющую** линейных (и полиномиальных) моделей регрессии, не концентрируясь на сторонних аспектах машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ЦЕЛИ ДАННОГО МОДУЛЯ:\n",
    "\n",
    "* познакомиться с неоднородными СЛАУ и случаями их решений;\n",
    "* изучить математическую формализацию метода наименьших квадратов;\n",
    "* научиться строить модель линейной регрессии с помощью МНК;\n",
    "* понять, какие проблемы возникают в МНК с математической точки зрения;\n",
    "* познакомиться с математической формализацией полиномиальной регрессии;\n",
    "* рассмотреть методы регуляризации и принципы их работы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Неоднородные СЛАУ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы начнём с **алгоритма классической линейной регрессии по методу наименьших квадратов** (OLS, Ordinary Least Squares). Данный алгоритм является базовым, но, тем не менее, весьма непрост для восприятия, поэтому данная сложносочинённая задача будет разделена на две части:\n",
    "\n",
    "* В этом юните мы обсудим случаи и алгоритм решения неоднородных СЛАУ.\n",
    "* В следующем юните подведём под эту задачу контекст задачи регрессии.\n",
    "\n",
    "Для начала давайте вспомним, что такое **неоднородные СЛАУ**.\n",
    "\n",
    "**Примечание**. Совокупность уравнений первой степени, в которых каждая переменная и коэффициенты в ней являются вещественными числами, называется **системой линейных алгебраических уравнений** (**СЛАУ**) и в общем случае записывается как:\n",
    "\n",
    "$$\\left\\{ \\begin{array}{c} a_{11}x_1+a_{12}x_2+\\dots +a_{1m}x_m=b_1 \\\\ a_{21}x_1+a_{22}x_2+\\dots +a_{2m}x_m=b_2 \\\\ \\dots \\\\ a_{n1}x_1+a_{n2}x_2+\\dots +a_{nm}x_m=b_n \\end{array} \\right.\\ (1),$$\n",
    "\n",
    "где\n",
    "\n",
    "* $n$ — количество уравнений;\n",
    "* $m$ — количество переменных;\n",
    "* $x_i$ — неизвестные переменные системы;\n",
    "* $a_{ij}$ — коэффициенты системы;\n",
    "* $b_i$ — свободные члены системы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CЛАУ (1) называется **однородной**, если все свободные члены системы равны 0 ($b_1=b_2=⋯=b_n=0$):\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\forall b_i=0$$\n",
    "\n",
    "СЛАУ (1) называется **неоднородной**, если хотя бы один из свободных членов системы отличен от 0:\n",
    "\n",
    "$$\\textrm{С}\\textrm{Л}\\textrm{А}\\textrm{У}-\\textrm{н}\\textrm{е}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{о}\\textrm{р}\\textrm{о}\\textrm{д}\\textrm{н}\\textrm{а}\\textrm{я},\\ \\textrm{е}\\textrm{с}\\textrm{л}\\textrm{и}\\ \\exists b_i\\neq 0$$\n",
    "\n",
    "Пример неоднородной СЛАУ:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Вспомним, что СЛАУ можно записать в матричном виде:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/35808bc9ca767f3c996106d8887c1108/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_3.png)\n",
    "\n",
    "где $A$ — матрица системы, $\\omega$ — вектор неизвестных коэффициентов, а $b$ — вектор свободных членов. \n",
    "\n",
    "Давайте введём новое для нас определение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Р**асширенной матрицей системы $A|b$ неоднородных СЛАУ** называется матрица, составленная из исходной матрицы и вектора свободных коэффициентов:\n",
    ">\n",
    "> $$(A \\mid \\vec{b})=\\left(\\begin{array}{cccc|c} a_{11} & a_{12} & \\ldots & a_{1 m} & b_{1} \\\\ a_{21} & a_{22} & \\ldots & a_{2 m} & b_{2} \\\\ \\ldots & \\ldots & \\ldots & \\ldots & \\ldots \\\\ a_{n 1} & a_{n 2} & \\ldots & a_{n m} & b_{n} \\end{array}\\right)$$\n",
    "\n",
    "Расширенная матрица системы — это обычная матрица. Черта, отделяющая коэффициенты $a_{ij}$ от свободных членов $b_i$ — чисто символическая. \n",
    "\n",
    "Над расширенной матрицей неоднородной СЛАУ можно производить те же самые действия, что и над обычной, а именно:\n",
    "\n",
    "* складывать/вычитать между собой строки/столбцы матрицы;\n",
    "* умножать строки/столбцы на константу;\n",
    "* менять строки/столбцы местами.\n",
    "\n",
    "Приведём пример расширенной матрицы системы. Пусть исходная система будет следующей:\n",
    "\n",
    "$\\left\\{\\begin{array}{c} w_{1}+w_{2}=1 \\\\ w_{1}+2 w_{2}=2 \\end{array}\\right.$\n",
    "\n",
    "Запишем её в матричном виде:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f928ba511cc7a2ac62881707fdfeea66/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_6.png)\n",
    "\n",
    "Тогда расширенная матрица системы будет иметь вид:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f534ccbdd7c85b138e033bbb178a520b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "еперь, когда мы вспомнили все определения и познакомились с термином расширенной матрицы, мы можем переходить к решению неоднородных СЛАУ.\n",
    "\n",
    "Существует три случая при решении неоднородных СЛАУ:\n",
    "\n",
    "* «Идеальная пара»\n",
    "\n",
    "    Это так называемые определённые системы линейных уравнений, имеющие единственные решения.\n",
    "\n",
    "* «В активном поиске»\n",
    "\n",
    "    Неопределённые системы, имеющие бесконечно много решений.\n",
    "\n",
    "* «Всё сложно»\n",
    "\n",
    "    Это самый интересный для нас случай — переопределённые системы, которые не имеют точных решений.\n",
    "\n",
    "**Примечание**. В данной классификации неоднородных СЛАУ допущено упрощение в терминологии. На самом деле неопределённые системы — это те, в которых независимых уравнений меньше, чем неизвестных. Они могут иметь бесконечно много решений (быть совместными) или ни одного решения (быть несовместными, если уравнения противоречат друг другу).\n",
    "\n",
    "На практике, например в обучении регрессий, этот случай практически не встречается.\n",
    "\n",
    "Что касается переопределённых систем, то в них, помимо несовместности (отсутствия решений), количество независимых уравнений превышает количество неизвестных — это тот самый случай, что мы видим в регрессионном анализе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее мы рассмотрим каждый из случаев на примере.\n",
    "\n",
    "## СЛУЧАЙ «ИДЕАЛЬНАЯ ПАРА»\n",
    "\n",
    "> Самый простой случай решения неоднородной СЛАУ — когда система имеет единственное решение. Такие системы называются **совместными**.\n",
    "\n",
    "На вопрос о том, когда СЛАУ является совместной, отвечает главная теорема СЛАУ — **теорема Кронекера — Капелли** (также её называют **критерием совместности системы**).\n",
    "\n",
    "> **Теорема Кронекера — Капелли:**\n",
    "> \n",
    "> Неоднородная система линейный алгебраических уравнений $A \\vec{w} = \\vec{b}$ является совместной тогда и только тогда, когда ранг матрицы системы $A$ **равен** рангу расширенной матрицы системы $(A| \\vec{b})$ и равен количеству независимых переменных $m$:\n",
    "> \n",
    "> $$rk(A) = rk(\\vec{b}) = m \\leftrightarrow \\exists ! \\vec{w} = (w_{1}, w_{2}, \\ldots w_m)^T$$\n",
    ">\n",
    "> Причём решение системы будет равно:\n",
    "> \n",
    "> $$\\vec{w} = A^{-1} \\vec{b}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Здесь значок $\\exists !$ переводится как «существует и причём единственное».\n",
    "\n",
    "Сложно и непонятно? Давайте разберёмся, как работает эта теорема, на примерах ↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Дана СЛАУ:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/cadd1684cd0b6d152ce5a1a784fc8973/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_8.png)\n",
    "\n",
    "*где $w_1$ и $w_2$ — неизвестные переменные.*\n",
    "\n",
    "При решении системы «в лоб» получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/37c7d45c0a88651cd57dd05b085adb78/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_9.png)\n",
    "\n",
    "Интерпретация:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5191ad698399cde0da309f8deea15cb6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_10.png)\n",
    "\n",
    "На языке линейной алгебры это означает что вектор $(1,2)^T$ линейно выражается через векторы коэффициентов системы $(1,1)^T$ и $(1,2)^T$.\n",
    "\n",
    "В матричном виде система запишется, как:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a1e86f40406ca7904279ac19a60844b0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразование уравнений будем таким же, как и при преобразовании расширенной матрицы системы $(A|\\vec{b})$, вычитая сначала первую строку из второй, а затем — результат из первой, получим то же решение, что и решение «в лоб».\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d0df10f19deeb809ba7cbf27264f85a9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_12.png)\n",
    "\n",
    "Других решений у системы нет. \n",
    "\n",
    "Посмотрим на ранги матрицы $A$ и расширенной матрицы $(A|\\vec{b})$ (количество ступеней в ступенчатых матрицах):\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/da90d08d05f922039ad20d58aa5aff45/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_13.png)\n",
    "\n",
    "Они совпадают и равны количеству неизвестных, а это и гарантирует существование и единственность решения. То есть в общем случае, чтобы узнать, сколько решений существует у системы, её необязательно было бы решать — достаточно было бы найти ранги матриц $A$ и $A|b$.\n",
    "\n",
    "Тут возникает вопрос: «Можно ли найти решение одной формулой?»\n",
    "\n",
    "Для удобства перепишем систему без стрелок:\n",
    "\n",
    "$Aw = b$\n",
    "\n",
    "Так как матрица квадратная и невырожденная, у неё обязательно есть обратная матрица."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умножим на $A^{-1}$ слева обе части уравнения. Стоит напомнить, что произведение матриц не перестановочно, поэтому есть разница, с какой стороны умножать.\n",
    "\n",
    "$$A^{-1} Aw = A^{-1}b$$\n",
    "\n",
    "$$w = A^{-1} b$$\n",
    "\n",
    "\n",
    "**Важно**! Отсюда явно видны **ограничения** этого метода: его можно применять только для квадратных невырожденных матриц (тех, у которых определитель не равен 0).\n",
    "\n",
    "Убедимся в правильности формулы. Найдём произведение матрицы $A^{-1}$ и вектора-столбца $b$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/207d07ac7518f16e548924a9bc2e3382/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_14.png)\n",
    "\n",
    "Меняем местами главную диагональ и меняем знаки у побочной. Как видим, результат совпал с прошлым методом.\n",
    "\n",
    "**Резюмируем ↓**\n",
    "\n",
    "У нас есть квадратная система с $m$ неизвестных. Если ранг матрицы коэффициентов $A$ **равен** рангу расширенной матрицы $(A|b)$ и **равен** количеству переменных ($rk(A)=rk(\\vec{b})=m$), то в системе будет ровно столько независимых уравнений, сколько и неизвестных $m$, а значит будет **единственное** решение.\n",
    "\n",
    "Вектор свободных коэффициентов $b$ при этом линейно независим со столбцами матрицы $A$, его разложение по столбцам $A$ единственно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.4\n",
    "\n",
    "*Решите систему линейных уравнений:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bb410f21af5bc73df165cf79d3623d37/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_23.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[x y] =  [-2.  4.]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[4, 7], [5, 10]])\n",
    "b = np.array([20, 30])\n",
    "A_inv = np.linalg.inv(A)\n",
    "print('[x y] = ', A_inv@b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «В АКТИВНОМ ПОИСКЕ»\n",
    "\n",
    "А что, если система не удовлетворяет теореме Кронекера — Капелли? То есть ранг матрицы системы равен расширенному рангу матрицы, но не равен количеству неизвестных. Неужели тогда система нерешаема?\n",
    "\n",
    "На этот вопрос отвечает первое следствие из теоремы ↓\n",
    "\n",
    "> **Следствие №1 из теоремы Кронекера — Капелли:**\n",
    "> \n",
    "> Если ранг матрицы системы $A$ равен рангу расширенной матрицы системы $(A|\\vec{b})$, но меньше, чем количество неизвестных $m$, то система имеет бесконечное множество решений:\n",
    "> \n",
    "> $$rk(A) = rk(A | \\vec{b}) < m  \\leftrightarrow  \\infty \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вновь рассмотрим пример."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Решим систему уравнений:*\n",
    "\n",
    "$w_1 + w_2 + w_3 = 10$\n",
    "\n",
    "Да, уравнение одно, но формально оно является неоднородной СЛАУ.\n",
    "\n",
    "Итак, мы имеем одно уравнение на три неизвестных, значит две координаты из трёх вектора $w$ мы можем задать как угодно. Например, зададим вторую и третью как $\\alpha$ и $\\beta$. Тогда первая будет равна $10 - \\alpha - \\beta$.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/426d121f84b92a0d96b78bd777ebd44d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_30.png)\n",
    "\n",
    "Вместо переменных $\\alpha$ и $\\beta$ мы можем подставлять любые числа и всегда будем получать равенство. \n",
    "\n",
    "Составим расширенную матрицу:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/81cc95b153a93a58dc3c517470779345/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_31.png)\n",
    "\n",
    "Её ранг, как и ранг $A$, равен 1, что меньше числа неизвестных $m=3$:\n",
    "\n",
    "$rk(A) = rk(A | \\vec{b}) = 1 < 3$\n",
    "\n",
    "Такая ситуация, по следствию из теоремы Кронекера — Капелли, говорит о существовании и не единственности решения, то есть решений бесконечно много.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранги матриц $A$ и $(A|\\vec{b})$ всё ещё совпадают, но уже меньше количества неизвестных ($rk(A) = rk(A | \\vec{b}) < m$), значит, уравнений не хватает для того, чтобы определить систему полностью, и решений будет бесконечно много.\n",
    "\n",
    "На языке линейной алгебры это значит, что вектор $\\vec{b}$ линейно зависим со столбцами матрицы $A$, но также и сами столбцы зависимы между собой, поэтому равнозначного разложения не получится, т. е. таких разложений может быть сколько угодно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СЛУЧАЙ «ВСЁ СЛОЖНО»\n",
    "\n",
    "А теперь посмотрим на самый интересный для нас случай. Его формально регламентирует второе следствие из теоремы Кронекера — Капелли.\n",
    "\n",
    "> Следствие №2 из теоремы Кронекера — Капелли:\n",
    "> \n",
    "> Если ранг матрицы системы $A$ меньше, чем ранг расширенной матрицы системы $(A|\\vec{b})$, то система несовместна, то есть не имеет точных решений:\n",
    "> \n",
    "> $$rk(A)  < rk(A | \\vec{b})  \\leftrightarrow  \\nexists \\ решений$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Решим систему уравнений:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/abcb754bb9f9181e93b95b49e77ff25c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_42.png)\n",
    "\n",
    "Посмотрим на первое и третье уравнение — очевидно, что такая система не имеет решений, так как данные уравнения противоречат друг другу.\n",
    "\n",
    "Но давайте обоснуем это математически. Для этого запишем расширенную матрицу системы:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/9f9d4dbee7564d18fa820c0a54a3785e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_43.png)\n",
    "\n",
    "Посчитаем ранги матриц $A$ и $A|\\vec[b$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2774b7f6264958ea7c7d6f23df959a77/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_44.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, $rk(A)=2$, в то время как $rk(A|\\vec(b))=3$. Это и есть **критерий переопределённости** системы уравнений: ранг матрицы системы меньше ранга расширенной матрицы системы.\n",
    "\n",
    "→ Получается, что идеальное решение найти нельзя, но чуть позже мы увидим, что такие системы возникают в задачах регрессии практически всегда, а значит нам всё-таки хотелось бы каким-то образом её решать. Можно попробовать найти приблизительное решение — вопрос лишь в том, какое из всех этих решений лучшее.\n",
    "\n",
    "?\n",
    "\n",
    "Найдем наилучшее приближение для $w_1$, $w_2$, если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/4267dd365e0b367f66729d1cf0eb4b5e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_45.png)\n",
    "\n",
    "Обозначим приближённое решение как $\\hat{w}$. Приближением для вектора $b$ будет $\\hat{b}=A\\hat{w}$. Также введём некоторый вектор ошибок $e = b - \\hat{b} = b - A \\hat{w}$.\n",
    "\n",
    "**Примечание**. Здесь мы снова опустили стрелки у векторов $b$, $\\hat{b}$ и $\\hat{w}$ для наглядности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, если мы возьмём в качестве вектора $\\hat{w}$ вектор $\\hat{w}_1=(1, 1)^T$, то получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a9f15e57bf47fd7db7b10f9986d878a1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_46.png)\n",
    "\n",
    "Теперь возьмём в качестве вектора $\\hat{w}_2 = (4, -1)^T$, получим:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/cc797ce965e4cfa42340afcc97494476/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_47.png)\n",
    "\n",
    "→ Конечно, нам хотелось бы, чтобы ошибка была поменьше. Но какая из них поменьше? Векторы сами по себе сравнить нельзя, но зато можно сравнить их длины.\n",
    "\n",
    "Для первого случая будем иметь:\n",
    "\n",
    "$\\left\\|e_1 \\right\\| = \\sqrt{(-1)^2 + (-1)^2 + (10)^2} = \\sqrt{102}$\n",
    "\n",
    "Для второго случая:\n",
    "\n",
    "$\\left\\|e_2 \\right\\| = \\sqrt{(-2)^2 + 0^2 + 9^2} = \\sqrt{85}$\n",
    "\n",
    "Видно, что вторая ошибка всё-таки меньше, соответственно, приближение лучше. Но в таком случае из всех приближений нам нужно выбрать то, у которого длина вектора ошибок минимальна, если, конечно, это возможно.\n",
    "\n",
    "$\\left\\|e \\right\\| \\rightarrow min$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернёмся к задаче поиска оптимальных приближений вектора $\\hat{w}$.\n",
    "\n",
    "**Примечание**. Проблема поиска оптимальных приближённых решений неоднородных переопределённых СЛАУ стояла у математиков вплоть до XIX века. До этого времени математики использовали частные решения, зависящие от вида уравнений и размерности. Впервые данную задачу для общего случая решил Гаусс, опубликовав метод решения этой задачи, который впоследствии будет назван методом наименьших квадратов (МНК). В дальнейшем Лаплас прибавил к данному методу теорию вероятности и доказал оптимальность МНК-оценок с точки зрения статистики.\n",
    "\n",
    "Сейчас мы почувствуем себя настоящими математиками и попробуем решить эту задачу самостоятельно с помощью простой геометрии и знакомых нам операций над матрицами.\n",
    "\n",
    "Вспомним, что на языке линейной алгебры неразрешимость системы\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/02f4b4ff440bbe548de99fd21d758ef2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_48.png)\n",
    "\n",
    "означает, что попытка выразить вектор $(1, 2, 12)^T$ через $(1, 1, 1)^T$ и $(1,2,1)^T$ не будет успешной, так как они линейно независимы.\n",
    "\n",
    "**Геометрически** это означает, что вектор свободных коэффициентов  (коричневый) не лежит в одной плоскости со столбцами матрицы  (синие векторы)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2448deb8bce151ef64ddb640a1fa57db/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_49.png)\n",
    "\n",
    "→ Напомним, что подобную задачу мы решали в предыдущем модуле по линейной алгебре, в юните «Практика: векторы». Вы можете вернуться в предыдущий модуль и освежить в памяти решение задачи.\n",
    "\n",
    "Идея состояла в том, что наилучшим приближением для коричневого вектора будет ортогональная проекция на синюю плоскость — голубой вектор. Так происходит потому, что наименьший по длине вектор ошибок — красный — должен быть перпендикулярен к синей плоскости:\n",
    "\n",
    "$$e=b=\\hat{b}$$\n",
    "\n",
    "В прошлом модуле мы производили расчёты интуитивно, а теперь настала пора вывести формулу.\n",
    "\n",
    "Давайте умножим наши уравнения слева на $A^T$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f6cba5210f0383927c24de2da993db79/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_50.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея заключается в следующем: справа мы найдём скалярное произведение столбцов матрицы $A$ на вектор $b$, а слева — произведение столбцов $A$ на приближённый вектор $\\hat{b}$ (по сути, на голубую проекцию).\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/04f3d1836f3f2e04db6db6dad688862a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_71.png)\n",
    "\n",
    "Упростим уравнение, перемножив всё, что содержит только числа. В левой части умножим $A^T$ на $A$, в правой — умножим $A^T$ на $b$. Тогда слева получим матрицу 2×2 — это не что иное, как матрица Грама столбцов $A$.\n",
    "\n",
    "Столбцы $A$ линейно независимы, а это значит, что, по свойству матрицы Грама, $A^T \\cdot A$  — невырожденная квадратная матрица (её определитель не равен нулю, и для неё существует обратная матрица). Получившаяся система — один в один случай «идеальная пара», а это значит, что теперь мы можем её решить.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/227fe95c6711c25be991e9c81e9c0d93/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_51.png)\n",
    "\n",
    "?\n",
    "Но ведь мы не могли решить изначальную задачу, так как она была переопределена, а эту — можем. Как так получилось?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы потребовали, чтобы у приближения $\\hat{b}$ были с векторами $(1,1,1)^T$ и $(1,2,1)^T$ такие же скалярные произведения, как у $b$. Это и означает что $\\hat{b}$ — ортогональная проекция на нашу синюю плоскость, в которой лежат столбцы матрицы $A$, и в этой плоскости мы можем найти коэффициенты.\n",
    "\n",
    "Мы с вами отлично умеем решать системы типа «Идеальная пара». Для этого нам нужно найти обратную матрицу  и умножить на неё слева всё уравнение. Так мы и получим наше приближение:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/926b933be4ea80370273f21776528ca2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_52.png)\n",
    "\n",
    "Находим определитель матрицы $(A^TA)$:\n",
    "\n",
    "$det(A^T A) = 3 \\cdot 6 - 4 \\cdot 4 = 2$\n",
    "\n",
    "Находим обратную матрицу (A^TA)^{-1}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/0a77b6e3a1603d68db6cda0ebca99758/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_53.png)\n",
    "\n",
    "Умножаем всё уравнение на обратную матрицу слева:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/72ad70cfed4736faeba2e57ace2fa47d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, вот он — долгожданный приближённый вектор $\\hat{w}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/456b762ff60c55233bd071d5da87ebfd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_55.png)\n",
    "\n",
    "Ещё раз посмотрим на финальную формулу:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/d22d5f53531f1b98a5c2608008f670dd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_56.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐ Пришло время открытий!\n",
    "\n",
    "Только что мы геометрическим образом вывели формулу оценки решения методом наименьших квадратов (МНК или OLS, Ordinary Least Squares).\n",
    "\n",
    "**Примечание**. Стоит отметить, что полученная матричная формула не зависит от размерностей и конкретных значений, а значит применима не только в нашем локальном случае, но и в общем.\n",
    "\n",
    "Нам осталось выполнить проверку полученных результатов, чтобы убедиться в верности решения.\n",
    "\n",
    "Вычислим голубой вектор $\\hat{b}$. Для этого возьмём линейную комбинацию столбцов матрицы $А$ с найденными нами коэффициентами $\\hat{w_1}$ и $\\hat{w_2}$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/18d29c611a2228d28dfd178039e199b0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_57.png)\n",
    "\n",
    "Вычислим вектор ошибок $e$:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1674251d755c8b031c983072f6557e00/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_58.png)\n",
    "\n",
    "Убедимся, что данный вектор действительно ортогонален столбцам матрицы $А$. Для этого найдём их скалярные произведения:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/583371dd14e680cce62a7c551fe859ee/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_59.png)\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dffc86afc7e86faa61d314c567027151/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_60.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скалярные произведения равны 0, а это означает, что вектор ошибок $e$ действительно ортогонален всей синей плоскости, а голубой вектор $b$ приближённого значения является ортогональной проекцией коричневого вектора $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Примечание**. Прежде чем перейти к выводам, стоит отметить, что обычно OLS-оценку выводят немного иначе, а именно минимизируя в явном виде длину вектора ошибок по коэффициентам $\\hat{w}$, вернее, даже квадрат длины для удобства вычисления производных.\n",
    "> \n",
    "> $$\\left\\|\\vec{e} \\right\\| \\rightarrow min$$\n",
    "> \n",
    "> $$\\left\\|\\vec{e} \\right\\|^2 \\rightarrow min$$\n",
    "> \n",
    "> $$\\left\\|\\vec{b} - A \\vec{w} \\right\\|^2 \\rightarrow min$$\n",
    "> \n",
    "> Формула получится точно такой же, какая есть у нас, просто способ вычислений будет не геометрический, а аналитический. Мы вернёмся к этому способу, когда будем обсуждать оптимизацию функции многих переменных в разделе по математическому анализу.\n",
    "> \n",
    "> Наконец, мы может подвести итоги для случая «Всё сложно»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Резюмируем ↓**\n",
    "\n",
    "Если ранг матрицы $A$ меньше ранга расширенной системы $(A|\\vec{b})$, то независимых уравнений больше, чем переменных $(rkA<(A|\\vec{b})<m)$, а значит некоторые из них будут противоречить друг другу, то есть решений у системы нет.\n",
    "\n",
    "Говоря на языке линейной алгебры, вектор $b$ линейно независим со столбцами матрицы $A$, а значит его нельзя выразить в качестве их линейной комбинации.\n",
    "\n",
    "Однако можно получить приближённые решения по методу наименьших квадратов ($OLS-оценка - \\hat{b} = (A^{T}A)^{-1}\\cdot A^{T} b$), идеей которого является ортогональная проекция вектора  на столбцы матрицы $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.9\n",
    "\n",
    "Вычислите вектор ошибок для приближённого решения системы , если:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e5455f82f2c719a89defd6fbf935bf99/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_61.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, -1,  0])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,-5],[2,1],[1,1]])\n",
    "b = np.array([1,2,2])\n",
    "w_hat = np.array([1,1])\n",
    "e = b - A@w_hat\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.11\n",
    "\n",
    "*Найдите $OLS$-оценку для коэффициентов $w_1$, $w_2$ СЛАУ:*\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/aabe72a5dc4493bf7f00fce4be816414/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md2_2_63.png)\n",
    "\n",
    "*Для этого выполните задания под цифрами 1-4 ниже.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A : \n",
      " [[ 1  2]\n",
      " [-3  1]\n",
      " [ 1  2]\n",
      " [ 1 -1]] \n",
      " b :  [1 4 5 0]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1,2],[-3,1],[1,2],[1,-1]])\n",
    "b = np.array([1,4,5,0])\n",
    "print(' A : \\n',A, '\\n', 'b : ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Вычислите матрицу Грама столбцов $A:A^{T}A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12,  0],\n",
       "       [ 0, 10]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Вычислите матрицу $(A^{T}A)^{-1}$. Она имеет вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08333333, 0.        ],\n",
       "       [0.        , 0.1       ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(A.T@A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Вычислите $A^{T} \\vec{b}$. Введите координаты полученного вектора через запятую, без пробелов. Пример ввода ответа: 1,1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6, 16])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T@b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Вычислите вектор оценок коэффициентов $\\vec{v}$.\n",
    "\n",
    "Примечание: для корректного ответа не округляйте $(A^TA)^{-1}$.\n",
    "\n",
    "Чему равен полученный вектор коэффициентов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  1.6])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_hat = np.linalg.inv(A.T@A)@A.T@b\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Линейная регрессия по методу наименьших квадратов\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950b5653ccfc34417735dd321d006fd482b31f7611416c3d8236dc5b17587d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
