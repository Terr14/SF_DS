{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DL-6. Введение в NLP и рекуррентные сети**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. Введение. Задачи NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом модуле мы поговорим об обработке естественного языка, то есть обо всём, что включает в себя работу с текстом, представлением слов, темами, документами, корпусами и т.д.\n",
    "\n",
    "**Обработка естественного языка** или иначе **NLP — Natural Language Processing**.\n",
    "\n",
    "**Задачи NLP:**\n",
    "\n",
    "* классификация текстов;\n",
    "* анализ эмоционального настроя;\n",
    "* суммаризация текстов;\n",
    "* поиск;\n",
    "* распознавание речи;\n",
    "* автоматический перевод;\n",
    "* чат-боты / диалоговые системы / голосовые помощники;\n",
    "* генерация текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Векторизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перед тем как переходить к самим сетям, поговорим о предварительной обработке текста в \"понятный\" для машин формат.**\n",
    "\n",
    "Сразу возникает проблема, как извлечь признаки из текста. Ведь для того, чтобы алгоритм машинного обучения мог работать с текстом, на входе он должен получить формализованные данные: векторы, тензоры и т.п. Появляется задача: извлечь изначальные признаки из текста. Делать это можно разными способами, но базовым является **векторизация**.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1867c39ee30187ba0d73db1f799f7a63/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-2_%D0%A0%D0%B8%D1%81%D1%83%D0%BD%D0%BE%D0%BA1.png)\n",
    "\n",
    "Существует множество различных подходов к векторизации текcтов, например:\n",
    "\n",
    "* WORD-2-VEC\n",
    "* GLOVE\n",
    "* TF-IDF\n",
    "* множество их модификаций\n",
    "\n",
    "**Современные комплексные сети проводят векторизацию уже самостоятельно.** Вам достаточно знать, что данный шаг существует и на текущий момент просто стал одним из гиперпараметров, позволяющим улучшить качество вашей модели. Но не забываем про очистку данных, которую никто не отменял.\n",
    "\n",
    "**Дополнительные материалы**\n",
    "\n",
    "Более подробно о векторизации и методах можно почитать в статьях на habr.com:\n",
    "\n",
    "* [Как решить 90% задач NLP: пошаговое руководство по обработке естественного языка](https://habr.com/ru/company/oleg-bunin/blog/352614/)\n",
    "* [Основы Natural Language Processing для текста](https://habr.com/ru/company/Voximplant/blog/446738/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Нейронные сети в NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом модуле мы поговорим о **нейронных сетях для решения задач обработки естественного языка (NLP)**.\n",
    "\n",
    "Речь пойдёт не только о рекуррентных нейронных сетях, но и о других типах, более современных, которые также используются в NLP. Начнём с повторения уже известного нам материала:\n",
    "\n",
    "**Нейронные сети прямого распространения**\n",
    "\n",
    "К этому классу можно отнести все сети, рассмотренные нами ранее. В таких полносвязных нейронных сетях на вход подаётся вектор и на выходе также получается вектор, т.о. информация распространяется только в одном направлении от входа к выходу по принципу «один объект на входе — один объект на выходе» и на этом обработка заканчивается.\n",
    "\n",
    "Обычная полносвязная нейронная сеть состоит из:\n",
    "\n",
    "* входного вектора X;\n",
    "* матриц весов B1, W1;\n",
    "* вектора скрытого состояния H;\n",
    "* иных матриц B2, W2;\n",
    "* выходного вектора Y.\n",
    "\n",
    "Таким образом, имеем простое линейное преобразование с некоторой нелинейной функцией H.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ffcb208a24e250fbc27d0a2eedbe818b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp1.png)\n",
    "\n",
    "В общем случае можем представить этот процесс как многослойную нейронную сеть\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/db31398133def3ec5d2c5e5f82996129/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сами слои — линейные преобразования, например, умножение на матрицу и взятие функции активации — удобно обозначать на рисунке стрелочками. Далее будем использовать обозначение: **стрелочка = слой**.\n",
    "\n",
    "## BACKPROPAGATION\n",
    "\n",
    "Нейронные сети такого типа обучаются с помощью обратного распространения ошибки (BACKPROPAGATION). Входной объект мы прогоняем через нейронную сеть и получаем выход, по которому оцениваем ошибку и по ней считаем градиент и применяем его к параметрам. Градиент ошибки считается по всем параметрам модели, параметрам  и весам каждого слоя. Необходимо понять, как зависит ошибка от параметром того или иного слоя. Для этого используется правило дифференцирования сложной функции.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/556d3bd7e68c2ab7f4a63b92d88b2a06/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp3_1.png)\n",
    "\n",
    "## Обработка последовательностей\n",
    "\n",
    "Для обработки последовательностей можно использовать нейронные сети прямого распространения и первое, что нужно сделать для этого — извлечь векторные представления слов и далее каждый вектор прогнать через нейронную сеть, получить ответ. Основная проблема в таком алгоритме — информация во времени никак не связана, слова подаются последовательно , но нет никакой информации о том, что было перед текущим словом или после текущего слова. Можно решить эту проблему, пройдя свёрточной нейронной сетью по входным векторам, уловив локальные зависимости.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/18da37c17e96ad0b758459431807205e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Рекуррентные нейронные сети\n",
    "\n",
    "Рекуррентные нейронные сети — это мощный способ обработки последовательностей. Основная идея рекуррентных сетей в том, что информация накапливается во внутреннем представлении нейронной сети. Таким образом, прогон элемента уже имеет какое-то представление о всей предыдущей последовательности.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/68b1225f83df355fb446d2f13af378e8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Рекуррентные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Граф, соответствующий простой рекуррентной нейронной сети, можно представить в следующем виде:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/515902d9783b2726bd781797e95755f1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp6_1.png)\n",
    "\n",
    "Помним, что стрелочками обозначены линейные слои (например, умножение на матрицу, сложение с вектором смещения или взятие функции активации). Здесь добавляется стрелка, обозначающая соединение вектора $H_t$ с самим собой в разные моменты времени. То есть **вычисление каждого $H_t$ будет зависеть не только от $X_t$, но и от $H_{t-1}$**. Поэтому при вычислении $H_t$ добавляется дополнительное слагаемое $W_hH_{t-1}$  — произведение матрицы весов на **вектор промежуточного скрытого состояния** с предыдущего момента времени. Таким образом в векторе H накапливается информация о всей последовательности.\n",
    "\n",
    "## UNFOLDING\n",
    "\n",
    "Полезно рассматривать такую сеть в виде развёрнутой нейронной сети — такая операция называется **UNFOLDING**.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/893db96ea4ceaf871cbf1f4868078403/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом представлении (см. рисунок) сеть с циклической зависимостью **представляется в виде обычной сети прямого распространения** с несколькими входными и выходными векторами. Но важное отличие от абстрактной сети прямого распространения в том, что параметры этой модели одни и те же в разные моменты времени. То есть все копии нейронной сети по моментам времени — это одна и та же нейронная сеть, одни и те же параметры. По этой последовательности прогоняем входной элемент и последовательно получаем выходные значения.\n",
    "\n",
    "## Как обучать такого рода сеть?\n",
    "\n",
    "Рассмотрим пример: пусть есть последовательность на входе и для простоты такая же последовательность на выходе (такого же размера и с некоторым соответствием «один в один»: X1 в Y1 и т. д.).\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/893db96ea4ceaf871cbf1f4868078403/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp7.png)\n",
    "\n",
    "На каждый выходной элемент последовательности мы можем навесить Loss (функцию потерь). То есть выход из нейронной сети мы хотим «притянуть» к известному нам правильному значению. Loss зависит от параметров сети, которые в свою очередь зависят от X1. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/2f03a26cb5e80246b4bc8b2496f2fbe7/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть, если мы применяем обратное распространение ошибки, градиент будет протекать от Loss к X1. От остальных элементов последовательности ошибка и градиент никак не зависят. Поэтому это эквивалентно обратному распространению ошибки в обычных нейронных сетях.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d2148fd65df07735c27ee5d3ac6aa25e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp9.png)\n",
    "\n",
    "Однако, если мы хотим посчитать функцию потерь, например, для третьего элемента, то возникнет более сложная зависимость и Loss будет зависеть не только от X3, но и от X1 и X2.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/c389eb22613e51ea292e6937a63969fd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp10.png)\n",
    "\n",
    "Таким образом, происходит не просто обратное распространение ошибки по слоям, а добавляется временнОе измерение. Градиент необходимо протаскивать во время, в те позиции, от которых зависит финальная ошибка. Такой алгоритм будет называться Backpropagation through time («обратное распространение ошибки сквозь время»). Градиент как бы растекается по двум направлениям: по сети (по её глубине) и по времени назад (в случае, если сеть однонаправленная).\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/dd489bc2034ae39f95c341aba5d0a2b2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp11.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Проблемы алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный в предыдущем разделе алгоритм простых рекуррентных нейронных сетей уже хорош для анализа последовательностей, но у него есть некоторые проблемы:\n",
    "\n",
    "1. **Проблема с градиентами**. Если цепочка (предложение) является очень длинной по времени, то для того, чтобы вычислить градиент ошибки по последнему слову в этой последовательности, пройти придётся по всем предыдущим элементам цепи. При подсчёте градиентов возникнет многократное перемножение одной и той же матрицы. В зависимости от масштаба этой матрицы возведение её в степень будет давать либо очень маленькую, либо очень большую матрицу на выходе (зависит от нормы матрицы). Эти случаи называются **затухающим** или **взрывающимся градиентом**. Есть различные способы, как с этим можно бороться: обрезание градиентов в процессе обучения, например, а также построение более продвинутых рекуррентных нейронных сетей.\n",
    "2. **Простая рекуррентная нейронная сеть мало способна изучать очень длинные последовательности.** То есть информация может потихонечку пропадать, стираться, и это никак не контролируется. Вектор промежуточных значений H обновляется в каждый момент времени, и информация потихонечку забывается. Поэтому хранить информацию о длинных последовательностях затруднительно. К концу обработки длинного предложения информация о первом слове уже будет практически забыта.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/dd489bc2034ae39f95c341aba5d0a2b2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp11.png)\n",
    "\n",
    "Решают первую и вторую проблему **продвинутые архитектуры нейронных сете**й."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Продвинутые рекуррентные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простая ячейка RNN\n",
    "\n",
    "Ещё раз вспомним, как выглядит обычная рекуррентная нейронная сеть. Рассмотрим её ячейку в представлении, которое далее понадобится для построения более сложных рекуррентных нейронных сетей:\n",
    "\n",
    "$x_t$ — входной элемент;\n",
    "\n",
    "$h_t-1$ — вектор промежуточного значения в предыдущий момент времени;\n",
    "\n",
    "$tanh$ (гиперболический тангенс) — функция активации;\n",
    "\n",
    "$h_t$ — вектор промежуточного значения.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/00a3f2bda2a8bec8d3517315dd334620/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp12_1.png)\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/59691af8acc5b2ae5460d0cc45ae07f6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp13.png)\n",
    "\n",
    "$h_t$ подаём либо вверх на следующий слой, либо дальше в следующий момент времени. \n",
    "\n",
    "# LONG SHORT-TERM MEMORY (LSTM)\n",
    "\n",
    "Сложная ячейка LSTM позволяет улучшить вышеупомянутый алгоритм. Как правило, функции, содержащиеся в этой ячейке используются как «чёрные ящики» — встроенные готовые модули. Содержимое такой ячейки никак не модифицируется. \n",
    "\n",
    "Основная идея LSTM-ячейки состоит в том, что кроме вектора промежуточного состояния $h_t$, который никак не контролируется, есть вектор $c_t$, который является памятью ячейки и задан явным образом.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/7481b7d468e7e6f636e0f98c14f963aa/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp14_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для программирования памяти ячейки используются контролируемые гейты, записывающие и считывающие информацию из ячейки. В LSTM-ячейках есть три гейта:\n",
    "\n",
    "* **Forget gate** зависит от входных данных, обучаем и представляет собой маленькую нейронную сеть. Forget gate сообщает, какую информацию в функции памяти следует забыть.\n",
    "* **Input gate** сообщает, что в итоге должно пойти в память, какую именно долю нового значения нужно записать.\n",
    "* **Output gate** сообщает, как нужно вычислить финальное значение h.\n",
    "\n",
    "Схема LSTM довольно сложная и требует некоторого времени для понимания. Данная схема избавляет от необходимости обрабатывать только короткие цепочки. Благодаря тому, что нейронная сеть сама контролирует, что можно забыть, мы можем обрабатывать длинные цепочки. Проблема с градиентами в этой схеме тоже решается.\n",
    "\n",
    "# GATED RECURRENT UNIT (GRU)\n",
    "\n",
    "GRU представляет собой похожий на LSTM тип ячеек. Схема в ней более упрощённая. Здесь так же есть гейты, но input и forget gate слились в один. Вектор памяти и вектор h также слились вместе. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/0f6f52e939fe0e6fdc610c08e57a8fcd/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/nlp15.png)\n",
    "\n",
    "**Основное, что нужно запомнить:** из LSTM- и GRU-ячеек строятся все современные рекуррентные нейронные сети. На этих базовых элементах основываются более сложные модели. Эти ячейки будем использовать как готовые блоки, не вдаваясь в подробности того, как внутри всё это работает."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дополнительные материалы\n",
    "\n",
    "* В качестве дополнительной литературы рекомендуем вам прочесть статью «[LSTM – сети долгой краткосрочной памяти](https://habr.com/ru/company/wunderfund/blog/331310/)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Типы обработки последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под слоем сети теперь будем понимать LSTM или GRU ячейки. Рассмотрим, какие рекуррентные нейронные сети можно  на их основе строить.\n",
    "\n",
    "**Many-to-One**: в нейронную сеть подаются входные элементы, которые считываются и протаскиваются во времени. На выходе — один элемент.\n",
    "\n",
    "**One-to-Many**: один элемент на входе (скаляр, матрица или тензор) разворачивается в сети в последовательность, выдавая на выходе один за другим выходные элементы.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3452178a4ff4101b9a28179c58dcac95/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_1_1.png)\n",
    "\n",
    "**Many-to-Many** (Sync): каждому из нескольких элементов на входе соответствует выходной элемент.\n",
    "\n",
    "**Many-to-Many** (Encoder-Decoder): входная и выходная цепочка могут не совпадать по длине. Идея в том, что сначала обрабатывается входная цепочка, последовательно считывается и протаскивается во времени. А далее финальный код декодируется и разворачивается в новую цепочку.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ffb4bc24fc6035a1539ac063a7f01297/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_2_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Тип MANY-TO-ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим несколько примеров обработки последовательностей типом Many-to-One.\n",
    "\n",
    "## Классификация текстов\n",
    "\n",
    "Для примера рассмотрим анализ эмоционального окраса комментария или текста (нейтральный, негативный или позитивный).\n",
    "\n",
    "Получив векторные представления слов, подаём их в рекуррентную нейронную сеть, где информация о последовательности накапливается. В конце сети информация о том, что собой представляет входной текст, уже сформирована. На неё навешивается классификатор.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1a2ce7062d8ab3e95cefb3aabc2e58ce/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_3_1.png)\n",
    "\n",
    "## Анализ временных рядов\n",
    "\n",
    "Рассмотрим пример предсказания следующего элемента последовательности (временного ряда).\n",
    "\n",
    "В нейронную сеть подаются векторные представления всех элементов последовательности, предшествующих искомому. Полученное на выходе предсказание трактуется, как искомое значение элемента.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/0e96b292f5bb560a4a49fd8b2389e069/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Текст → изображение\n",
    "\n",
    "Рассмотрим пример отображения текста в картинку.  Идея остаётся той же: на входе последовательность, на выходе скаляр. В этом случае не обойтись без свёрточных нейронных сетей.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/85ffa8e582f880ed3c6bcc609bc86c47/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_5.png)\n",
    "\n",
    "Из текста извлекаются векторные представления и подаются в рекуррентную нейронную сеть. Эта информация кодируется на входе. На выходе получаем одно значение или вектор. Далее этот вектор рассматривается как признаковое представление всего текста. Задача: сгенерировать картинку из этого представления. Сделать это можно с помощью генеративной сети, принцип работы которой будет похож на свёрточную сеть, которая используется для классификации, но в обратном порядке: из признакового представления текста получаем картинку. Для улучшения качества работы такого метода можно использовать генеративные состязательные сети.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3ea4bc6bc4ca7fd2cd13ccabdb16d701/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_6.png)\n",
    "\n",
    "**Подытожим**: часто в задачах с типом обработки последовательности Many-to-One на вход подаётся текст или последовательность, а на выходе либо финальный ответ, либо признаковое представление (вектор), которое соответствует всему входному тексту."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Тип ONE-TO-MANY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К типу обработки последовательностей One-to-Many относится генерация текстов. \n",
    "\n",
    "## Генерация текстов\n",
    "\n",
    "Сразу заметим, что One-to-Many не является оптимальной архитектурой для генерации последовательностей. Как мы помним, в любых рекуррентных нейронных сетях в каждый момент времени есть значение скрытого слоя с предыдущего  момента времени. Также на вход подаётся входной объект. А когда на вход ничего не идет, как в данном случае, на второй момент времени, например, можно подавать нулевой вектор и использовать в качестве информации только скрытый слой с предыдущей итерации.\n",
    "\n",
    "Улучшить этот способ можно следующим образом: на вход подать объект не как входной, а как предыдущее значение промежуточного слоя, то есть «сбоку». Тогда снизу, как вход, подадим индикатор начала последовательности.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/9f0b7ff1fa251ee968a91a650f2b84b7/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_7.png)\n",
    "\n",
    "Полученное на первом шаге предсказание Y1 подаётся на вход в нейронную сеть и т.д. с последующими предсказаниями. Конец такой последовательности кодируется при помощи специального токена.\n",
    "\n",
    "Такая архитектура является классической."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Изображение → Текст (CAPTIONING)\n",
    "В отличие от классификации картинок в этом примере необходимо сгенерировать текст на естественном языке. За эту задачу отвечают рекуррентные нейронный сети. Разберём пошагово пример технологии Image Caption:\n",
    "\n",
    "1. Берём картинку, свёрточную нейронную сеть и извлекаем признаки.\n",
    "1. Признаковое представление картинки подаём в рекуррентную нейронную сеть. Строить выходную цепочку будем последовательно, как рассматривали в примере выше. С помощью токена задаём начало цепочки, а признаковое представление картинки подадим в качестве предыдущего значения скрытого слоя.\n",
    "1. Используя оба значения, поданных в сеть, предсказываем первое слово.\n",
    "1. Признаковое представление, скрытую информацию цепочки и сгенерированное первое слово на вход передаём дальше. И так последовательно генерируем весь текст.\n",
    "1. По соответствующему токену останавливаемся.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/05b18b668786be7f3ea7def564da9b92/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  10. ONE-TO-MANY: Языковая модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К типу обработки последовательностей **One-to-Many** относится и **языковая модель**.\n",
    "\n",
    "Языковая модель позволяет пойти дальше и на вход в нейронную сеть подавать только одно слово. На выходе сеть будет выдавать текст, соответствующий языковой модели языка (точнее сжатому описанию того корпуса, на котором была обучена сеть). \n",
    "\n",
    "Иными словами, **задача: сгенерировать текст из ничего**. На вход сети отправляется только одно слово. Сеть выдаёт второе слово, которое статистически в тексте, на котором проходило обучение, встречалось очень часто в паре с первым словом. Таким образом появляется цепочка «первое слово + второе слово». Информация о втором слове подаётся на вход, а информация о первом слове и о всей истории предыдущей цепочки подаётся в качестве вектора скрытого состояния предыдущего момента времени.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/259a4bafa3c92975b5f3ced325efa7b1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_10.png)\n",
    "\n",
    "Таким образом генерируется весь текст.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f0c320c65e602c52424eb5091ecedae1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_9.png)\n",
    "\n",
    "Такой алгоритм построения языковой модели позволяет генерировать тексты, соответствующие по стилю или содержанию заданному корпусу. Например, можно сгенерировать текст «в стиле Уильяма Шекспира»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковая модель: CHAR-RNN\n",
    "\n",
    "Если пойти ещё дальше, то в качестве элементов можно использовать не только слова или словосочетания, но и буквы, составляя на выходе слова и предложения. Даже такая простая сеть будет улавливать закономерности и генерировать новые тексты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Тип SEQUENCE-TO-SEQUENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тип SEQUENCE-TO-SEQUENCE или SEQ 2 SEQ предполагает обработку последовательностей по принципу «на входе последовательность и на выходе последовательность».\n",
    "\n",
    "## SEQ 2 SEQ: ENCODER-DECODER\n",
    "\n",
    "Схема Encoder-decoder используется для машинного перевода, чат-ботов и пр. Encoder кодирует входную цепочку,  decoder разворачивает код в выходную цепочку.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/b4c44fb7a05e71c7eb06c12aa327fce3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_11.png)\n",
    "\n",
    "## SEQ 2 SEQ: машинный перевод\n",
    "\n",
    "SEQ 2 SEQ используется для машинного перевода. На вход подаётся фраза на одном языке, на выходе — на другом. Необязательно фразы должны иметь одинаковую длину, т. к. входная и выходная цепочки никак не связаны по длине. Вся входная цепочка подаётся в сеть до остановки в какой-то момент, а выходная цепочка синтезируется, пока не встретится токен <END>.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f6cfe051f0b647aabb6f623ef1b4071f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_12png.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQ 2 SEQ: decoder\n",
    "\n",
    "Так как decoder соответствует алгоритму One-to-Many, то улучшить его можно, подавая выходной элемент последовательности как входной элемент в следующий момент времени. Таким образом, на вход в decoder подаётся токен <START> и закодированное состояние из encoder в качестве признаков. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/dadc37e3d755f3abb044298bb6e7a6bc/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_13.png)\n",
    "\n",
    "# SEQ 2 SEQ: чат бот\n",
    "\n",
    "Ещё один пример SEQ 2 SEQ это чат бот. Предложение или весь предыдущий диалог на входе кодируется одним вектором, который подаётся в decoder, разворачивающий его и генерирующий текст.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e233c62ea53b38ce55e3fb8a176bdbf7/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Механизм внимания. ATTENTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблемы Encoder-decoder\n",
    "\n",
    "Основная проблема Encoder-decoder в том, что вся **информация о входной цепочке хранится в одном векторе**. Проблема эта возникает даже несмотря на использование LSTM и GRU ячеек, которые следят за тем, чтобы длинные цепочки так же хорошо обрабатывались и информация накапливалась в промежуточном состоянии. Проблема связана с тем, что элементы, которые стояли в начале предложения и те, что стояли в конце, воспринимаются сетью по-разному. Иными словами, начало и конец предложения используются неравномерно. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/29f0b66f673d4a8cc22e6855b141c8e2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_15_1.png)\n",
    "\n",
    "Эта п**роблема может быть исправлена с помощью **двунаправленных рекуррентных нейронных сетей, которые действительно часто используются в качестве encoder. Два вектора, полученных в ходе работы двунаправленной сети, суммируются. Таким образом, в каждый момент времени сеть помнит, что происходило до и после текущего момента.\n",
    "\n",
    "Но такой способ тем не менее не решает всей проблемы, и какие-то элементы всё-таки находятся ближе, а какие-то дальше от финального закодированного состояния. Как можно эту архитектуру улучшить?\n",
    "\n",
    "### Механизм внимания (ATTENTION)\n",
    "\n",
    "В предыдущем материале архитектура Encoder-decoder выглядела следующим образом:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bbdf8160e06647364540599335b96e79/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данную архитектуру легко можно улучшить, применив **Механизм внимания (ATTENTION)**. ATTENTION предлагает смотреть на всю входную цепочку сразу, а не только на последний кодированный элемент, чтобы предсказать один выходной элемент последовательности.  Предлагается закодированный входной вектор подавать в качестве входа в decoder и кроме того агрегировать в него информацию со всех элементов входной цепочки.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bfe83a4e181f55d41692d31ba1d1d37b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_17.png)\n",
    "\n",
    "Выходная цепочка с помощью такого механизма генерируется как **линейная комбинация скрытых векторов с весами**, которые соответствуют тому, насколько важен вклад того или иного входного слова в генерацию текущего выходного слова. \n",
    "\n",
    "В нашем примере первым словом скорее всего будет «привет». Механизм внимания сообщает, что с **бОльшим весом** нужно взять информацию с первого слова «hello», потому что скорее всего именно оно соответствует первому слову в выходной цепочке. Используя эту информацию, генерируем первое слово «привет», которое далее подаём на вход в сеть, а в качестве второго входа — механизм внимания на все элементы последовательности, вновь определяющий веса каждого слова входной цепочки. И т.д.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3f5e0c4f1df907cb9b802ccf4a06f832/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной вопрос: **как вычислять веса?** Как понять, с каким весом необходимо взять то или иное слово входной последовательности?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. ATTENTION: вычисление весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как проходит обучение\n",
    "\n",
    "Пусть стоит задача сгенерировать вектор $c_j$, соответствующий линейной комбинации всех входных слов. $c_j$ зависит от входных векторных представлений, пропущенных через рекуррентную сеть encoder'а. Кроме векторных представлений на вход будет подано предыдущее скрытое состояние в decoder'е $s_{j-1}$, которое говорит о том, какая цепочка была сгенерирована ранее.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4e0ec87ef035b7603f0ada6e10867117/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_19_1.png)\n",
    "\n",
    "**Вектор $c_j$** есть линейная комбинация входных векторов  ei.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/31d1e241d326b3a017ecb6810928f9f8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_22.png)\n",
    "\n",
    "**Весовые коэффициенты $α_{ij}$** — это softmax  от некоторых иных параметров, задающий вероятностное распределение, показывающее, с каким весом будет взято каждое слово.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/32137ca298b846a881b333ed8ba588b3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_21.png)\n",
    "\n",
    "**Каждый элемент вектора $А_j$** представляет собой нейронную сеть, которая на вход принимает знание о том, какое в данный момент входное слово и какая история была до этого сгенерирована последовательностью. Именно от этого и будет зависеть, с каким весом будет взято текущее слово.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/b582f0eee32b254fccdc74e7915cc980/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так вычисляется ATTENTION и это уже достаточно мощная архитектура обработки последовательностей. Следующая идея позволит улучшить эту архитектуру."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Отказ от рекуррентных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**“ATTENTION IS ALL YOU NEED”** — это следующая идея после ATTENTION, предлагающая не использовать рекуррентные связи, а только лишь механизм внимания. Рассмотрим, как это можно сделать.\n",
    "\n",
    "**Идея**: Пусть вся входная цепочка без рекуррентных связей отображается в признаковое представление, по которому генерируется первое слово. Для генерации второго слова будет использоваться вся имеющаяся информация: входная цепочка и предыдущая история сети.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/a8decccb7183102184cd07ff276ae2c4/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_23.png)\n",
    "\n",
    "Таким образом генерируется вся цепочка.\n",
    "\n",
    "Отличие этого алгоритма в том, что инструмент, анализирующий входные и выходные слова, **не использует рекуррентных временных связей**, а сразу за один проход смотрит на всю цепочку, руководствуясь специальным видом ATTENTION.\n",
    "\n",
    "Такую идея использует архитектура TRANSFORMER. Перед тем, как переходить к подробному её рассмотрению, познакомимся с механизмом Self-Attention.\n",
    "\n",
    "## SELF-ATTENTION\n",
    "\n",
    "Self-Attention — это способ в самой цепочке определить, как соотносятся друг с другом её элементы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если в случае ATTENTION используется информация и о выходной цепочке, то Self-Attention позволяет сгенерировать векторное представление элементов входной последовательности, используя информацию о всей цепочке. Механизм Self-Attention достаточно нетривиальный и заслуживает дополнительного изучения.\n",
    "\n",
    "### Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Self-Attention Mechanisms in Natural Language Processing](https://medium.com/@Alibaba_Cloud/self-attention-mechanisms-in-natural-language-processing-9f28315ff905)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. TRANSFORMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER — это достаточно сложная архитектура с множеством вариаций. Мы рассмотрим упрощённую схему.\n",
    "\n",
    "TRANSFORMER применяется в задачах машинного перевода, чат ботов и пр., по качеству обыгрывая рекуррентные нейронные сети. Рекуррентные нейронные сети с LSTM, GRU и механизмом внимания показывают хороший результат и применяются в настоящее время во многих бизнес задачах, но для улучшения качества работы сообществом всё больше применяется именно архитектура TRANSFORMER, как **качественная мощная модель**. Рассмотрим упрощённую схему на примере.\n",
    "\n",
    "Итак, пусть в настоящее время в модель TRANSFORMER подаётся входная цепочка и вся сгенерированная ранее информация\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3b47b42f2651cb0baf6cd605698446e8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_24_1.png)\n",
    "\n",
    "Self-Attention анализирует информацию о входной цепочке и Encoder кодирует признаковое представление входной цепочки. В Decoder подаётся ранее сгенерированная выходная цепочка, к которой так же был применён механизм Self-Attention, и информация из encoder. Всё это пропускается через механизм Attention, после чего может быть пропущено ещё через какие-то слои. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/fbb75a62e92e89f63fc892c0c0d37aa5/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL-1_10_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_25.png)\n",
    "\n",
    "Таким образом, предсказанное на выходе слово будет содержать информацию о входной и о выходной цепочке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER является сложной современной архитектурой и на всех задачах типа машинного перевода и чат ботов показывает самые высокие результаты. Рекуррентные нейронные сети тоже продолжают быть использованными, т.к. порой проще обучаются, а на маленьких задачах могут показать более хорошие результаты. Будем иметь в виду обе архитектуры.\n",
    "\n",
    "### Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Азбука ИИ: «Рекуррентные нейросети](https://nplus1.ru/material/2016/11/04/recurrent-networks)» от МФТИ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве практики мы организовали для вас [закрытое соревнование на площадке kaggle](https://www.kaggle.com/t/57b8e98907fd47cd9c2011557f7767db). \n",
    "\n",
    "Задача состоит в том, чтобы предсказать жанр фильма (например, комедию, драму и т. д.) по краткому описанию сюжета.\n",
    "\n",
    "Участие — индивидуальное. Обратите внимание, что у вас будет 5 сабмитов в сутки!\n",
    "\n",
    "Для участия в соревновании отводится 2 недели.\n",
    "\n",
    "Обратите внимание на Ноутбуки, предоставляемые к соревнованиям. На их основе вы сможете выстроить свое решение и лучше разобраться в теме. \n",
    "\n",
    "Успехов в соревновании!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
