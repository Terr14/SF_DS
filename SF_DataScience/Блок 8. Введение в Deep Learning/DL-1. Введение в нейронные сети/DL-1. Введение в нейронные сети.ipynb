{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DL-1. Введение в нейронные сети**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение\n",
    "\n",
    "В первом модуле мы рассмотрим:\n",
    "\n",
    "* как строить нейронные сети;\n",
    "* какие типы нейронных сетей бывают;\n",
    "* где сегодня применяются нейронные сети."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Машинное обучение и типы данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем переходить к Deep Learning (глубокому обучению), актуализируем информацию по машинному обучению.\n",
    "\n",
    "Машинное обучение можно рассматривать как некоторое отображение входных данных в выходные данные. Как правило, этим занимается некоторая модель, которая зависит от заданных параметров. Эти параметры характеризуют то, как модель себя ведёт и как предсказывает выходные значения по входным. Подстройка этих параметров и есть машинное обучение.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/b89ec2f618cbfd8f9c9fc0320fca1033/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_1_1.png)\n",
    "\n",
    "**Классическое обучение с учителем**\n",
    "\n",
    "Как правило, в МО используется **обучение с учителем** (supervised learning), в котором есть база данных, состоящая из пар «объект — ответ». Мы показываем модели эти пары одну за другой и корректируем параметры таким образом, чтобы на новых примерах наша модель предсказала правильные ответы. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какие типы данных могут использоваться в таких задачах?**\n",
    "\n",
    "1. **Низкоразмерная информация.**\n",
    "\n",
    "    Например, это могут быть векторы, отвечающие за характеристики пользователей (рост, вес, возраст и т. д.), и мы хотим сделать их классификацию, то есть отобразить низкоразмерный вектор в другой низкоразмерный вектор или скаляр.\n",
    "1. **Изображение или видео.**\n",
    "\n",
    "    Это более сложный тип данных. В качестве примера здесь можно рассмотреть задачу компьютерного зрения — отображение визуальной информации в низкоразмерную высокоуровневую информацию. \n",
    "1. **Текст.**\n",
    "\n",
    "    Пример: обработка текста и его отображение в качестве низкоразмерной высокоуровневой информации.\n",
    "1. **Аудио- и звуковые сигналы.**\n",
    "\n",
    "    Знакомый всем пример — распознавание речи и перевод аудио в текст.\n",
    "Мы можем делать практически любое отображение из любого типа данных в любой тип данных. При этом на каждое такое отображение есть соответствующая задача и методы её решения. Большинство этих задач наилучшим образом решаются с помощью нейронных сетей, или технологии Deep Learning. \n",
    "\n",
    "Важно понимать, что нейронные сети не всегда являются лучшим вариантом. Например, отображение низкоуровневой информации в саму себя лучше реализовать с помощью других алгоритмов (случайный лес, градиентный бустинг и др.). Однако в случае с высокоразмерными сложными данными (изображения, звуки, тексты) очень хорошо работают именно нейронные сети.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bd50a06d3775d0f7c1cabaa9da38c7a8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная идея нейронных сетей — это обучение представления. Получение выхода из заданного входа начинается с **извлечения признаков**, когда из входных данных извлекается некоторое промежуточное представление.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5934bd03ed90b191bc024a6dc90b3b92/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_4_1.png)\n",
    "\n",
    "### Многослойный перцептрон\n",
    "\n",
    "Многослойный перцептрон — это простая нейронная сеть. Он состоит из:\n",
    "\n",
    "* входного вектора;\n",
    "* выходного вектора;\n",
    "* вектора промежуточного представления (скрытый слой).\n",
    "\n",
    "Вычисление распространяется от входа к выходу, связям между нейронами соответствуют некоторые веса. Такая сеть является полносвязной.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/e89135fb77af559681f0c8a86e2b0d8b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что в нейроне?\n",
    "\n",
    "В нейрон входит несколько значений $x_1, x_2, x_3$ с несколькими связями. Связям соответствуют некоторые коэффициенты $w_1, w_2, w_3$ (если входов в нейрон три).\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ea7a7d97f84adf6407dd25814917c541/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/15.jpeg)\n",
    "\n",
    "Дальше внутри нейрона происходит вычисление двух операций, а точнее композиция линейной и нелинейной операций:\n",
    "\n",
    "* В линейной операции мы выполняем взвешивание суммы всех входных значений ($x_1$ умножаем на $w_1$, $x_2$ — на $w_2$ и так далее), всё это вместе суммируем и прибавляем некоторое значение **смещения b**. \n",
    "* В нелинейной операции от полученных на предыдущем шаге значений мы берём нелинейную функцию.\n",
    "\n",
    "Так вычисляется выходное значение в одном нейроне. Дальше эти нейроны можно уже агрегировать в большую нейронную сеть.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1b9653eea934b953a26bd7fda7963d03/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_5.png)\n",
    "\n",
    "Одному слою такой нейронной сети уже будет соответствовать некоторая **матрица параметров** и некоторый **вектор смещения B**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом проходят вычисления в одном слое полносвязной нейронной сети и вычисляется матричное умножение и прибавление вектора, а затем поэлементное взятие нелинейности.\n",
    "\n",
    "### Как работают нейроны?\n",
    "В одном нейроне нейронной сети происходит локальное принятие решения: мы взвешиваем и суммируем входные данные и на основе полученных результатов локально что-то предсказываем, принимая маленькое решение на каждом шаге. Все полученные решения агрегируются и подаются на следующий слой и уже новые значения используются для более сложного высокоуровнего принятия решений.\n",
    "\n",
    "В математическом смысле нейронная сеть — это универсальный **аппроксиматор*** (может аппроксимировать любую функцию).\n",
    "\n",
    "Если у вас есть какая-то сложная зависимость между входом и выходом, можно описать её с помощью нейронной сети. \n",
    "\n",
    "Если брать больше слоёв, то это будет уже многослойная нейронная сеть, и у неё будет более сложное промежуточное представление.\n",
    "\n",
    "> **Аппроксима́ция** (от лат. proxima — ближайшая), или приближе́ние — научный метод, состоящий в замене одних объектов другими, в каком-то смысле близкими к исходным, но более простыми.\n",
    "\n",
    "### Вычисления в многослойной нейронной сети\n",
    "\n",
    "Вычисления в такой сети можно записать с помощью следующего рекуррентного соотношения в многослойной нейронной сети:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/93b0c35d3efdf8dc46899553737a2d0f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выход каждого слоя — это вход, умноженный на какую-то матрицу, плюс вектор смещения, и от всего этого берётся нелинейность. Все веса, которые соответствуют смещениям bias, мы назовём параметрами нашей модели.\n",
    "\n",
    "### Типы параметров нейронных сетей\n",
    "\n",
    "Обычно у нейронных сетей выделяют два типа параметров: просто параметры (W) и гиперпараметры.\n",
    "\n",
    "**Гиперпараметры** — параметры системы, которые мы не обучаем, а «создаём руками», как конструкторы проектируют нейронную сеть. Мы определяем, сколько нейронов в слое и сколько всего слоёв, какую функцию активации использовать и так далее. Всё это мы задаём вручную, а вот параметры связи между нейронами и bias получаются автоматически в процессе обучения.\n",
    "\n",
    "### Какую функцию активации взять?\n",
    "\n",
    "Долгое время использовались **сигмоидальные функции**, или **sigmoid**. В этой функции мы помогаем выходу из нейрона принять какое-то бинарное решение, то есть отображаем все его значения во что-то больше или меньше нуля. Сигмоида — это нелинейная функция.\n",
    "\n",
    "Если бы мы использовали обычную линейную функцию (или вообще не использовали никакую функцию), то композиция слоёв без нелинейности давала бы одну большую линейную операцию. Поэтому не имело бы смысла настраивать много слоёв в нейронной сети: они все были бы эквивалентны какому-то одному слою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если же мы ставим нелинейность между слоями, аппроксиматор становится более сложным и уже может аппроксимировать достаточно сложные функции. Это функция sigmoid. Она использовалась раньше, но у неё есть некоторые проблемы, и поэтому сейчас, как правило, используют **функцию ReLU** или её модификации. В этой функции мы зануляем всё, что меньше 0, а всё, что больше 0, оставляем как есть. У ReLU очень простая производная, и именно производная этой функции будет участвовать в процессе обучения и в алгоритме обратного распространения ошибки.\n",
    "\n",
    "### Как применить нейронную сеть для задачи классификации?\n",
    "\n",
    "Представим, что у нас есть некоторые объекты в признаковом пространстве. Объекты задаются тремя числами, и у нас есть три компонента этого вектора.\n",
    "\n",
    "Мы так построили нейронную сеть, что у неё есть три входных нейрона — как раз соразмерно входному вектору. Например, мы хотим сделать бинарную классификацию на два класса: фиолетовый и оранжевый. Поэтому мы сделали в нашей сети два выходных нейрона. Допустим, мы уже как-то обучили нейронную сеть. Как теперь её использовать?\n",
    "\n",
    "Ставим на вход сети наш вектор из трёх компонентов, делаем прямое распространение  по тем формулам, что описаны выше, и получаем два значения на выходе. Они уже отвечают на вопрос, к какому классу принадлежит объект, но после некоторого специального нормирующего преобразования мы получаем другие два числа — $P_1$, и $P_2$. Именно они уже явно характеризуют вероятность принадлежности к первому или второму классу. На выходе нейронной сети в случае классификации — распределение вероятностей принадлежности к тому или иному классу.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ae8a93d8e7bee26ac3509eb78d8c5630/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Как обучить нейронную сеть?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети **обучаются с помощью обучения с учителем**, или на примерах. Для этого нужна обучающая выборка, которая состоит из пар «входной объект — выходной объект». Мы подаём эту обучающую выборку в процесс обучения, который состоит в том, чтобы найти такие параметры **модели W**, чтобы наша нейронная сеть правильно предсказывала те самые ответы, которые мы уже знаем.\n",
    "\n",
    "Таким образом мы решаем задачу минимизации (оптимизации): мы минимизируем ошибку на тех примерах, которые есть в нашей обучающей выборке.\n",
    "\n",
    "Ошибку можно записать по-разному:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/07277b4c296443c85ebb369babf51760/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_10_2.png)\n",
    "\n",
    "Здесь она записана как разница между **правильным ответом $D$** и **предсказанием сети $G_W(Z)$**. От этой разницы мы берём норму и получаем скаляр, который хотим минимизировать по всей обучающей выборке. Затем мы ищем такие веса (нижнее соотношение $W^*$), которые минимизируют эту ошибку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как решать задачу оптимизации \n",
    "\n",
    "Для этого есть различные способы. В теории оптимизации есть такой известный алгоритм, как **градиентный спуск** — объясним его смысл на примере.\n",
    "\n",
    "Допустим, у вас есть какая-то функция (здесь представлена одномерная функция, но в общем случае она может быть и многомерной), и вы хотите найти её минимум. Вы стоите в некоторой точке, и вам нужно понять, в какую сторону двигаться из этой точки, чтобы приблизиться к минимуму. Есть вектор, который называется **градиент** — он указывает в сторону возрастания функции. Градиент со знаком «минус» (антиградиент) указывает в сторону убывания функции.\n",
    "\n",
    "Подобный алгоритм предлагает двигаться в сторону антиградиента и таким образом приближаться к локальному минимуму.\n",
    "\n",
    "Итак, мы вычислили градиент, и с некоторым **параметром α**, который ещё называют **learning rate** (скорость обучения), мы прибавляем этот антиградиент к текущем весам — так получается итерационное движение к минимуму.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4480010d90260a7aac5504fcff8ca27b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_11.png)\n",
    "\n",
    "В случае нейронных сетей используется небольшая модификация — **стохастический градиентный спуск**. Отличие от предыдущего градиентного спуска в том, что мы вычисляем градиент не на всех образцах выборки, а только на одном образце за одну итерацию или на группе образцов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как вычислить градиент?\n",
    "\n",
    "Это вектор или даже некоторый **тензор**, размерность которого совпадает с размерностью всех наших обучаемых параметров.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/26eb057a00c5896bffe1bf315d3cdae8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_12.png)\n",
    "\n",
    "Градиент может особенно зависеть от слоёв, далёких от той ошибки, от которой мы считаем градиент. Например, у нас ошибка зависит от параметров первого слоя очень сложным образом, и, тем не менее, мы всё равно можем вычислить градиент с помощью алгоритма обратного распространения ошибки (backpropagation), который заключается в том, что мы используем  правила дифференцирования сложной функции.\n",
    "\n",
    "Нейронная сеть, даже если она представляет собой сложную функцию — на самом деле просто композиция каких-то маленьких простых вещей, например умножили на матрицу, прибавили вектор, взяли поэлементную матрицу и так далее. Используя такое дифференцирование сложной функции, можно узнать, как наша ошибка зависела от параметров второго слоя через это цепное правило. Именно таким образом можно вычислить градиент по любому весу и по всем весам.\n",
    "\n",
    "### Для каких задач применять?\n",
    "\n",
    "* классификация;\n",
    "* регрессия;\n",
    "* машинное зрение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Задача компьютерного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача компьютерного** зрения состоит в отображении визуальной информации (изображения или видео) в высокоуровневую, семантическую информацию. \n",
    "\n",
    "В **классификации** изображений на входе находится картинка, на выходе — метка класса.\n",
    "\n",
    "При **детектировании** и **локализации** объектов, помимо классификации, даётся ограничивающий прямоугольник или локализация, где тот или иной объект присутствует на изображении.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f083ea0e6032854de6718e4e1c1fd6d0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_13.png)\n",
    "\n",
    "**Семантическая сегментация** — более сложная задача, в ней классифицируется каждый пиксель изображения, или выделяются некоторые сегменты на изображении, и каждому сегменту присваивается какая-то метрика.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d3b2e2e4e5d955529cbb927e39df1f36/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная трудность этих задач в том, что изображение представляет собой огромный объём неструктурированной информации, к которой непонятно, как подойти — это просто какие-то числа RGB. В машинном зрении ещё до прихода нейронных сетей использовались **признаки**. Признаки — это то, что  описывает изображения. Особенности изображения записываются в виде уже осмысленного высокоуровневого вектора. Далее к этому вектору применяется алгоритм классификации и получается ответ.\n",
    "\n",
    "В итоге решение задачи сводится к двум частям:\n",
    "\n",
    "1. Извлечение признаков.\n",
    "2. Подача признаков в алгоритм машинного обучения.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/63bfbe9589973005d658d4f6d30b4402/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_15.png)\n",
    "\n",
    "До появления машинного зрения все признаки создавались инженерами, которые анализировали изображение и вручную кодировали то, как должны выглядеть признаки.\n",
    "\n",
    "**Свёртка** — это способ извлечения признаков.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5fa6a57300c3979fe672cc3a032188f8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Где взять ядро свёртки?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея свёрточных нейронных сетей состоит в том, чтобы использовать свёртку как специальный тип слоя в нейронной сети.\n",
    "\n",
    "Обратимся к предыдущему примеру:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bf6ce48ff66bcc5fa83a313545e8cb68/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_17.png)\n",
    "\n",
    "Посмотрим на свёртку: здесь видно, что у нас используется некоторая линейная операция для вычисления значения в выходной матрице. Такая же линейная операция использовалась нами в нейронных сетях. \n",
    "\n",
    "Вернёмся:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4576ea7e79d49e331cbecddf24f46c4c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_18.png)\n",
    "\n",
    "Разница в том, что нейроны уже связаны в соседних тензорах: не каждый с каждым, а только какая-то группа нейронов во входном слое связана с одним нейроном в выходном слое. Именно так и делается в свёрточных нейронных сетях. \n",
    "\n",
    "Веса в ядре свёртки получаются с помощью оптимизации. Теперь они соответствуют связям между нейронами. Таким образом, мы получили автоматически обучаемые ядра свёрток или, другими словами, **обучаемые признаки** (в отличие от тех признаков в компьютерном зрении, которые использовались ранее, когда люди задавали их вручную)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Основная идея глубокого обучения** — не просто изучение признаков, а извлечение их иерархии: извлечение признаков вышестоящего уровня в пространстве нижестоящего."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Что такое Deep Learning? Типы слоёв"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning** — это обучение иерархии признаковых представлений.\n",
    "\n",
    "Что здесь важно: \n",
    "\n",
    "* Тут есть признаки: сначала из входных данных извлекаются признаки, и дальше работа идёт уже с ними. \n",
    "* Тут есть обучаемые признаки: мы делаем так, чтобы нейронная сеть извлекала их автоматически.\n",
    "* Тут не просто один слой признаков — здесь иерархия признаков. \n",
    "\n",
    "Какие типы слоёв бывают в таких сетях?\n",
    "\n",
    "* Свёрточный слой.\n",
    "* Понижение размерности (Pooling):\n",
    "    * локальное усреднение;\n",
    "    * локальный максимум.\n",
    "* Полносвязный слой (Fully-connected)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Обработка последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработка последовательности с использованием рекуррентных нейронных сетей состоит в том, что после каждой обработки какого-то элемента последовательности сеть запоминает всё, что уже было до этого. То есть нейронная сеть обладает своеобразной внутренней памятью.\n",
    "\n",
    "Более сложные рекуррентные сети:\n",
    "\n",
    "* [LSTM](https://ru.wikipedia.org/wiki/%D0%94%D0%BE%D0%BB%D0%B3%D0%B0%D1%8F_%D0%BA%D1%80%D0%B0%D1%82%D0%BA%D0%BE%D1%81%D1%80%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BF%D0%B0%D0%BC%D1%8F%D1%82%D1%8C%C2%A0);\n",
    "* [GRU](https://ru.wikipedia.org/wiki/%D0%A3%D0%BF%D1%80%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D0%BC%D1%8B%D0%B9_%D1%80%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D0%B1%D0%BB%D0%BE%D0%BA%C2%A0).\n",
    "\n",
    "Где применяются рекуррентные нейронные сети:\n",
    "\n",
    "* умная клавиатура;\n",
    "* анализ комментариев;\n",
    "* машинный перевод;\n",
    "* чат-боты;\n",
    "* распознавание речи;\n",
    "* отображение изображений в текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. От распознания к синтезу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Синтез** — это обратная задача, то есть перевод информации высокого уровня в информацию низкого уровня. \n",
    "\n",
    "На примере человека: восприятие и творчество.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/eb4f06ccc91e7452e207b7649bfa6e50/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_19.png)\n",
    "\n",
    "На примере работы техники: распознавание и синтез.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/a29fa8d8bfd596d6c2ecd41e6dc7ba5b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Состязательные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея **состязательной сети** (GAN — Generative Adversarial Networks) состоит в том, чтобы построить вторую сеть, которая называется **дискриминатор**. Это просто бинарный классификатор: на входе — изображение, а на выходе — ответ с предположением, реальное изображение или синтезированное. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4bb4ebfc58e0a93e8e0d6dd603590c0b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_21.png)\n",
    "\n",
    "**Важный момент**: генератор должен учиться обманывать дискриминатор. Эти сети учатся параллельно, играя в антагонистическую игру, и в итоге мы получаем идеальный генератор и идеальный дискриминатор.\n",
    "\n",
    "Примеры применения:\n",
    "\n",
    "* синтез изображений;\n",
    "* преобразование текста в изображение;\n",
    "* отображение изображения в изображение;\n",
    "* отображение видео в видео;\n",
    "* синтез речи ([Wavenet](https://en.wikipedia.org/wiki/WaveNet%C2%A0));\n",
    "* отображение аудио в видео."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Причины успеха"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Причины успеха глубокого обучения:\n",
    "\n",
    "* совершенствующиеся алгоритмы и архитектуры;\n",
    "* доступные объёмы данных;\n",
    "* ускорение обучения и вывода с помощью GPU.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/9a570b2dddd4220ffe451100a9cc8871/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/DL_1_%D0%BC%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Теоретическое резюме"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Решаемые задачи**: отображение X в Y.\n",
    "\n",
    "► Текст, изображения, видео, звук, низкоразмерные данные.\n",
    "\n",
    "Типы архитектур:\n",
    "\n",
    "* Для распознавания и синтеза визуальных данных → **свёрточные** сети (**CNN**).\n",
    "* Для распознавания и синтеза последовательностей → **рекуррентные** сети (**RNN**).\n",
    "* Для улучшения качества синтеза → **состязательные** сети (**GAN**).\n",
    "\n",
    "**Факторы успеха**: топология, данные, железо (GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Практика. Открываем ноутбук в Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый в видео notebook: [mnist_intro.ipynb](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1b43a90a5ed21fc41db22b85f39ec798/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/mnist_intro.ipynb)\n",
    "\n",
    "Вы можете сохранить себе ноутбук из примера.\n",
    "\n",
    "**MNIST** — задача классификации рукописных цифр. В ней есть десять классов: цифры от 0 до 9.\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) — облачный Jupyter notebook от Google. Для сохранения всех ваших действий в ноутбуке советуем залогиниться в аккаунте Google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Практика. Линейная модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый в видео notebook: [mnist_intro.ipynb](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1b43a90a5ed21fc41db22b85f39ec798/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/mnist_intro.ipynb)\n",
    "\n",
    "Начинаем с импорта следующих библиотек:\n",
    "\n",
    "* **numpy** — для матричных операций;\n",
    "* **sklearn** — для моделей машинного обучения;\n",
    "* **matplotlib** — для графиков;\n",
    "* **tensorflow** — для загрузки dataset MNIST.\n",
    "\n",
    "## Как работает линейная модель?\n",
    "\n",
    "Например, есть модель логистической регрессии: она взвешивает коэффициентами все наши признаки и прогоняет через логистическую функцию. Такую модель можно быстро обучать с помощью градиентного спуска, который немного меняет параметры этой модели на каждой интеграции, тем самым улучшая качество. Довольно быстро модель находит линию, которая разделяет два класса в нашем примере. \n",
    "\n",
    "Чтобы обучить нашу модель, необходимо подать вектор признаков на вход линейной регрессии. Сейчас у нас есть двумерная картинка размера 28 × 28. Вытягиваем картинку построчно в вектор с помощью функции **reshape**.\n",
    "\n",
    "Прежде чем обучать линейную модель, данные лучше отцентрировать и отнормировать с помощью **StandardScaler**.\n",
    "\n",
    "После этого мы можем обучить линейную модель с помощью **градиентных методов оптимизации**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Практика. Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый в видео notebook: [mnist_intro.ipynb](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1b43a90a5ed21fc41db22b85f39ec798/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/mnist_intro.ipynb)\n",
    "\n",
    "На основе деревьев решений работает **градиентный бустинг**. Он строит следующие деревья, которые будут ещё лучше решать поставленную задачу, исправляя ошибки предыдущих деревьев.\n",
    "\n",
    "**GradientBoosting** — переборный алгоритм: перебирает он много и достаточно долго, поэтому для реальных картинок лучше использовать другой алгоритм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. Практика. Нейронная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нейросеть** строит сложную функцию логистических регрессий. Наслоение логистических регрессий усложняет функцию, но не усложняет способ её обучения.\n",
    "\n",
    "Для оптимизации всех параметров нам не нужен перебор — мы можем посчитать производную потерь по каждому из параметров и немного их подвинуть.\n",
    "\n",
    "**MLPClassifier** — полносвязная сеть. Позже мы рассмотрим её более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первом домашнем задании вам предстоит **улучшить модель** из ноутбука, рассмотренного в практической части модуля, в соответствии с собственной интуицией, а затем поделиться результатами в канале модуля в Slack.\n",
    "\n",
    "Это **разогрев** перед более сложным материалом и полезное упражнение, чтобы в дальнейшем научиться осознанно совершенствовать модель.\n",
    "\n",
    "Желаем успехов в выполнении задания!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
