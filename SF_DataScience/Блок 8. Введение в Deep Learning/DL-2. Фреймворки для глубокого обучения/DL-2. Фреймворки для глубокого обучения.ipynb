{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DL-2. Фреймворки для глубокого обучения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом модуле мы разберёмся с фреймворком для нейросетей TensorFlow, посмотрим на детали того, что происходит «под капотом», и какие API существуют.\n",
    "\n",
    "## Линейная классификация\n",
    "\n",
    "Представьте, что у нас есть признаки ***x = (x1, x2)*** и выборка положительных и отрицательных точек ***y ∈ {+1, −1}***.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/301f1b273f5daf380360132ff37d12b0/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl1_1.png)\n",
    "\n",
    "Нам нужно найти разделяющую гиперплоскость между ними. В данном случае это просто линия, которая задаётся тремя коэффициентами:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/0e05eb6c69b6512958ad3b6f47557ec2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/fun1.png)\n",
    "\n",
    "Нам нужно найти три коэффициента ***w***, которые зададут линию. Далее мы можем взять точку ***х*** и понять, где она находится относительно линии — выше или ниже. Для этого мы должны узнать знак линейной комбинации. Вектор весов ***w*** задаёт нормаль к нашей линии, то есть он перпендикулярен ей (фиолетовый вектор на графике ниже).\n",
    "\n",
    "**Линейная комбинация** — это скалярное произведение и длина проекции какого-нибудь другого вектора на наш вектор w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d61b08d27dbc194755fc29e3bf7ac3b8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl1_2.png)\n",
    "\n",
    "Поэтому проекция становится разных знаков. Из этих соображений мы делаем линейный классификатор. Наш алгоритм:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/1df4fa0464eec3b489eb21484103556a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/fun21.png)\n",
    "\n",
    "Здесь появляется знак нашей линейной комбинации. Настроить линейный классификатор — значит найти эти коэффициенты.\n",
    "\n",
    "## Логистическая регрессия\n",
    "\n",
    "Она тоже решает задачу классификации, но в конце применяется не функция знака, а **сигмоидная функция**. Она превращает длину проекции в уверенность.\n",
    "\n",
    "Уверенность и неуверенность появляется из-за краевых эффектов: на границе классов может быть какой-то шум, и в классификации точек, которые находятся рядом с красной разделяющей линией, мы не очень уверены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/c0fbb0c94cf7c2140fa72fc284e54e60/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl2_0.png)\n",
    "\n",
    "Если мы уходим далеко от линии вглубь классов, то предполагается, что мы более уверены в этом предсказании. Сигмоида делает именно это — превращает длину проекции линейной комбинации в уверенность.\n",
    "\n",
    "Сигмоида устроена неслучайным образом:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/59e61439b38334fc16ff4da30829a8e3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl2_2.png)\n",
    "\n",
    "Если длина проекции 0 (точка ровно на красной линии), то сигмоида даёт 0.5. Логистическая регрессия предсказывает вероятность положительного класса. Вероятность отрицательного будет равна единица минус предсказанная вероятность положительного класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другой пример\n",
    "\n",
    "Представим, что у нас есть следующая задача:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3a096f1287913d671c9dd92fcd460f37/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl3.png)\n",
    "\n",
    "Чтобы решить подобную задачу, мы можем «подпереть» треугольник тремя линиями и создать алгоритм, используя только логистическую регрессию.\n",
    "\n",
    "Для начала отделим минусы слева и построим логистическую регрессию:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/4cacc2ddf570400e4e9356a346b6ef49/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эти данные подадим для обучения логистической регрессии и получим коэффициенты красной линии. \n",
    "\n",
    "Отделим остальные минусы:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5a47e1c994534185bab53e6a10466aa1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl5.png)\n",
    "\n",
    "Все коэффициенты мы получили практически вручную. Сейчас у нас есть коэффициенты трёх логистических регрессий, каждая из которых решает свою маленькую подзадачу. \n",
    "\n",
    "Теперь возьмём какую-нибудь точку и посмотрим, какие три предсказания дают эти линии в точке:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/80ebfdc4bec430d912c56930b6467065/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что делать с этими тремя значениями?\n",
    "\n",
    "В координатах х1 и х2 эта задача не решается, поэтому полученные коэффициенты мы можем рассматривать как новые координаты.\n",
    "\n",
    "Получим три признака, каждый из которых говорит, где мы находимся относительно каждой стороны треугольника.\n",
    "\n",
    "Давайте возьмём наш целевой признак y, добавим его в новую таблицу, где находятся новые признаки с предсказаниями, и попробуем решить задачу с новыми признаками с помощью линейной логистической регрессии. На новой выборке получим логистическую регрессию:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ea26d51daa8fcd16594b1c67bd77302e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl7.png)\n",
    "\n",
    "Теперь она даёт нам некоторые коэффициенты и взвешивает уже не признаки, а предсказания. То, что мы получили — простейшая нейросеть. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Граф вычислений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ГРАФ ВЫЧИСЛЕНИЙ \n",
    "\n",
    "На данный момент мы «вручную» нашли параметры всех линий:\n",
    "\n",
    "$z_i = \\sigma (w_{0,i} + w_{1,i} x_1 + w_{2,i} x_2)$\n",
    "\n",
    "$a(x) = \\sigma (w_0 + w_1z_1(x) + w_2z_2(x) + w_3z_3(x))$\n",
    "\n",
    "При этом на нашу сложную комбинацию функций можно посмотреть как на **граф** вычислений. У графа есть **вершины**, и каждая вершина — вычисляемое значение.\n",
    "\n",
    "Также есть **рёбра** — зависимости, которые имеют направления (на картинке ниже это стрелочки). Ребро идёт от ***х1*** к ***z1*** в случае, если нам необходим ***х1***, чтобы вычислить значение ***z1***. Это граф зависимости между вычисляемыми значениями:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/408b137789dea2e44723a2b3a575e92a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl8_2.png)\n",
    "\n",
    "Граф соответствует комбинации наших функций. Такой граф называют **многослойным персептроном (MLP)**, и здесь уже можно видеть некоторые слои:\n",
    "\n",
    "* входной слой (признаки);\n",
    "* скрытый слой (нейроны);\n",
    "* выходной слой (предсказания)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## НЕЛИНЕЙНОСТИ В НЕЙРОНАХ \n",
    "\n",
    "Возьмём в качестве примера следующий граф:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/bdecd5c44affa0a9da981861287a2460/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl10_3.png)\n",
    "\n",
    "Если нелинейности убрать, то на этом примере видно, что наша модель станет очень простой: мы можем подставить выражения для ***z1*** и ***z2*** в нашу модель ***а***:\n",
    "\n",
    "$z_1 = w_{1,1}x_1 + w_{2,1}x_2$\n",
    "\n",
    "$z_2 = w_{1,2}x_1 + w_{2,2}x_2$\n",
    "\n",
    "$a = w_1z_1 + w_2z_2$\n",
    "\n",
    "Мы можем раскрыть скобки, привести подобные слагаемые и всё, что мы получим — линейную комбинацию х1 и х2. При этом модель не становится сложнее.\n",
    "\n",
    "$a = (w_1w_{1,1} + w_2w_{1,2})x_1 + (w_1w_{2,1} + w_2w_{2,2})x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP — это простейший пример нейросети:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d8bffaae7644af617659ad2fe72a9235/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl11.png)\n",
    "\n",
    "Слои в MLP называют:\n",
    "\n",
    "* Dense layer (плотный);\n",
    "* Fully-connected layer (полносвязный).\n",
    "\n",
    "Архитектура MLP:\n",
    "\n",
    "* количество слоёв;\n",
    "* количество нейронов в каждом слое;\n",
    "* функция активации, которую будем использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow (TF) — Deep Learning фреймворк**\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/9e5fc510a50e8068a9fe67f46c5808db/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/tflogo.png)\n",
    "\n",
    "Основа вычислений в TF — граф. Каждая вершина графа — это одна операция, у которой есть входы и выходы.\n",
    "\n",
    "* Вход любой операции — это набор тензоров (многомерных массивов).\n",
    "* Выход любой операции — это тоже набор тензоров.\n",
    "\n",
    "А у нас целый граф операций, между которыми перекидываются тензоры.\n",
    "\n",
    "**Дополнительные материалы**\n",
    "\n",
    "* [Hello, TensorFlow!](https://www.oreilly.com/learning/hello-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Пример обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/3b183f13642aafe36219b174bd309289/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sgd.png)\n",
    "\n",
    "Функция потерь обычно дифференцируемая. Если представить, что у нас есть параметр w, который нужно оптимизировать, то мы можем начать с любой точки (initial weight) и посчитать в ней производную. В данном случае это tg наклона касательной.\n",
    "\n",
    "**Дополнительные материалы**\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем ознакомиться с визуализацией обучения нейронной сети с помощью [TensorFlow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.82416&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Умеем дифференцировать:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/03d845279e1588acbc68f1a9c5ffca30/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl26_1.png)\n",
    "\n",
    "Возьмём сложную функцию:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/10b19857f0a3a5b9a6ff1cb0b6d38072/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl26_2.png)\n",
    "\n",
    "Цепное правило:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/42b4386bd4b285891f1825f9a74f0956/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl26_3.png)\n",
    "\n",
    "Пример для $h(x)=f(x)g(x)$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/a45ea41f4557ff4bb25a4253d16f532d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl26_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Граф вычисления производных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Граф для вычисления производных**\n",
    "\n",
    "Граф для вычисления нашей композиции: \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/6ffe0db0e84238ce885b1951d5dacc30/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl27_1.png)\n",
    "\n",
    "Построим из него граф из производных:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/782bde1189b775de2cc6edf0a4c9026a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl27_2.png)\n",
    "\n",
    "Каждому ребру приписана производная начала по концу. Можно догадаться, как работает цепное правило:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/19907ab7d89f57625033091ee36703a8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl28_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Алгоритм вычисления производной в графе**\n",
    "\n",
    "Как посчитать производную a по b:\n",
    "\n",
    "1. находим непосещённый путь из a в b;\n",
    "1. перемножаем значения на рёбрах в пути;\n",
    "1. добавляем в сумму;\n",
    "1. возвращаемся к п.1.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f2a33379f8bdb0ffc6c9c672638cc643/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl38_1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Пример одного нейрона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на прямой и обратный граф на примере одного нейрона. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/453a18779c1b4d7ca9e2f40c4cd5e79c/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl39_1.png)\n",
    "\n",
    "Рассмотрим, из чего он состоит по порядку:\n",
    "\n",
    "* $x_1, x_2$ — признаки, то, что подаётся на вход (плейсхолдеры);\n",
    "* $\\alpha, \\beta$ — веса модели (переменные);\n",
    "* $*$  — операции произведения (после идёт подсчёт суммы);\n",
    "* $s$ — применяем сигмоиду;\n",
    "* $z$ — записываем в новую переменную;\n",
    "* $L$ — функция потерь, которая берёт на вход правильные ответы *y*, предсказанные ответы *z*  и говорит, насколько они близки друг к другу;\n",
    "* $y$ — плейсхолдер, правильный ответ.\n",
    "\n",
    "Производные функции нам нужны, чтобы узнать, как нужно изменить $\\alpha$ и $\\beta$, чтобы минимизировать наши потери: \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/8b6ccbd0dfcc92e9a4df58acc572d733/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl39_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы взять производную, мы переворачиваем наш граф:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/08eb984f58a434aebbadbafe4c590fb9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl40_1.png)\n",
    "\n",
    "Для каждой стрелочки считаем производную. По сути нам нужно найти обратный путь от L до $\\alpha$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/90e32716da72806dc66829fef198402a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl42_1.png)\n",
    "\n",
    "Так же считается и по $\\beta$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/a29f772ea52ff35460aa5b870428bdf9/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl42_2.png)\n",
    "\n",
    "В TF  все производные строятся автоматически и достаточно быстро. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Цепное правило и граф производных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть алгоритм для подсчёта производных для любых дифференцируемых графов вычислений. Эффективный способ вычисления всех производных называется **backpropagation** (обратное распространение ошибки):\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/edbc89abd20bbb9fe94240c70159c3bb/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl43_1.png)\n",
    "\n",
    "## Backpropagation (Backprop)\n",
    "\n",
    "В backpropogation есть два прохода: прямой и обратный. Те производные, которые считает обратный граф, нужно считать в определённой точке. Именно для этого нужен прямой проход — он рассчитает аргументы всех этих производных:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d547ed0b443087fb69623864f735216a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация весов\n",
    "\n",
    "Мы не можем инициализировать нулями. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/2ae288141d70646e2da641ff976a8447/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/sl46_1.png)\n",
    "\n",
    "Если на данном примере мы стартуем на нулевых весах, то w2 и w3 будут обновляться симметрично, а значит, мы не научим никакую сложную комбинацию за счёт простоты сети. Это называют симметрией. \n",
    "\n",
    "Чтобы сломать симметрию, мы можем использовать случайный шум. \n",
    "\n",
    "## Дополнительные материалы\n",
    "\t\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Step-by-step backpropagation example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Практика. TF в Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используемый в видео notebook: [skillfactory-dl-2-screencast.ipynb](https://lms-cdn.skillfactory.ru/assets/courseware/v1/01c396401a3629a34b6560a4dd724ce2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/skillfactory-dl-2-screencast.ipynb). Notebook использует устаревшую версию Tensorflow, и изучать его не нужно.\n",
    "\n",
    "В этом видео мы познакомимся со средой Google Colab и рассмотрим, что можно делать в TF.\n",
    "\n",
    "[Google Colab](https://colab.research.google.com/) — сервис, который позволяет запускать Jupyter-ноутбуки. В нём есть видеокарты и много оперативной памяти.\n",
    "\n",
    "Перейдём на страницу https://colab.research.google.com/notebooks/welcome.ipynb.\n",
    "\n",
    "Рекомендуем вам залогиниться через аккаунт Google.\n",
    "\n",
    "Используемый в видео notebook [skillfactory-dl-2-screencast.ipynb](https://lms-cdn.skillfactory.ru/assets/courseware/v1/01c396401a3629a34b6560a4dd724ce2/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/skillfactory-dl-2-screencast.ipynb) сразу можно открыть в Google Colab.\n",
    "\n",
    "Вы можете сразу изучить все комбинации клавиш во вкладке Tools — Keyboard chortcuts, например: Ctrl + Enter для запуска ячейки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ноутбук](Ноутбук к скринкасту можно открыть в Google Colab.) к скринкасту можно открыть в Google Colab."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
