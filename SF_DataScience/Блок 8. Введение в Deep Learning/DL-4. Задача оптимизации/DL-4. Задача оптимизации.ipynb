{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DL-4. Задача оптимизации**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение\n",
    "\n",
    "В этом модуле мы поговорим о следующем:\n",
    "\n",
    "* функциях активации;\n",
    "* инициализации;\n",
    "* batch-нормализации;\n",
    "* dropout-регуляции;\n",
    "* градиентном спуске;\n",
    "* стохастическом градиентном спуске;\n",
    "* матричных операциях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция активации** нейрона определяет выходной сигнал, который, в свою очередь, определяется входным сигналом или набором входных сигналов. Функцию активации используют, чтобы получить выходные данные узла.\n",
    "\n",
    "Функции активации делятся на два типа:\n",
    "\n",
    "* линейные функции активации;\n",
    "* нелинейные функции активации.\n",
    "\n",
    "**Сигмоида (Sigmoid)** — возрастающая нелинейная функция, имеющая форму буквы «S». В нейронных сетях она используется потому, что позволяет усиливать слабые сигналы.\n",
    "\n",
    "Проблемы Sigmoid-активации:\n",
    "\n",
    "* Нейроны с сигмоидой могут насыщаться и приводить к угасающим градиентам.\n",
    "* Не центрированы в нуле.\n",
    "* Дорого вычислять.\n",
    "\n",
    "![](https://lh5.googleusercontent.com/GUebtq1pIUBCABD57tcVaroBte8t8R5u_d_67_gIVh8qviON0YzEX0Pa5B3M5xNX-dgzVfXRNanLKvF8UrapFjfzOC3LPfk6_tdm79K1wz64m5f8pwuUGNDnCQ0IS4z4yfrDO2jY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция Tanh** похожа на сигмоиду, но её преимущество состоит в том, что отрицательные входные данные будут отображаться строго отрицательными, а нулевые входные данные будут отображаться вблизи нуля.\n",
    "\n",
    "Характеристики Tanh активации:\n",
    "\n",
    "* центрирована в нуле;\n",
    "* но все ещё как сигмоида.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/ym0u8XXlxMKyFpBQTGgwFtEL9Ud5k4TBeHJu-4Nn3UqeMQx66ROD1LESkMaESnZ3sRIF0zhIBcmuVl6LLoqwWPmSEa6UBfq36GCIv3RuIBcFz4Xsf3Of08sgVV4S_i7bVetaABeB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ReLU (rectified linear unit)** является наиболее часто используемой функцией активации в мире, так как она используется практически во всех свёрточных нейронных сетях или для Deep Learning.\n",
    "\n",
    "Характеристики ReLU активации:\n",
    "\n",
    "* быстро считается;\n",
    "* градиенты не угасают при х > 0;\n",
    "* не центрирована в нуле;\n",
    "* если не было активации, не будет обновления. \n",
    "\n",
    "![](https://lh5.googleusercontent.com/nM2HgGZV2wpgjeZn6zzeSu6e5UJXFRXMifXJL8aAGy0nZOFm9xTiQjB3RYaNY2EGK8aZk7Bb8Gd61pjZfqdFaGdplV3bhJA_O2oJmxSDUYDOolcBsN2bWV9vhvN_FQ0R4R7vC-92)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leaky ReLU** является попыткой решить проблему выхода ReLU из строя.\n",
    "\n",
    "Характеристики Leaky ReLU-активации:\n",
    "\n",
    "* всегда будут обновления;\n",
    "* примерно центрирована в нуле;\n",
    "* а ≠ 1.\n",
    "\n",
    "![](https://lh6.googleusercontent.com/MKlNLAjwKZfDyWIc2IW008IG6dcldDlPclIfNmwuFMlNPN7W7FX-zcxKyhwB7k4cSu3NkEewx_EBDRlgItpDwhlrQHnlqjkNrBjwT313BHgrPCbqwFG76YpqUVITLXJthElZHhsr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция активации ELU** (Exponential Linear Unit), по результатам исследований, быстрее сводит к нулю и даёт более точные результаты.\n",
    "\n",
    "В отрицательной части аргументов использует экспоненту.\n",
    "\n",
    "![](https://lh4.googleusercontent.com/7KJzWSA8nungmCEveVUjb4EFil2My660uPnzP8u08VhcwtoC_ZXfeWuS0vH-VL8dMhRSAe1UVti3UfkgYlYVA9FOZt0MV6VOad-6A_daTzpJS3Jo_Bw3jVs0kSoeQJzFLyTCCL3h)\n",
    "\n",
    "Характеристики ELU:\n",
    "\n",
    "* примерно центрирована в нуле;\n",
    "* сходимость быстрее ReLU.\n",
    "\n",
    "![](https://lh5.googleusercontent.com/TIutETjeAVISSvdtoQ_3-TEy7V3CopaPJLNno6aKJ0WGI-vfbHohQgtu4guNnbDmUcUD8lpJwXkXHNWo_ajIbuJolTNQFMOUsqVJ8CpeFx8tddnDXuio_MG4bm7Bebps1RK3iyr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Fast and accurate deep network learning by exponential linear units (ELUs)](https://arxiv.org/pdf/1511.07289.pdf)»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Инициализация весов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронные сети нельзя инициализировать нулевыми весами, так как в этом случае появляется эффект симметрии и, как следствие, сеть становится менее мощной.\n",
    "\n",
    "## Нейрон и дисперсия до активации\n",
    "\n",
    "Если у каждого входа нейросети среднее равно 0, и мы генерируем веса независимо от входов, то это нам гарантирует, что **среднее нейрона до активации тоже будет равно 0**. \n",
    "\n",
    "Но дисперсия может расти, и это замедлит сходимость.\n",
    "\n",
    "![](https://lh5.googleusercontent.com/tcoq3wxOUHVARW8gQYq6WFH9SnN8OTCKDemuBwFpp_Zld3_rO3m_Dh8l7c2MrAt0kgTE9oypNJ9gVzRr5sumbm2kEMJSYcfkdrq1xr0Hy9wNGhInokgMs5cw7an9qM8l--UJNPEg)\n",
    "\n",
    "**Дисперсия суммы некоррелированных величин** — это сумма дисперсий, слагаемые некоррелированы, потому что веса, которые мы генерируем, независимы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сходимость\n",
    "\n",
    "Гиперболический тангенс для маленьких функций (около 0) похож на линейную функцию, то есть наши ранние выкладки примерно верны, и это означает, что, так как мы гарантируем одинаковую дисперсию на разных слоях нейросети, то выход будет распределён примерно одинаково после применения функции активации и, следовательно, градиенты будут в примерно одинаковом масштабе. Этот факт позволит **ускорить сходимость**.\n",
    "\n",
    "## Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам:\n",
    "\n",
    "* ознакомиться с содержанием «[Computacion Inteligente Derivative-Based Optimization](https://slideplayer.com/slide/7341917/)»;\n",
    "* прочесть раздел «[Sum of uncorrelated variables (Bienaymé formula)](https://en.wikipedia.org/wiki/Variance)»;\n",
    "* изучить статью «[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Влияние learning rate и масштаба признаков на сходимость"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ИСПОЛЬЗУЕМЫЙ В ВИДЕО NOTEBOOK](https://lms-cdn.skillfactory.ru/assets/courseware/v1/d0571757d0b1bc88377a888adbcd4e8b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dl_mod_4_3.ipynb)\n",
    "\n",
    "Рассмотрим, как влияет масштаб признаков в линейной регрессии на сходимость градиентных методов.\n",
    "\n",
    "Начнём с импорта библиотек. Здесь нам понадобится TensorFlow для визуализации процесса градиентного спуска и вспомогательные функции, которые будут отрисовывать анимацию.\n",
    "\n",
    "### Задача регрессии:\n",
    "\n",
    "У нас есть два признака: $x_1$,$x_2$, на которых настроена линейная модель для предсказания $y$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/5e7837f20ed9df05d625a9f6b11db059/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_03.jpg)\n",
    "\n",
    "Среднеквадратичное отклонение будем рисовать в пространстве параметров при помощи линий уровня. **Линия уровня** — это множество точек, на которых функция принимает одно и то же значение.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/80b18a9236e3083b5fe31942cb8d683a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_02.jpg)\t\n",
    "\n",
    "Глядя на линии уровня, мы можем представить, как устроена трёхмерная поверхность функции двух переменных $w_1$ и $w_2$.\n",
    "\n",
    "Красная точка на рисунке — это оптимальный набор параметров.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как выглядят матрицы W1 и W2. Для расчёта функции потерь вытянем их в вектор:\n",
    "\n",
    "```py\n",
    "Z = np.mean((x.dot(np.vstack([W1.ravel(), W2.ravel()])) - y)**2, axis=0).reshape(W1.shape)\n",
    "```\n",
    "\n",
    "Рассмотрим, как изменятся линии уровня, если ось x умножить на коэффициент 2, например:\n",
    "\n",
    "```py\n",
    "x[:, 0] *= 2  # признак x1 в x_scale раз больше (создает долины в лоссе)\n",
    "```\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/938da5668e0b82ce3acde18177ff0b1a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_04.jpg)\t\n",
    "\n",
    "Эллипс сузился по горизонтальной оси. \n",
    "\n",
    "Если мы будем увеличивать масштаб $x_1$, это будет сплющивать ось $w_1$ на графике линий уровня.\n",
    "\n",
    "Если $x_1$ — большое число, то при небольшом изменении w1 лосс изменится сильно.\n",
    "\n",
    "Можно посчитать градиенты функций потерь по $w_1$ и $w_2$:\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/77c6a73e885ed95f788c5aa3dc7f7ffc/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_05.jpg)\n",
    "\n",
    "Когда $x_1$ и $x_2$ будут иметь сильно разный масштаб, градиенты по параметрам тоже будут иметь сильно разный масштаб. И это именно то, что будет ломать градиентный спуск.\n",
    "\n",
    "Далее разберём функцию, которая будет рисовать градиентный спуск, и также поэкспериментируем с линиями уровня."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Batch-нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После инициализации и запуска backpropagation нет гарантий, что дисперсия не будет расти. Для нормирования выходов нейронов (до и после активации) можно использовать **Batch-нормализацию**.\n",
    "\n",
    "Batch-нормализация пытается нормировать каждый выход нейрона таким образом, чтобы у него были нулевое среднее и фиксированная дисперсия.\n",
    "\n",
    "Экспоненциальное сглаживание работает следующим образом: когда мы получаем среднее по батчу и Var по батчу, мы добавляем их в накопитель с некоторым весом, а старое значение в накопителе немного дискаунтируем.\n",
    "\n",
    "### Что делать с γ и β?\n",
    "\n",
    "**γ** и **β** нужны для масштабирования и сдвига активации: нейросети может быть полезно иметь, например,  не единичную дисперсию для какого-то нейрона, а большую, что поможет ей сойтись лучше.\n",
    "\n",
    "Линейная комбинация каких-то других входов легко дифференцируется, поэтому эти коэффициенты можно добавить в  backpropagation, по ним будут считаться градиенты, и они будут автоматически подобраны."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Dropout-регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout-регуляризация** на каждом шаге backpropagation **семплирует** сеть и с вероятностью $р$ оставляет каждый нейрон, но с вероятность $1 - р$ заменяет его на $0$. Таким образом сеть подстраивается по частям.\n",
    "\n",
    "\n",
    "\n",
    "### Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Производная функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В зависимости от того, какую величину шага в градиентном спуске *learning rate* вы выберете (большую или маленькую), вариации градиентного спуска будут выглядеть по-разному:\n",
    "\n",
    "* Если *learning rate* слишком большой, это означает, что сначала оптимизация была, но остановилась и качество становится хуже, мы выходим из оптимальной точки.\n",
    "* Если *learning rate* слишком маленький, то обучение слишком медленное.\n",
    "* Если *learning rate* недостаточно большой, то поначалу обучение идёт отлично, а потом мы застреваем и не двигаемся дальше.\n",
    "* Если *learning rate* оптимальный, то мы обучаемся быстро вначале, и к концу мы выходим на оптимальную точку.\n",
    "\n",
    "![](https://lh4.googleusercontent.com/QjN7JzwYrWpqrPrbHwlVAPAF70XfLqOO06m8F0yPbxgSThUfC9h7CPnpy24hJ6IvKxLD2-itMzfmmwQIxJldSGu95uFFNbRCH7pC2SXZD8FSPvjh3AaCIqONWMB-EiaBFQ8Cq_cU)\n",
    "\n",
    "**Почему стоит снижать *learning rate* с каждой итерацией?** Если уменьшить *learning rate*, когда вы увидели, что обучение больше не происходит, то сеть ещё чуть-чуть доучится.\n",
    "\n",
    "Проблема градиентного спуска — он застревает в локальных минимумах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Дополнительные материалы\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статьи:\n",
    "\n",
    "* «[Beyond SGD: Gradient Descent with Momentum and Adaptive Learning Rate](https://wiseodd.github.io/techblog/2016/06/22/nn-optimization/)»\n",
    "* «[Life is gradient descent](https://hackernoon.com/life-is-gradient-descent-880c60ac1be8)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Стохастический градиентный спуск (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Стохастический градиентный спуск (SGD)**  является итерационным методом оптимизации с дифференцируемой целевой функцией, суть градиентного спуска — минимизировать функцию, делая небольшие шаги в сторону наискорейшего убывания функции.\n",
    "\n",
    "Плюсы SGD:\n",
    "\n",
    "*  позволяет быстрее делать шаги и быстрее сходиться;\n",
    "*  траектория становится более шумной, что помогает выпрыгивать из локальных оптимумов.\n",
    "\n",
    "**RMSProp — адаптивный шаг.** *RMSProp — метод*, в котором скорость обучения адаптируется для каждого из параметров.\n",
    "\n",
    "Градиентные методы медленно сходятся, если градиенты по разным параметрам в разном масштабе. Как их привести в один масштаб? Если мы делали несколько маленьких шагов по какой-то переменной, то можно увеличивать шаг, чтобы не делать оптимизацию слишком медленной по некоторым переменным. То есть мы делим скорость обучения для веса на скользящее среднее значение после градиентов для этого веса.\n",
    "\n",
    "**Adam — Adaptive Moment Estimation.** Adam — метод, в котором сочетаются инерция и адаптивность шага. Если перед вами стоит выбор вариации SGD, то выбирайте Adam.\n",
    "\n",
    "**Дополнительные материалы**\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статьи:\n",
    "\n",
    "* [«Why Momentum Really Works»](https://distill.pub/2017/momentum/)\n",
    "* [«An overview of gradient descent optimization algorithms»](http://ruder.io/optimizing-gradient-descent/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Adam – Adaptive Moment Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ИСПОЛЬЗУЕМЫЙ В ВИДЕО NOTEBOOK](https://lms-cdn.skillfactory.ru/assets/courseware/v1/63072142684a125cc105741c1c19035a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/skillfactory_dl_4_screencast.ipynb)\n",
    "\n",
    "В предыдущей задаче оптимизации MSE мы остановились на том, что при увеличении learning rate возникает осциллирование траектории.\n",
    "\n",
    "Теперь мы рассмотрим на практике Adam. Запустим его на задаче, в которой признаки (а значит, и градиент) приблизительно одинакового масштаба. \n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/ea49ce24260b133bb0d5584be86aa53a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_06.jpg)\n",
    "\n",
    "Сплющив вертикальную ось, заметим, что линии уровня станут сплюснутыми, а Adam сходится всё так же быстро.\n",
    "\n",
    "```py\n",
    "# узкие долины функции\n",
    "plot_gd(x_scale=0.5, lr=0.1, steps=25, optimizer='adam')\n",
    "```\n",
    "\n",
    "Всё дело в адаптивном шаге, что нормирует градиенты и позволяет оптимально двигаться во всех направлениях.\n",
    "\n",
    "Задавая ещё более узкие долины, убедимся в этом.\n",
    "\n",
    "Теперь увеличим learning rate в 20 раз.\n",
    "\n",
    "```py\n",
    "# сломает ли его шаг побольше?\n",
    "# слишком большие шаги, закручиваемся вокруг решения из-за момента\n",
    "plot_gd(x_scale=0.2, lr=2.0, steps=25, optimizer='adam')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что Adam шагает слишком далеко, и его траектория выглядит курьёзно.\n",
    "\n",
    "![](https://lms-cdn.skillfactory.ru/assets/courseware/v1/f8268b5bb35913f197d0264d8e99dc14/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MAT_3_unit_07.jpg)\n",
    "\n",
    "Но, тем не менее, в итоге мы сошлись к самому хорошему решению.\n",
    "\n",
    "*Learning rate* — это гиперпараметр, который нужно подбирать, и всё, конечно, зависит от того, как выглядит форма Loss-функции.\n",
    "\n",
    "**Дополнительные материалы**\n",
    "\n",
    "В качестве дополнительной литературы рекомендуем вам прочесть статью «[Preconditioning the Network](https://cnl.salk.edu/~schraudo/teach/NNcourse/precond.html)»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  9. Матричные операции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерфейс прямого прохода\n",
    "\n",
    "Для прямого прохода интерфейс очень простой — это функция, которая принимает вход и генерирует выход, то есть один вход и один выход.\n",
    "\n",
    "**Обратный проход** производится по графу из производных, то есть, когда мы идём по нему в обратную сторону, мы рассчитываем с помощью цепного правила производные по всем параметрам, которые есть в сети.\n",
    "\n",
    "Чтобы реализовать Backward pass для новой вершины, нужно реализовать функцию, у которой есть входы —  то, что использовалось как вход сигмоиды во время прямого прохода. Так как необходимо считать производную в какой-то точке, сначала  backpropagation делает прямой проход, запоминает все значения аргументов, а потом использует их во время обратного прохода, чтобы рассчитать производные функций в нужных точках.\n",
    "\n",
    "![](https://lh5.googleusercontent.com/pxJn7e1eaLBvSpdEwQXx1dOZnM_0pD2w_amh71T8TQh6Fdbih6nwLjh1LlPzsC8drEwQy4sPP5aiv4xI9yiYdFhP0l7FfwRRi0qjvFzqCixOS6M0foWxAladvnCbz3G83GFw0hz5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязный слой как произведение матриц \n",
    "\n",
    "Матричные операции используются часто, они реализованы быстро. Существуют такие пакеты, как **CPU (BLAS)** и **GPU (cuBLAS)**, которые производят численные операции очень быстро, используя векторные операции процессора.\n",
    "\n",
    "### Обратный проход\n",
    "\n",
    "Для обратного шага необходимо посчитать производную наших потерь (обычно это скалярное значение) по каждому весу, который мы использовали.\n",
    "\n",
    "### Быстрая реализация в NumPy\n",
    "\n",
    "Производит матричные операции очень быстро, не на Python, и NumPy можно настроить таким образом, чтобы он использовал те же инструкции, что заложены в пакетах BLAS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В домашнем задании мы продолжим работать над классификацией изображений в датасете CIFAR10, пробуя некоторые трюки по оптимизации нейронных сетей.\n",
    "\n",
    "Для выполнения домашнего задания сохраните [Jupyter notebook](https://colab.research.google.com/drive/1rAnPF4UDSlFrSBE42W8Cin1n5QObLdcJ) с инструкциями по решению задачи на свой Google Диск, с которого вы сможете работать на Google Colaboratory. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
