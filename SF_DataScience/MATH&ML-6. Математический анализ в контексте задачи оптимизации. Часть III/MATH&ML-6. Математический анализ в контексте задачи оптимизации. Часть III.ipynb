{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MATH&ML-6. Математический анализ в контексте задачи оптимизации. Часть III**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ Мы подошли к заключительной части раздела, посвящённого математическому анализу и оптимизации. В предыдущих модулях мы уже познакомились с функциями и их исследованием, научились находить минимальные и максимальные значения для функций, а также применять эти значения для решения реальных задач. В данном модуле мы продолжим углубляться в методы оптимизации, сравним их и обсудим, для каких случаев какие алгоритмы подходят лучше всего.\n",
    "\n",
    "За два прошлых модуля мы узнали достаточно много важных понятий и инструментов — давайте повторим их ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.1\n",
    "\n",
    "Пусть прибыль в вашей компании выражается следующей функцией, которая зависит от параметра  — количества производимых товаров:\n",
    "\n",
    "$f(x)=-x^{4}+6 x^{3}-4 x^{2}+80$\n",
    "\n",
    "Найдите максимально возможную прибыль, которую вы можете получить, варьируя количество произведённых товаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max(f(x)) =  144.0\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "\n",
    "x = symbols('x')\n",
    "\n",
    "f = -x**4 + 6*x**3 - 4*x**2 + 80\n",
    "\n",
    "df = diff(f, x)\n",
    "\n",
    "x_sol = solve(Eq(df, 0), x)\n",
    "\n",
    "f_sol = []\n",
    "\n",
    "for xn in x_sol:\n",
    "    f_sol.append(float(f.subs(x, xn)))\n",
    "    \n",
    "f_max = max(f_sol)\n",
    "\n",
    "print('max(f(x)) = ', f_max)\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.4\n",
    "\n",
    "Допустим, вы хотите произвести некоторое количество товара, которое зависит от часов работы двух ключевых сотрудников следующим образом:\n",
    "\n",
    "$f(x, y)=x^{2}+2 y^{2}$\n",
    "\n",
    "Однако вы можете оплатить этим сотрудникам не более 20 часов работы.\n",
    "\n",
    "Какое наибольшее количество товаров вы сможете произвести в таком случае?\n",
    "\n",
    "Помните, что количество часов работы должно быть целым, поэтому, прежде чем вычислять итоговый результат, округлите часы работы до целого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{x: 40/3, y: 20/3, l: -80/3}\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 267$"
      ],
      "text/plain": [
       "267"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, l = symbols('x y l')\n",
    "\n",
    "f = x**2 + 2*y**2 + l*(x + y - 20)\n",
    "\n",
    "dx = f.diff(x)\n",
    "dy = f.diff(y)\n",
    "dl = f.diff(l)\n",
    "\n",
    "eq1 = Eq(dx, 0)\n",
    "eq2 = Eq(dy, 0)\n",
    "eq3 = Eq(dl, 0)\n",
    "\n",
    "sol = solve([eq1, eq2, eq3], [x, y, l])\n",
    "\n",
    "print(sol)\n",
    "\n",
    "f.subs([(x, 40/3), (y, 20/3), (l, -80/3)]).round()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы повторили пройденное — теперь можно приступать к дальнейшему изучению темы.\n",
    "\n",
    "## ЭТОТ МОДУЛЬ МЫ ПОСВЯТИМ ИЗУЧЕНИЮ НОВЫХ МЕТОДОВ ОПТИМИЗАЦИИ:\n",
    "\n",
    "* рассмотрим, какие вариации существуют у уже известного вам алгоритма градиентного спуска, и узнаем, в чём суть **обратного распространения ошибки**;\n",
    "* познакомимся с **методом Ньютона** и **квазиньютоновскими методами BFGS** и **L-BFGS**;\n",
    "разберём область применения задач линейного программирования и попрактикуемся в их решении;\n",
    "* узнаем, что такое **метод отжига** и **метод координатного спуска**.\n",
    "\n",
    "Из предыдущего модуля вы помните, что в каких-то методах оптимизации мы использовали лишь значение функции, где-то — считали градиент, а где-то — находили матрицы производных. Эти особенности дают возможность поделить все алгоритмы на три группы:\n",
    "\n",
    "* методы нулевого порядка (их работа основана на оценке значений самой целевой функции в разных точках);\n",
    "* методы первого порядка (при работе они используют первые производные в дополнение к информации о значении функции);\n",
    "* методы второго порядка (для них необходимо оценивать и значение функции, и значение градиента, и гессиан (матрицу Гессе)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Иногда в литературе можно встретить термины «оракул первого порядка» или «оракул нулевого порядка». Так обозначают компоненты алгоритма, которые находят информацию на каждом шаге для метода соответствующего порядка.\n",
    "\n",
    "В данном модуле мы будем рассматривать методы разных порядков и практически в каждом юните будем практиковаться в их применении. В конце мы обобщим полученные знания и сравним, какие методы и в каких случаях показывают наилучшие результаты.\n",
    "\n",
    "→ Важно отметить, что некоторые алгоритмы оптимизации мы пока не сможем использовать по прямому назначению: мы опробуем их на известных вам моделях линейной или логистической регрессии или на функциях одной или нескольких переменных, но основная сфера их применения — это искусственные нейронные сети. Тем не менее, понимание всего пула алгоритмов поможет вам в будущем без проблем подбирать необходимый метод вне зависимости от того, задачу какой сложности вы решаете: минимизируете простейшую функцию или обучаете нейронную сеть со сложной архитектурой."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Градиентный спуск: применение и модификации\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем модуле мы познакомились с алгоритмом градиентного спуска, а также с его очень популярной вариацией — градиентным спуском с momentum. Но на самом деле у градиентного спуска очень много модификаций, и, поскольку это действительно самый известный алгоритм, он является основой множества методов оптимизации. В этом юните мы рассмотрим и сравним три вариации градиентного спуска, которые используются наиболее часто.\n",
    "\n",
    "Чтобы лучше понимать, какую роль играют методы оптимизации в DS и почему их так много, важно понимать основные **принципы работы нейронных сетей**. Также в современных статьях и обзорах разных методов оптимизации они практически всегда рассматриваются в контексте применения в нейронных сетях.\n",
    "\n",
    "Процесс обучения человеческого мозга очень сложен, и наука пока не может дать достаточно подробный ответ на вопрос о том, как мы получаем и усваиваем знания, как принимаем решения. Однако той информации, которую уже удалось получить, оказалось достаточно, чтобы по аналогии создать модели искусственных нейронных сетей.\n",
    "\n",
    "Люди учатся через пробы и ошибки, через процесс **синаптической пластичности** (это понятие, которое используется для описания того, как формируются и укрепляются нейронные связи после получения новой информации). Точно так же, как связи в мозге укрепляются и формируются по мере того, как мы переживаем новые события, мы обучаем искусственные нейронные сети, вычисляя ошибки нейросетевых предсказаний и усиливая или ослабляя внутренние связи между нейронами на основе этих ошибок.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для лучшего понимания удобно воспринимать нейронную сеть как функцию, которая принимает входные данные для получения итогового прогноза. Переменными этой функции являются параметры, или веса, нейрона.\n",
    "\n",
    "Следовательно, ключевым моментом для решения задачи, которую мы ставим для нейронной сети, будет **корректировка значений весов** таким образом, чтобы они аппроксимировали или наилучшим образом представляли набор данных.\n",
    "\n",
    "На изображении ниже показана простая нейронная сеть, которая получает входные данные ($X_1, \\ X_2, \\  X_3, \\ X_n$). Эти входные данные передаются нейронам в слое, содержащем веса ($W_1, \\ W_2, \\ W_3, \\ W_n$). Входные данные и веса подвергаются операции умножения, и результат суммируется с помощью специальной функции, а функция активации регулирует конечный результат модели.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/604e30ced0fcce4fc8de6700e3712df1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы оценить, насколько эффективно работает наша нейронная сеть, необходим показатель оценки разницы между предсказанием нейронной сети и фактическим значением целевой функции, позволяющий корректировать параметры сети так, чтобы разница между прогнозом и реальностью была как можно меньше. В Data Science эту разницу часто называют **функцией стоимости**.\n",
    "\n",
    "Ниже представлена модель работы простейшей нейронной сети:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a8edf804d82a38c7dca11e9e3186d3d6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_2.png)\n",
    "\n",
    "На этом изображении мы можем видеть модель простой нейронной сети из плотно связанных нейронов, которая классифицирует рукописные представления цифр 0, 1, 2, 3. Каждый нейрон в выходном слое соответствует цифре. Чем выше активация соединения с нейроном, тем выше вероятность, выдаваемая нейроном. Вероятность соответствует вероятности того, что цифра, переданная вперёд по сети, связана с активированным нейроном.\n",
    "\n",
    "Когда цифра 3 передаётся через сеть, мы ожидаем, что соединения (представленные на диаграмме стрелками), ответственные за классификацию этой цифры, будут иметь более высокую активацию, что приводит к более высокой вероятности для выходного нейрона, связанного с цифрой 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если объяснить происходящее более простыми словами, то механизм работы примерно такой:\n",
    "\n",
    "1. На первом слое мы выделяем из цифры какие-то первичные признаки (округлости, палочки).\n",
    "2. На втором слое мы выделяем уже какие-то паттерны (фрагменты цифры).\n",
    "3. На третьем слое паттерны складываются в целую цифру и мы можем предсказать результат.\n",
    "Каждый раз элементы, более похожие на элементы цифры 3, дают более высокую активацию.\n",
    "\n",
    "За активацию нейрона отвечает несколько компонентов, и мы должны менять их в процессе обучения нашей нейронной сети, повышая качество её предсказания.\n",
    "\n",
    "Допустим, мы используем уже хорошо известную вам среднеквадратичную ошибку как меру для оценки качества модели. В этом случае мы каждый раз вычисляем её, получаем результат и отправляем его обратно для коррекции весов в сети. Здесь как раз и появляется понятие **обратного распространения ошибки**.\n",
    "\n",
    "> **Обратное распространение (backpropagation)** — это механизм, с помощью которого компоненты, влияющие на итоговый результат, итеративно корректируются для уменьшения функции стоимости.\n",
    "\n",
    "Важнейшим математическим процессом, связанным с обратным распространением, является вычисление производных. Операции обратного распространения вычисляют частную производную функции стоимости по отношению к весам и активациям предыдущего слоя, чтобы определить, какие значения влияют на градиент функции стоимости."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ В процессе минимизации  функции ошибки мы постоянно вычисляем значение градиентов и таким образом приходим к локальному минимуму. На каждом этапе обучения нейронной сети её веса пересчитываются с помощью найденного градиента, причём скорость обучения (или, мы уже называли её ранее, темп обучения или шаг градиентного спуска) определяет коэффициент, с которым вносятся изменения в значения весов. Это повторяется на каждом шаге обучения нейронной сети. Нашей целью является постоянное приближение к локальному минимуму.\n",
    "\n",
    "Принцип обратного распространения ошибки заключается в том, что сначала мы устанавливаем какие-то случайные веса (параметры) для модели, находим итоговую ошибку и движемся по сети обратно, корректируя веса (для этого вычисляем частные производные, т. е. градиент).\n",
    "\n",
    "Данный процесс выглядит следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b19046193292c57cf06f2bc0532ddc9d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_3.gif)\n",
    "\n",
    "Распространение ошибок и использование частных производных для корректировки весов происходит до тех пор, пока не будут скорректированы все параметры в сети вплоть до первого слоя."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вам хочется больше узнать про механизм обратного распространения ошибки, рекомендуем обратиться к [этой статье](https://brilliant.org/wiki/backpropagation/).\n",
    "\n",
    "Как вы можете видеть, даже в самом простом примере нейронной сети много переменных и действий, которые с ними происходят. Из-за этого ландшафт функции потерь для нейронных сетей становится очень сложным. К примеру, ландшафт для нейронной сети с 56 слоями может выглядеть так:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/dcd9e0bd78da224572e761ce0358ead3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_4.png)\n",
    "\n",
    "В предыдущем модуле мы без проблем решили задачу с помощью классического градиентного спуска, и результат совпал с итогом, полученным с помощью реализации алгоритма в стандартной библиотеке Python. Но функция потерь там была совершенно другой. Когда дело доходит до искусственных нейронных сетей, алгоритмы оптимизации сталкиваются с множеством проблем."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Основные проблемы при реализации градиентного спуска:**\n",
    "\n",
    "* Классический градиентный спуск склонен застревать в точках локального минимума и даже в седловых точках, словом — везде, где градиент равен нулю. Это мешает найти глобальный минимум.\n",
    "* Обычно у оптимизируемой функции очень сложный ландшафт: где-то она совсем пологая, где-то более крутой обрыв. В таких ситуациях градиентный спуск показывает не лучшие результаты. Так происходит потому, что в алгоритме градиентного спуска фиксированный шаг, а нам в идеале хотелось бы его изменять в зависимости от формы функции прямо в процессе обучения.\n",
    "* Много проблем возникает из-за темпа обучения: при низком алгоритм сходится невероятно медленно, при быстром — «пролетает» мимо минимумов.\n",
    "* При обучении градиентного спуска координаты в некоторых измерениях могут редко изменяться, что приводит к плохой обобщающей способности алгоритма. Можно попытаться придать каждого признаку бόльшую важность, но в таком случае есть серьёзный риск переобучить модель.\n",
    "\n",
    "Тем не менее, нельзя отрицать, что градиентный спуск — невероятно эффективный и популярный алгоритм. Допустим, он прекрасно подходит для минимизации среднеквадратичной ошибки в случае решения задачи регрессии. Однако в силу его несовершенств, которые очень явно проявляются в сложных моделях (например, в нейронных сетях), были созданы некоторые его модификации, которые позволяют решать задачи с большей результативностью."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно выделяют **три основных вариации градиентного спуска**:\n",
    "\n",
    "* Batch Gradient Descent;\n",
    "* Stochastic Gradient Descent;\n",
    "* Mini-batch Gradient Descent.\n",
    "\n",
    "Далее мы рассмотрим все эти модификации и обсудим их различия. Важно понимать, что, несмотря на то что про их применение часто говорят именно в контексте нейронных сетей, они прекрасно подходят и для использования с обычными методами машинного обучения. К примеру, для стандартной линейной регрессии также подходит и обычный градиентный спуск.\n",
    "\n",
    "## BATCH GRADIENT DESCENT\n",
    "\n",
    "Первая вариация — это **Batch Gradient Descent**. По-русски её называют **пакетным градиентным спуском**, или **ванильным градиентным спуском** (хотя англоязычную вариацию Vanilla Gradient Descent чаще не переводят). По сути, это и есть классический градиентный спуск, который мы с вами рассматривали в предыдущем модуле.\n",
    "\n",
    "Пакетным его называют по той причине, что он использует всю выборку (весь пакет) на каждом шаге, для того чтобы получить результат.\n",
    "\n",
    "Слово \"vanilla\" в названии используют для того, чтобы указать, что это самый простой вариант, без «примесей». В английском языке такое название может применяться не только для градиентного спуска, но и для любого метода в целом. В русском языке такое название, как правило, чаще присутствует в дословных переводах англоязычных текстов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/ad02f9aeba9cab2f06cf437e01c411f8/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_5.png)\n",
    "\n",
    "Часто в различных источниках шаг Batch Gradient Descent записывается в следующих обозначениях:\n",
    "\n",
    "$$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J(\\theta)$$\n",
    "\n",
    "Примечание. На самом деле совершенно не важно, какими буквами выражать те или иные объекты, но мы будем использовать в этом модуле обозначения, которые часто встречаются в научных статьях и различной литературе.\n",
    "\n",
    "Здесь $\\theta$ — вектор с параметрами функции, $\\eta$ — шаг градиента, $\\nabla_{\\theta} J(\\theta)$ — градиент функции, найденный по её параметрам.\n",
    "\n",
    "Таким образом, на каждом шаге градиентный спуск находит направление наискорейшего убывания функции и движется чётко по нему. Поэтому для несложных функций он достаточно быстро сходится. Его обучение можно изобразить следующим образом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a4938996e0fb2e0f0a00193ad27c631d/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_6.png)\n",
    "\n",
    "Такой градиентный спуск достаточно хорошо работает, если мы рассматриваем выпуклые или относительно гладкие функции ошибки. Такие, к примеру, мы можем наблюдать у линейной или логистической регрессии. Мы обсуждали, что градиентный спуск не умеет находить глобальный минимум среди прочих, но, например, для выпуклой функции он может это сделать. Поэтому с выпуклыми функциями (допустим, со среднеквадратичной ошибкой для линейной регрессии) нам удобнее всего использовать именно его.\n",
    "\n",
    "Но всё усложняется, когда мы хотим применить его при обучении нейронных сетей. Как уже говорилось, у таких моделей функция потерь имеет много локальных экстремумов, в каждом из которых градиентный спуск может застрять. Когда данных очень много, а оптимизируемая функция очень сложная, данный алгоритм применять затруднительно, так как поиск градиента по всем наблюдениям делает задачу очень затратной в плане вычислительных ресурсов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOCHASTIC GRADIENT DESCENT\n",
    "\n",
    "Представим, что мы реализуем градиентный спуск для набора данных объёмом 10 000 наблюдений и у нас десять переменных. Среднеквадратичную ошибку считаем по всем точкам, то есть для 10 000 наблюдений. Производную необходимо посчитать по каждому параметру, поэтому фактически за каждую итерацию мы будем выполнять не менее 100 000 вычислений. И если, допустим, у нас 1000 итераций, то нам нужно 100000*1000=100000000 вычислений. Это довольно много, поэтому градиентный спуск на сложных моделях и при использовании больших наборов данных работает крайне долго.\n",
    "\n",
    "Чтобы преодолеть эту проблему, придумали **стохастический градиентный спуск.** Слово «стохастический» можно воспринимать как синоним слова «случайный». Где же при использовании градиентного спуска может возникнуть случайность? При выборе данных. При реализации стохастического спуска вычисляются градиенты не для всей выборки, а только для случайно выбранной единственной точки.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/34c59eb72fab899262cf5514d4981e6a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_7.png)\n",
    "\n",
    "Это значительно сокращает вычислительные затраты.\n",
    "\n",
    "В виде формулы это можно записать следующим образом:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i)} ; y^{(i)}\\right)$\n",
    "\n",
    "На визуальном представлении ниже можно увидеть, что стохастический спуск создаёт много колебаний при сходимости. Это происходит как раз за счёт того, что берётся не вся выборка, а только один объект, и между объектами может быть достаточно большая разница. Чем меньше выборка, тем меньше стабильности при реализации.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/aa52e836e3a7b359b496351d8a1c4b62/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_8.png)\n",
    "\n",
    "Стохастический градиентный спуск очень часто используется в нейронных сетях и сокращает время машинных вычислений, одновременно повышая сложность и производительность крупномасштабных задач."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINI-BATCH GRADIENT DESCENT\n",
    "\n",
    "Третья вариация градиентного спуска — **Mini-batch Gradient Descent**. Также можно называть его **мини-пакетным градиентным спуском**. По сути, эта модификация сочетает в себе лучшее от классической реализации и стохастического варианта. На данный момент это наиболее популярная реализация градиентного спуска, которая используется в глубоком обучении (т. е. в обучении нейронных сетей).\n",
    "\n",
    "В ходе обучения модели с помощью мини-пакетного градиентного спуска обучающая выборка разбивается на пакеты (**батч**и), для которых рассчитывается ошибка модели и пересчитываются веса.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/29566a5834ca122fa8aa342003c67441/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_9.png)\n",
    "\n",
    "То есть, с одной стороны, мы используем все преимущества обычного градиентного спуска, а с другой — уменьшаем сложность вычислений и повышаем их скорость по аналогии со стохастическим спуском. Кроме того, алгоритм работает ещё быстрее за счёт возможности применения векторизованных вычислений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формализовать это можно следующим образом:\n",
    "\n",
    "$\\theta=\\theta-\\eta \\cdot \\nabla_{\\theta} J\\left(\\theta ; x^{(i: i+n)} ; y^{(i: i+n)}\\right)$\n",
    "\n",
    "Как показывает визуализация ниже, амплитуда колебаний при сходимости алгоритма больше, чем в классическом градиентном спуске, но меньше, чем в стохастическом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e4307771553e518dbc2d85c4ac80218a/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_2_10.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резюмируем сравнение трёх вариаций градиентного спуска:\n",
    "\n",
    "BATCH GRADIENT DESCENT|STOCHASTIC GRADIENT DESCENT|MINI-BATCH GRADIENT DESCENT\n",
    "-|-|-\n",
    "Рассматриваются все обучающие данные|Рассматривает только один объект|Рассматривается подвыборка\n",
    "Затрачивает много времени на работу|Работает быстрее пакетного|Работает быстрее двух других\n",
    "Плавное обновление параметров модели|Сильные колебания в обновлении параметров модели|Колебания зависят от размера подвыборки (увеличиваются с уменьшением её объема)\n",
    "\n",
    "✍ Итак, мы рассмотрели алгоритмы градиентного спуска и сравнили их параметры. Теперь, если вам необходимо будет выбрать метод градиентного спуска, вы сможете понять, какой из них подходит лучше всего."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.7\n",
    "\n",
    "Давайте потренируемся применять стохастический градиентный спуск для решения задачи линейной регрессии. Мы уже рассмотрели его реализацию «с нуля», однако для решения практических задач можно использовать готовые библиотеки.\n",
    "\n",
    "Загрузите стандартный датасет об алмазах из библиотеки Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = sns.load_dataset('diamonds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалите часть признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['depth', 'table', 'x', 'y', 'z'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируйте категориальные признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Логарифмируйте признаки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['carat'] = np.log(1+df['carat'])\n",
    "df['price'] = np.log(1+df['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определите целевую переменную и предикторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cols = [col for col in df.columns if col!='price'] \n",
    "X = df[X_cols]\n",
    "y = df['price']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите выборку на обучающую и тестовую (объём тестовой возьмите равным 0.33)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь реализуйте алгоритм линейной регрессии со стохастическим градиентным спуском (функция SGDRegressor). Отберите с помощью gridsearch() оптимальные параметры по следующей сетке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36139, 18)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.0026826957952797246, eta0=0.001, l1_ratio=0.0,\n",
      "             learning_rate='constant', loss='epsilon_insensitive',\n",
      "             max_iter=21.544346900318832, penalty='elasticnet',\n",
      "             random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "estimator = SGDRegressor(random_state = 42)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': ['constant'],\n",
    "    'eta0': np.logspace(-4,-1,4),\n",
    "    'max_iter': np.logspace(0,3,10),\n",
    "    'loss': ['squared_error', 'epsilon_insensitive'],\n",
    "    'penalty': ['elasticnet'], \n",
    "    'alpha': np.logspace(-3,3,15),\n",
    "    'l1_ratio': np.linspace(0,1,11)}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите регрессию с оптимальными параметрами. В качестве ответа введите получившееся значение MSE, предварительно округлив его до третьего знака после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество по MSE: 0.043\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(f'Качество по MSE: {round(mean_squared_error(y_test, y_pred), 3)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *БОНУС: АЛГОРИТМЫ, ОСНОВАННЫЕ НА ГРАДИЕНТНОМ СПУСКЕ\n",
    "\n",
    "Градиентный спуск настолько популярен и хорошо применим для решения различных задач, что послужил основой множества дополнительных методов. Поговорим про некоторые из них.\n",
    "\n",
    "Иногда в данных присутствуют очень редко встречающиеся входные параметры.\n",
    "\n",
    "Например, если мы классифицируем письма на «Спам» и «Не спам», таким параметром может быть очень специфическое слово, которое встречается в спаме намного реже других слов-индикаторов. Или, если мы говорим о распознавании изображений, это может быть какая-то очень редкая характеристика объекта.\n",
    "\n",
    "В таком случае нам хотелось бы иметь для каждого параметра свою скорость обучения: чтобы для часто встречающихся она была низкой (для более точной настройки), а для совсем редких — высокой (это повысит скорость сходимости). То есть нам очень важно уметь обновлять параметры модели, учитывая то, насколько типичные и значимые признаки они кодируют.\n",
    "\n",
    "Решение этой задачи предложено в рамках алгоритма AdaGrad (его название обозначает, что это адаптированный градиентный спуск). В нём обновления происходят по следующему принципу:\n",
    "\n",
    "$G_t = G_t + g^2_t$\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t$\n",
    "\n",
    "Здесь мы храним сумму квадратов градиентов для каждого параметра. Таким образом, параметры, которые сильно обновляются каждый раз, начинают обновляться слабее. Скорость обучения в таком алгоритме будет постоянно затухать. Мы будем начинать с больших шагов, и с приближением к точке минимума шаги будут уменьшаться — это улучшит скорость сходимости.\n",
    "\n",
    "Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный алгоритм достаточно популярен и работает лучше стохастического градиентного спуска. Его использует и компания Google в своих алгоритмах классификации изображений.\n",
    "\n",
    "Однако снижение скорости обучения в AdaGrad иногда происходит слишком радикально, и она практически обнуляется. Чтобы решить эту проблему, были созданы алгоритмы RMSProp, AdaDelta, Adam и некоторые другие.\n",
    "\n",
    "Если вам интересно подробнее узнать о перечисленных алгоритмах и окунуться в процесс оптимизации нейронных сетей, рекомендуем обратиться к следующим статьям:\n",
    "\n",
    "* [\"An overview of gradient descent optimization algorithms\"](https://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n",
    "* [«Методы оптимизации нейронных сетей»](https://habr.com/ru/post/318970/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Метод Ньютона"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы познакомились с различными вариациями градиентного спуска. Но для того чтобы наш арсенал методов был более полным и позволял решать самые разные задачи, нам необходимо разобраться и с рядом других алгоритмов. В этом юните мы будем изучать **метод Ньютона**.\n",
    "\n",
    "Метод Ньютона используется во многих алгоритмах машинного обучения. Часто в литературе его сравнивают с градиентным спуском, так как два этих алгоритма очень популярны. Вы уже сталкивались с методом Ньютона, но не знали об этом.\n",
    "\n",
    "В [документации](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) для функции LogisticRegression из библиотеки scikit-learn представлено пять вариантов алгоритмов оптимизации, которые можно использовать при обучении модели:\n",
    "\n",
    "* 'newton-cg';\n",
    "* 'lbfgs';\n",
    "* 'liblinear';\n",
    "* 'sag';\n",
    "* 'saga'.\n",
    "\n",
    "Последние два являются вариациями стохастического градиентного спуска (а значит вам уже понятен принцип их работы), а с первыми тремя нам только предстоит познакомиться. В этом юните мы рассмотрим алгоритм 'newton-cg', в следующем — 'lbfgs', а в седьмом юните — 'liblinear'. Вы будете понимать суть всех методов, представленных в самой популярной библиотеке для машинного обучения, и выбирать подходящий, исходя из особенностей поставленной задачи.\n",
    "\n",
    "Начнём с метода Ньютона. Этот алгоритм работает быстрее, чем градиентный спуск, и тратит меньше времени для достижения минимума, однако у него есть и определённые недостатки, о которых мы поговорим позже."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Метод Ньютона изначально появился как метод решения уравнений вида $f(x)=0$.\n",
    "\n",
    "Проиллюстрируем принцип его работы геометрически. Пусть у нас есть график некоторой функции. Проведём к нему касательную в точке $x_n$.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5a3c5135a9e4da28d101707c4ed16b91/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_3_1.png)\n",
    "\n",
    "Тогда эта касательная имеет наклон, равный $f'(x_n)$, и проходит через точку $x_n, f(x_n)$. В таком случае мы можем сказать, что уравнение этой касательной: $y = f'(x_n) (x - x_n) + f(x_n)$.\n",
    "\n",
    "Так как нам необходимо решить уравнение, то нужно попасть в такую точку $x_{n+1}$, чтобы в ней значение координаты по оси ординат было нулевым, то есть в точку с координатами $x_{n+1}$ и $y=0$.\n",
    "\n",
    "Подставим это в наше уравнение касательной:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=f^{\\prime}\\left(x_{n}\\right)\\left(x-x_{n}\\right)+f\\left(x_{n}\\right)$\n",
    "\n",
    "$x=x_{n+1}, y=0$\n",
    "\n",
    "$0=f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)+f\\left(x_{n}\\right)$\n",
    "\n",
    "$f^{\\prime}\\left(x_{n}\\right) \\cdot\\left(x_{n+1}-x_{n}\\right)=-f\\left(x_{n}\\right)$\n",
    "\n",
    "$x_{n+1}-x_{n}=-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}$\n",
    "\n",
    "$x_{n+1}=x_{n}{-\\frac{f\\left(x_{n}\\right)}{f^{\\prime}\\left(x_{n}\\right)}}$\n",
    "\n",
    "Можно посмотреть на это и в анимации:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/b2f35525a4938a45e4d72610779be3c5/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_3_2.gif)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, как для $x$ вычисляется $f(x)$, строится касательная, и в точке пересечения касательной с осью $Ox$ строится новая точка, к которой также строится касательная, и так далее. Математически доказано, что таким образом $x_i$ приближается к значению, где $f(x)=0$. \n",
    "\n",
    "Формально первый  шаг этого алгоритма мы можем записать следующим образом:\n",
    "\n",
    "$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "Все остальные шаги можно обобщить с помощью следующей зависимости:\n",
    "\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "\n",
    "Шаги могут повторяться сколько угодно раз до достижения необходимой точности.\n",
    "\n",
    "Давайте посмотрим, как с использованием этого метода можно решить уравнение ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 1***\n",
    "\n",
    "*Найти корень уравнения $x^2 - 4x - 7 = 0$, который находится рядом с точкой $x=5$, с точностью до тысячных.*\n",
    "\n",
    "Функция: $f(x) = x^2 - 4x - 7 = 0$\n",
    "\n",
    "Начальная точка: $x_0 = 5$\n",
    "\n",
    "Производная для функции: $f'(x) = 2x - 4$\n",
    "\n",
    "Начнём поочерёдно совершать шаги и переходить в следующие точки, используя формулу, которую мы рассмотрели ранее:\n",
    "\n",
    "$x_1 = 5 - \\frac{5^2 - 4 \\times 5 - 7}{2 \\times 5 - 4} = 5 - \\frac{(-2)}{6}  = \\frac{16}{3} \\approx 5.33333$\n",
    "\n",
    "$x_2 = \\frac{16}{3} - \\frac{(\\frac{16}{3})^2 - 4 (\\frac{16}{3}) - 7}{2 (\\frac{16}{3}) - 4} = \\frac{16}{3} - \\frac{\\frac{1}{9}}{\\frac{20}{3}} = \\frac{16}{3} - \\frac{1}{60} = \\frac{319}{60} \\approx 5.31667$\n",
    "\n",
    "$x_3 = \\frac{319}{60} - \\frac{(\\frac{319}{60})^2 - 4 (\\frac{319}{60}) - 7}{2 (\\frac{319}{60}) - 4} = \\frac{319}{60} - \\frac{\\frac{1}{3600}}{\\frac{398}{60}} \\approx 5.31662$\n",
    "\n",
    "Мы видим, что в последних двух точках мы уже находимся примерно в одном и том же месте. На всякий случай проверим ещё одну:\n",
    "\n",
    "$x_4 = 5.31662 - \\frac{(5.31662)^2 - 4 (5.31662) - 7}{2 (5.31662) - 4} = 5.31662$\n",
    "\n",
    "Действительно, мы нашли точку с точностью до тысячных, в которой останемся — это и будет нашим ответом."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 2***\n",
    "\n",
    "*Найти корни сложного полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$.*\n",
    "\n",
    "Как мы знаем, к сожалению, для полинома пятой степени нет формулы поиска корней, поэтому будем использовать численные методы. В этих случаях приходится прибегать к числовому линейному приближению.\n",
    "\n",
    "Ниже представлен график нашего полинома. У него три корня: в точках 0, 1 и где-то между ними. Как найти третий корень?\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7ff22f2332a716cf7b565feba5fa248b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_3_3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В методе Ньютона мы берём случайную точку $x_0$, а затем проводим касательную в ней. Точка $x_1$, где эта касательная пересекает ось абсцисс, станет нашим следующим предположением. Так что теперь мы  строим уже касательную в этой точке, и так далее . Мы продолжаем до тех пор, пока не достигнем необходимой точности. В целом, мы можем сделать приближение настолько близким к нулю, насколько хотим.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.1\n",
    "\n",
    "Найдите третий корень полинома $f(x) = 6x^5 - 5x^4 - 4x^3 + 3x^2$, взяв за точку старта 0.7. Введите получившееся значение с точностью **до трёх знаков после точки-разделителя**.\n",
    "\n",
    "Попробуйте реализовать алгоритм с использованием Python на основе алгоритма градиентного спуска, изученного в предыдущем модуле."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx = \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 30 x^{4} - 20 x^{3} - 12 x^{2} + 6 x$"
      ],
      "text/plain": [
       "30*x**4 - 20*x**3 - 12*x**2 + 6*x"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = symbols('x')\n",
    "\n",
    "f = 6*x**5 -5*x**4 - 4*x**3 + 3*x**2\n",
    "\n",
    "dx = f.diff(x)\n",
    "print('dx = ')\n",
    "display(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  0.629\n",
      "Iterations:  23\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 6*x**5 -5*x**4 - 4*x**3 + 3*x**2\n",
    "\n",
    "def dx(x):\n",
    "    return 30*x**4 - 20*x**3 - 12*x**x+6*x\n",
    "\n",
    "def newtons_method(f1, f2, x_0, n_round = 3, eps=0.0001, count=0, max_iter=1000):\n",
    "    \n",
    "    while true:    \n",
    "        count += 1\n",
    "        if count < max_iter:\n",
    "            x_n = x_0 - f1(x_0)/f2(x_0)\n",
    "            if abs(f(x_0) - f(x_n)) >= eps:\n",
    "                x_0 = x_n\n",
    "            else:\n",
    "                print('x = ', round(x_n, n_round))\n",
    "                print('Iterations: ', count)\n",
    "                break\n",
    "        else:\n",
    "            print('Max iter out')\n",
    "  \n",
    "newtons_method(f, dx, x_0=0.8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, мы научились искать приближённые значения для корней уравнения. Но как же это поможет нам найти минимум или максимум для функции?\n",
    "\n",
    "Дело в том, что в задаче оптимизации можно решать не $f(x)=0$, а $f'(x)=0$ — тогда мы найдём потенциальные точки экстремума.\n",
    "\n",
    "В многомерном случае по аналогичным рассуждениям производная превращается в градиент, а вторая производная превращается в гессиан (матрица вторых производных или, как мы её называли в предыдущем модуле, матрица Гессе). Поэтому в формуле появится обратная матрица.\n",
    "\n",
    "Для многомерного случая формула выглядит следующим образом:\n",
    "\n",
    "$$x^{(n+1)} = x^{(n)} - \\left [Hf(x^{(n)})  \\right ]^{-1} \\nabla f(x^{(n)})$$\n",
    "\n",
    "Можно заметить, что эта формула совпадает с формулой для градиентного спуска, но вместо умножения на learning rate (темп обучения) используется умножение на обратную матрицу к гессиану. Благодаря этому функция может сходиться за меньшее количество итераций, так как мы учитываем информацию о выпуклости функции через гессиан. Можно увидеть это на иллюстрации работы двух методов, где метод Ньютона явно сходится быстрее:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/5a6350807f0c71c3c77f61d3372cf322/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_3_4.png)\n",
    "\n",
    "Метод Ньютона, если считать в количестве итераций, в многомерном случае (с гессианом) работает быстрее градиентного спуска."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выше мы уже разобрали применение метода Ньютона для поиска корней уравнения. Теперь давайте снова используем его, но уже для оптимизации функции ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример:***\n",
    "\n",
    "*Оптимизируем функцию $f(x) = x^3 - 3x^2 -45x  + 40$.*\n",
    "\n",
    "Находим производную функции:\n",
    "\n",
    "$f^{\\prime}=3 x^{2} - 6x - 45$\n",
    "\n",
    "Находим вторую производную:\n",
    "\n",
    "$f^{\\prime \\prime}=6x - 6$\n",
    "\n",
    "Сразу определим их в Python, чтобы можно было параллельно решить задачу и с помощью программирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    return 3*x**2 - 6*x -45\n",
    "def func2(x):\n",
    "    return 6*x - 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь необходимо взять какую-нибудь изначальную точку. Например, пусть это будет точка $x=42$. Также нам необходима точность — её возьмем равной 0.0001. На каждом шаге будем переходить в следующую точку по уже упомянутой выше формуле:\n",
    "\n",
    "$x^{(n+1)}=x^{(n)}-\\frac{f^{\\prime}\\left(x^{(n)}\\right)}{f^{\\prime \\prime}\\left(x^{(n)}\\right)}$\n",
    "\n",
    "Например, в нашем случае следующая после $42$ точка будет рассчитываться следующим образом:\n",
    "\n",
    "$x_{2}=42-\\frac{f^{\\prime}\\left(x_{1}\\right)}{f^{\\prime \\prime}\\left(x_{2}\\right)}$\n",
    "\n",
    "$f^{\\prime}\\left(x_{1}\\right)=3 x^{2}-6 x-45 \\mid _{x_{1}=42}\\;= 3 \\cdot 42^{2}-6 \\cdot 42-45=4995$\n",
    "\n",
    "$f^{\\prime \\prime}\\left(x_{1}\\right)=6 x-6 \\mid _{x_{1}=42} \\; = \\;6.42-6=246$\n",
    "\n",
    "$x_{2}=42-\\frac{4995}{246} \\approx 42-20.305=21.695$\n",
    "\n",
    "Третья точка будет вычисляться по аналогичному принципу:\n",
    "\n",
    "$x_3 = 21.695 - \\frac{f'(21.695)}{f''(21.695)}$\n",
    "\n",
    "Но, к счастью, нам совсем не обязательно высчитывать всё вручную — воспользуемся Python и распишем наш алгоритм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.695121951219512\n",
      "11.734125501243229\n",
      "7.1123493600499685\n",
      "5.365000391507974\n",
      "5.015260627016227\n",
      "5.000029000201801\n",
      "5.000000000105126\n",
      "5.000000000000001\n"
     ]
    }
   ],
   "source": [
    "init_value = 42\n",
    "iter_count = 0\n",
    "x_curr = init_value\n",
    "epsilon = 0.0001\n",
    "f = func1(x_curr)\n",
    "\n",
    "while (abs(f) > epsilon):\n",
    "    f = func1(x_curr)\n",
    "    f_prime = func2(x_curr)\n",
    "    x_curr = x_curr - (f)/(f_prime)\n",
    "    iter_count += 1\n",
    "    print(x_curr)\n",
    "\n",
    "#21.695121951219512\n",
    "#11.734125501243229\n",
    "#7.1123493600499685\n",
    "#5.365000391507974\n",
    "#5.015260627016227\n",
    "#5.000029000201801\n",
    "#5.000000000105126\n",
    "#5.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно объединить всё в одну функцию:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def newtons_method(f, der, eps, init):\n",
    "#     iter_count = 0\n",
    "#     x_curr = init\n",
    "#     f = f(x_curr)\n",
    "#     while (abs(f) > eps):\n",
    "#         f = f(x_curr)\n",
    "#         f_der = der(x_curr)\n",
    "#         x_curr = x_curr - (f)/(f_prime)\n",
    "#         iter_count += 1\n",
    "#     return x_curr\n",
    " \n",
    "from scipy.optimize import newton\n",
    "newton(func=func1,x0=50,fprime=func2, tol=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У метода Ньютона есть ряд достоинств и недостатков.\n",
    "\n",
    "### Достоинства\n",
    "\n",
    "* Если мы минимизируем квадратичную функцию, то с помощью метода Ньютона можно попасть в минимум целевой функции за один шаг.\n",
    "\n",
    "* Также этот алгоритм сходится за один шаг, если в качестве минимизируемой функции выступает функция из класса поверхностей вращения (т. е. такая, у которой есть симметрия).\n",
    "\n",
    "* Для несимметричной функции метод не может обеспечить сходимость, однако скорость сходимости  всё равно превышает скорость методов, основанных на градиентном спуске.\n",
    "\n",
    "### Недостатки\n",
    "\n",
    "* Этот метод очень чувствителен к изначальным условиям.\n",
    "\n",
    "    При использовании градиентного спуска мы всегда гарантированно движемся по антиградиенту в сторону минимума. В методе Ньютона происходит подгонка параболоида к локальной кривизне, и затем алгоритм движется к неподвижной точке данного параболоида. Из-за этого мы можем попасть в максимум или седловую точку. Особенно ярко это видно на невыпуклых функциях с большим количеством переменных, так как у таких функций седловые точки встречаются намного чаще экстремумов.\n",
    "\n",
    "    Поэтому здесь необходимо обозначить ограничение: метод Ньютона стоит применять только для задач, в которых целевая функция выпуклая.\n",
    "    \n",
    "    Впрочем, это не является проблемой. В линейной регрессии или при решении задачи классификации с помощью метода опорных векторов или логистической регрессии мы как раз ищем минимум у выпуклой целевой функции, то есть данный алгоритм подходит нам во многих случаях.\n",
    "\n",
    "* Также метод Ньютона может быть затратным с точки зрения вычислительной сложности, так как требует вычисления не только градиента, но и гессиана и обратного гессиана (при делении на матрицу необходимо искать обратную матрицу).\n",
    "\n",
    "    Если у задачи много параметров, то расходы на память и время вычислений становятся астрономическими. Например, при наличии 50 параметров нужно вычислять более 1000 значений на каждом шаге, а затем предстоит ещё более 500 операций нахождения обратной матрицы. Однако метод всё равно используют, так как выгода от быстрой сходимости перевешивает затраты на вычисления."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на его ограниченное практическое применение, метод Ньютона по-прежнему представляет большую ценность. Он имеет большое преимущество перед градиентным спуском в силу своей быстроты и отсутствия необходимости в настройке гиперпараметра шага (мы помним, что в градиентном спуске выбор шага — довольно непростая задача, а здесь можно обойтись без этого). Причём преимущество в быстроте очень ощутимое: [в сравнении на реальных данных](https://www.researchgate.net/publication/352122891_Application_of_Newton's_method_to_solve_optimization_geodetic_tasks) метод Ньютона находит решение задачи за 3 итерации, а градиентный спуск — за 489. То есть мы сильно выигрываем в скорости сходимости, а для анализа данных это очень важно, ведь экономия времени и вычислительных ресурсов позволяет решать задачи быстрее.\n",
    "\n",
    "?\n",
    "Мы увидели, какой эффективной может быть оптимизация второго порядка при правильном использовании. Но что, если бы мы могли каким-то образом использовать эффективность, полученную при использовании производных второго порядка, но при этом избежать вычислительных затрат на вычисление обратного гессиана? Другими словами — можем ли мы создать алгоритм, который будет своего рода гибридом между градиентным спуском и методом Ньютона, где мы сможем получать более быструю сходимость, чем градиентный спуск, но меньшие вычислительные затраты на каждую итерацию, чем в методе Ньютона?\n",
    "\n",
    "Оказывается, такой алгоритм существует. Точнее, целый класс таких методов оптимизации, называемых **квазиньютоновскими методами**. Мы познакомимся с ними уже в следующем юните, но для начала давайте закрепим пройденный материал ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.6\n",
    "\n",
    "Дана функция $f(x) = x^3 - 72x - 220$. Найдите решение уравнения $f(x)=0$ для поиска корня в окрестностях точки $x_0=12$. Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.211"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**3 - 72*x - 220\n",
    "\n",
    "def dx(x):\n",
    "    return 3*x**2 - 72\n",
    "\n",
    "x_0 = 12\n",
    "\n",
    "round(x_0 - f(x_0)/dx(x_0), 3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.7\n",
    "\n",
    "Найдите положительный корень для уравнения $x^2 + 9x - 5 = 0$.\n",
    "\n",
    "В качестве стартовой точки возьмите $x_0=2.2$.\n",
    "\n",
    "Расчёт произведите поэтапно или с помощью Python.\n",
    "\n",
    "Ответ округлите до двух знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x) =  0.0\n",
      "x =  0.52\n",
      "Iterations:  4\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 + 9*x - 5\n",
    "\n",
    "def dx(x):\n",
    "    return 2*x + 9\n",
    "\n",
    "newtons_method(f, dx, x_0=2.2, n_round=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.9\n",
    "\n",
    "С помощью метода Ньютона найдите точку минимума для функции $f(x) = 8x^3 - 2x^2 - 450$.\n",
    "\n",
    "Для расчётов используйте Python.\n",
    "\n",
    "В качестве стартовой точки возьмите $42$, точность примите за $0.0001$.\n",
    "\n",
    "Ответ округлите до трёх знаков после точки-разделителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  0.167\n",
      "Iterations:  1\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return 8*x**3 - 2*x**2 - 450\n",
    "\n",
    "def dx(x):\n",
    "    return 24*x**2 - 4*x\n",
    "\n",
    "def d2x(x):\n",
    "    return 48*x - 4\n",
    "\n",
    "newtons_method(dx, d2x, eps = 0.0001, x_0=0.16, n_round=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Квазиньютоновские методы"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍ В предыдущем юните мы рассмотрели метод Ньютона. В отличие от градиентного спуска, метод Ньютона использует на каждой итерации не только градиент, но и матрицу Гессе. Это обеспечивает более быструю сходимость к минимуму, но в то же время это приводит к слишком большим вычислительным затратам. В данном юните мы рассмотрим класс методов, в которых решается  эта проблема, — класс **квазиньютоновских методов**.\n",
    "\n",
    "Напомним, что в методе Ньютона мы обновляем точку на каждой итерации в соответствии со следующим правилом:\n",
    "\n",
    "$$\\mathbf{x}_{k+1}=\\mathbf{x}_{k}-\\left[H\\left(\\mathbf{x}_{k}\\right)\\right]^{-1} \\nabla f\\left(\\mathbf{x}_{k}\\right)$$\n",
    "\n",
    "На каждом шаге здесь вычисляется гессиан, а также его обратная матрица.\n",
    "\n",
    "В квазиньютоновских методах вместо вычисления гессиана мы просто аппроксимируем его матрицей, которая обновляется от итерации к итерации с использованием информации, вычисленной на предыдущих шагах. Так как вместо вычисления большого количества новых величин мы использует найденные ранее значения, квазиньютоновский алгоритм тратит гораздо меньше времени и вычислительных ресурсов.\n",
    "\n",
    "Формально это описывается следующим образом:\n",
    "\n",
    "$$x_{k+1} = x_k - H_k \\nabla f(x_k)$$\n",
    "\n",
    "В данном случае вместо обратного гессиана появляется матрица $H_k$, которая строится таким образом, чтобы максимально точно аппроксимировать настоящий обратный гессиан."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Математически это записывается так:\n",
    "\n",
    "$$H_k - \\left [\\nabla^2 f(x_k) \\right ]^{-1} \\to 0 \\ при \\ k \\to \\infty$$\n",
    "\n",
    "Здесь имеется в виду, что разница между матрицей в квазиньютоновском методе и обратным гессианом стремится к нулю.\n",
    "\n",
    "Эта матрица обновляется на каждом шаге, и для этого существуют разные способы. Для каждого из способов есть своя модификация квазиньютоновского метода. Эти способы объединены **ограничением**: процесс обновления матрицы должен быть достаточно эффективным и не должен требовать вычислений гессиана. То есть, по сути, на каждом шаге мы должны получать информацию о гессиане, не находя непосредственно сам гессиан.\n",
    "\n",
    "Если вас интересует математическая сторона обновления и аппроксимации матрицы, прочитайте [эту статью](http://www.machinelearning.ru/wiki/images/6/65/MOMO17_Seminar6.pdf). В силу того, что понимание этой части метода требует очень серьёзной математической подготовки, мы опустим её. Однако можем заверить вас, что для успешного использования алгоритма и его понимания знание всех математических выводов не требуется.\n",
    "\n",
    "**Три самые популярные схемы аппроксимации:**\n",
    "\n",
    "* симметричная коррекция ранга 1 (SR1);\n",
    "* схема Дэвидона — Флетчера — Пауэлла (DFP);\n",
    "* схема Бройдена — Флетчера — Гольдфарба — Шанно (BFGS)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последняя схема (**BFGS**) самая известная, стабильная и считается наиболее эффективной. На ней мы и остановимся. Своё название она получила из первых букв фамилий создателей и исследователей данной схемы: Чарли Джорджа Бройдена, Роджера Флетчера, Дональда Гольдфарба и Дэвида Шанно.\n",
    "\n",
    "У этой схемы есть **две известных вариации**:\n",
    "\n",
    "* L-BFGS;\n",
    "* L-BFGS-B.\n",
    "\n",
    "Обе этих вариации необходимы в случае большого количества переменных для экономии памяти (так как во время их реализации хранится ограниченное количество информации). По сути, они работают одинаково, и L-BFGS-B является лишь улучшенной версией L-BFGS для работы с ограничениями.\n",
    "\n",
    "Метод BFGS очень устойчив и на данный момент считается одним из наиболее эффективных. Поэтому, если, например, применить функцию optimize без указания метода в библиотеке SciPy, то по умолчанию будет использоваться именно BFGS либо одна из его модификаций, указанных выше. Также данный метод используется в библиотеке sklearn при решении задачи логистической регрессии.\n",
    "\n",
    "Рассмотрим **алгоритм применения этого метода**. Постарайтесь понять последовательность действий и основной принцип."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Реализация алгоритма начинается с того, что мы задаём начальную точку , выбираем точность алгоритма, а также изначальную аппроксимацию для обратного гессиана функции. \n",
    "\n",
    "    Здесь как раз таится главная проблема: нет никакого универсального рецепта для выбора этого приближения. Можно поступить по-разному: использовать гессиан, вычисленный в изначальной точке, взять единичную матрицу (такая настройка стоит по умолчанию в некоторых функциях библиотек Python) или другую матрицу, если она невырождена и хорошо обусловлена.\n",
    "\n",
    "2. Когда мы определили, откуда будем начинать, необходимо понять, как попасть в следующую точку. Поэтому на втором шаге мы вычисляем направление для поиска следующей точки:\n",
    "\n",
    "    $$p_k = -H_k * \\nabla f_k$$\n",
    "\n",
    "3. Далее находим следующую точку, используя соотношение:\n",
    "\n",
    "    $$x_{k+1} = x_k + k * p_k$$\n",
    "\n",
    "    Здесь важным вопросом является нахождение коэффициента $k$, который регулирует шаг. Его подбирают линейным поиском в соответствии с условиями, о которых можно подробно прочитать [здесь](https://en.wikipedia.org/wiki/Wolfe_conditions).\n",
    "\n",
    "    Нам важно лишь понимать суть: мы находим такое значение $k$, при котором получим минимальное значение функции $f(x_k + k * p_k)$ (так как мы хотим попасть в минимум). Также важно отметить, что в расчёте коэффициента $k$ участвуют две константы $0 \\leq c_1 \\leq c_2 \\leq 1$. Обычно в качестве их значений берут $0.001$ и $0.9$ (это считается хорошей эвристикой, показывающей высокие результаты)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. На следующем шаге необходимо определить два следующих вектора:\n",
    "\n",
    "    $$s_{k}=x_{k+1}-x_{k}$$\n",
    "\n",
    "    $$y_{k}=\\nabla f_{k+1}-\\nabla f_{k}$$\n",
    "\n",
    "    Здесь $s_k$ — шаг алгоритма, a $y_k$ — изменение градиента на данной итерации. Они не требовались для перехода в следующую точку и нужны строго для того, чтобы найти следующее приближение обратной матрицы гессиана.\n",
    "\n",
    "5. После того как мы их нашли, обновляем гессиан, руководствуясь следующей формулой:\n",
    "\n",
    "    $$H_{k+1}=\\left(I-k * s_{k} * y_{k}^{T}\\right) H_{k}\\left(I-k * y_{k} * s_{k}^{T}\\right)+* s_{k} * s_{k}^{T}$$\n",
    "\n",
    "    Не стоит её пугаться. Как уже было сказано выше, эта формула —  результат очень серьёзных и длительных математических исследований. Её не требуется знать наизусть или уметь реализовывать вручную. Однако для полноты повествования мы не можем не привести её.\n",
    "\n",
    "    В данной формуле за $I$ обозначена единичная матрица, $s_k$ и $y_k$ мы вычислили на предыдущем шаге, а $k$ вычисляется следующим образом:\n",
    "\n",
    "    $$k=\\frac{1}{y_{k}^{T} s_{k}}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм довольно сложный, поэтому давайте рассмотрим **пример** ↓\n",
    "\n",
    "Сразу оговоримся, что несколько шагов в этом алгоритме мы приведём без ручных расчётов (например, нахождение следующей аппроксимации гессиана) в силу их высокой сложности. Постарайтесь сильнее всего сконцентрироваться на шагах решения и понять логику работы алгоритма и последовательность действий. Мы начнём реализовывать алгоритм «вручную», а затем вы сможете самостоятельно завершить решение задачи с использованием Python (разумеется, далее будет указано, как это сделать). К сожалению, серьёзные и эффективные методы настолько сложны, что решать с их помощью задачи, используя только лист бумаги, ручку и калькулятор (как мы могли это делать, например, с методом Лагранжа), уже невозможно.\n",
    "\n",
    "Будем искать экстремум для функции следующего вида:\n",
    "\n",
    "$f(x, y)=x^{2}-x y+y^{2}+9 x-6 y+20$\n",
    "\n",
    "В качестве начальной точки выберем следующую:\n",
    "\n",
    "$x_0 = (1,1)$\n",
    "\n",
    "Находим градиент для нашей функции:\n",
    "\n",
    "$\\begin{aligned} f_{x}^{\\prime} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime}_{ x}=\\\\ &=2 x-y+0+9-0+0=2 x-y+9 \\\\ f^{\\prime}_{ y} &=\\left(x^{2}-x y+y^{2}+9 x-6 y+20\\right)^{\\prime} _{y}=\\\\ &=0-x+2 y+0-6+0=-x+2 y-6 \\end{aligned}$\n",
    "\n",
    "$\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right)$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начинаем первую итерацию с точки $x_0 = (1,1)$. Вычисляем для неё градиент:\n",
    "\n",
    "$\\nabla f=\\left(\\begin{array}{c}2 x-y+9 \\\\ -x+2 y-6\\end{array}\\right) \\quad=\\left(\\begin{array}{c}2 \\cdot 1-1+9 \\\\ -1 \\times 2 \\cdot 1-6\\end{array}\\right)=\\left(\\begin{array}{c}10 \\\\ -5\\end{array}\\right)$\n",
    "\n",
    "Теперь необходимо выяснить, стоит ли заканчивать поиск (ведь, возможно, мы уже в минимуме). Для этого находим длину вектора градиента:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = \\sqrt{10^2 + (-5)^2} = 11.18$\n",
    "\n",
    "Сравниваем полученный результат с точностью, которая нам необходима. Допустим, мы хотим достигнуть точности $0.001$:\n",
    "\n",
    "$\\left|\\nabla f(x_0) \\right| = 11.18 > 0.001$\n",
    "\n",
    "Итак, точность не достигнута, так как градиент в экстремуме должен быть равен или очень близок к нулю. Это значит, что надо искать дальше.\n",
    "\n",
    "Теперь необходимо определить, в каком направлении искать нужную точку. Для этого выполняем следующие вычисления, согласно нашему алгоритму:\n",
    "\n",
    "$p_0 = -H_0 * \\nabla f(x_0) = - \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -5 \\end{pmatrix} = \\begin{pmatrix} -10 \\\\ 5 \\end{pmatrix}$\n",
    "\n",
    "$x_0 + \\alpha_0 * p_0 = (1,1) + \\alpha_0 (10,-5) = (1 - 10 \\alpha_0, 1 + 5 \\alpha_0)$\n",
    "\n",
    "$f(\\alpha_0) = (1 - 10 \\alpha_0)^2 - (1 - 10 \\alpha_0) (1 + 5 \\alpha_0) + (1 + 5 \\alpha_0)^2 + 9 (1 - 10 \\alpha_0) - 6 (1 + 5 \\alpha_0) + 20$\n",
    "\n",
    "Упрощаем выражение:\n",
    "\n",
    "$1-20 a_{0}+100 a_{0}^{2}-1+10 a_{0}-5 a_{0}+50 a_{0}^{2}+1+10 a_{0}+25 a_{0}^{2}+9-90 a_{0}-6-30 a_{0}+20= 175 a_{0}^{2}-125 a_{0}+24$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим производную от результата:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial a_0}=350_{0}-125=0 \\Rightarrow a_{0}=0.357$\n",
    "\n",
    "Теперь мы можем найти следующую точку:\n",
    "\n",
    "$x_{1}=x_{0}+{a }_{0} * p_{0}=(-2.571,2.786)$\n",
    "\n",
    "$s_{0}=x_{1}-x_{0}=(-2.571,2.786)-(1,1)=(-3.571,1.786)$\n",
    "\n",
    "Вычисляем значение градиента в следующей найденной точке, т. е. в $x_1$:\n",
    "\n",
    "$\\nabla f\\left(x_{1}\\right)=\\left(\\begin{array}{l}1.071 \\\\ 2.143\\end{array}\\right)$\n",
    "\n",
    "$y_{0}=\\nabla f\\left(x_{1}\\right)-\\nabla f\\left(x_{0}\\right)=(1.071,2.143)-(10,-5)=(-8.929,7.143)$\n",
    "\n",
    "Находим приближение гессиана:\n",
    "\n",
    "$H_{1}=\\left(\\begin{array}{ll}0.694 & 0.367 \\\\ 0.367 & 0.709\\end{array}\\right)$\n",
    "\n",
    "Проверяем, стоит ли остановиться в этой точке:\n",
    "\n",
    "$\\left|\\nabla f(x_1) \\right| = 2.396 > 0.001$\n",
    "\n",
    "К сожалению, необходимая точность всё ещё не достигнута.\n",
    "\n",
    "Перечисленные выше шаги стоит повторять до тех пор, пока не будет достигнуто значение, близкое к нулю (меньшее, чем изначально заданная точность)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разумеется, при решении прикладных задач не понадобится делать ничего подобного, ведь мы умеем программировать и знаем, что в библиотеках Python есть все необходимые методы.\n",
    "\n",
    "Давайте рассмотрим, как с помощью функций Python мы сможем применить квазиньютоновские методы для оптимизации функции $f(x,y) = x^2 + y^2$.\n",
    "\n",
    "Подгрузим необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим функцию, которую будем оптимизировать. Вместо отдельных  и  можно взять координаты единого вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return x[0]**2.0 + x[1]**2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь определим градиент для функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_func(x):\n",
    "    return np.array([x[0] * 2, x[1] * 2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим начальную точку:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0 = [1.0, 1.0]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим алгоритм:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем результаты:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации: Optimization terminated successfully.\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    " \n",
    "#Статус оптимизации Optimization terminated successfully.\n",
    "#Количество оценок: 3\n",
    "#Решение: f([0. 0.]) = 0.00000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы получили, что минимум функции достигается в точке $(0, 0)$. Значение функции в этой точке также равно нулю.\n",
    "\n",
    "Можно повторить то же самое с вариацией  L-BFGS-B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "Количество оценок: 3\n",
      "Решение: f([0. 0.]) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "# # определяем нашу функцию\n",
    "# def func(x):\n",
    "#     return x[0]**2.0 + x[1]**2.0\n",
    " \n",
    "# #  определяем градиент функции\n",
    "# def grad_func(x):\n",
    "#     return np.array([x[0] * 2, x[1] * 2])\n",
    " \n",
    "# # определяем начальную точку\n",
    "# x_0 = [1, 1]\n",
    "# # реализуем алгоритм L-BFGS-B\n",
    "\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат будет тем же самым.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Иногда количество итераций у двух модификаций различается, но ответ совпадает. Бывает также, что одна из вариаций может не сойтись, а другая — достичь экстремума, поэтому советуем не воспринимать их как взаимозаменяемые алгоритмы. На практике лучше пробовать разные варианты: если у вас не сошёлся алгоритм BFGS, можно попробовать L-BFGS-B, и наоборот. Также можно экспериментировать одновременно с обоими алгоритмами, чтобы выбрать тот, который будет сходиться для функции за меньшее число итераций и тем самым экономить время.\n",
    "\n",
    "→ Важно понимать, что для некоторых функций не из всех стартовых точек получается достичь сходимости метода. Тогда их можно перебирать, к примеру, с помощью цикла.\n",
    "\n",
    "✍ Итак, мы обсудили один из самых эффективных на сегодняшний день алгоритмов — вариацию BFGS квазиньютоновских методов. Вы будете регулярно сталкиваться с этим алгоритмом при решении различных задач и при использовании библиотек для оптимизации. Так что давайте попрактикуемся: в этом юните мы посмотрели фрагмент поэтапного разбора метода BFGS для функции $f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20$ — давайте завершим начатое и найдём точку минимума ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.1\n",
    "\n",
    "Найдите точку минимума для функции $f(x,y) = x^2 - xy + y^2 + 9x - 6y + 20$.\n",
    "\n",
    "В качестве стартовой возьмите точку $(-400, -400)$.\n",
    "\n",
    "Значения координат округлите до целого числа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "Количество оценок: 8\n",
      "Решение: f([-4.  1.]) = -1.00000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x[0]**2 - x[0]*x[1] + x[1]**2 + 9*x[0] - 6*x[1] + 20\n",
    "\n",
    "def grad_func(x):\n",
    "    return np.array([2*x[0] - x[1] + 9, -x[0] + 2*x[1] - 6])\n",
    "\n",
    "x_0 = [-400, 400]\n",
    "\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.4\n",
    "\n",
    "Найдите минимум функции $f(x) = x^2 - 3x + 45$ с помощью квазиньютоновского метода BFGS.\n",
    "\n",
    "В качестве стартовой точки возьмите $x=10$.\n",
    "\n",
    "В качестве ответа введите минимальное значение функции в достигнутой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации: Optimization terminated successfully.\n",
      "Количество оценок: 5\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x**2 - 3*x + 45\n",
    "\n",
    "def grad_func(x):\n",
    "    return 2*x - 3\n",
    "\n",
    "x_0 = 10\n",
    "\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.5\n",
    "\n",
    "Решите предыдущую задачу, применяя модификацию L-BFGS-B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статус оптимизации: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\n",
      "Количество оценок: 3\n",
      "Решение: f([1.5]) = 42.75000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x**2 - 3*x + 45\n",
    "\n",
    "def grad_func(x):\n",
    "    return 2*x - 3\n",
    "\n",
    "x_0 = 10\n",
    "\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 4.7\n",
    "\n",
    "Найдите минимум функции $f(x,y) = x^4 + 6*y^2 + 10$, взяв за стартовую точку .\n",
    "\n",
    "Какой алгоритм сошелся быстрее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BFGS\n",
      "Статус оптимизации: Optimization terminated successfully.\n",
      "Количество оценок: 37\n",
      "Решение: f([1.31617159e-02 6.65344582e-14]) = 10.00000\n",
      "\n",
      "L-BFGS-B\n",
      "Статус оптимизации: b'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH'\n",
      "Количество оценок: 40\n",
      "Решение: f([-9.52718297e-03 -2.32170510e-06]) = 10.00000\n"
     ]
    }
   ],
   "source": [
    "def func(x):\n",
    "    return x[0]**4 + 6*x[1]**2 + 10\n",
    "\n",
    "def grad_func(x):\n",
    "    return np.array([4*x[0]**3, 12*x[1]])\n",
    "\n",
    "x_0 = [100, 100]\n",
    "\n",
    "print('BFGS')\n",
    "result = minimize(func, x_0, method='BFGS', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))\n",
    "\n",
    "print()\n",
    "print('L-BFGS-B')\n",
    "result = minimize(func, x_0, method='L-BFGS-B', jac=grad_func)\n",
    "# получаем результат\n",
    "print('Статус оптимизации: %s' % result['message'])\n",
    "print('Количество оценок: %d' % result['nfev'])\n",
    "solution = result['x']\n",
    "evaluation = func(solution)\n",
    "print('Решение: f(%s) = %.5f' % (solution, evaluation))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Линейное программирование\n",
    "\n",
    "✍ В предыдущем модуле мы уже немного говорили о задачах условной оптимизации. Теперь пришло время вернуться к ним и рассмотреть их более основательно. Мы разберём один из наиболее часто встречающихся случаев задачи условной оптимизации — **задачу линейного программирования**.\n",
    "\n",
    "> **Линейное программирование** — это метод оптимизации для системы линейных ограничений и линейной целевой функции. Целевая функция определяет оптимизируемую величину, и цель линейного программирования состоит в том, чтобы найти значения переменных, которые максимизируют или минимизируют целевую функцию.\n",
    "\n",
    "Линейное программирование полезно применять для многих задач, требующих оптимизации ресурсов:\n",
    "\n",
    "* В производстве — чтобы рассчитать человеческие и технические ресурсы и минимизировать стоимость итоговой продукции.\n",
    "* При составлении бизнес-планов — чтобы решить, какие продукты продавать и в каком количестве, чтобы максимизировать прибыль.\n",
    "* В логистике — чтобы определить, как использовать транспортные ресурсы для выполнения заказов за минимальное время.\n",
    "* В сфере общепита — чтобы составить расписание для официантов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача линейного программирования — это задача оптимизации, в которой целевая функция и функции-ограничения линейны, а все переменные неотрицательны.\n",
    "\n",
    "Есть и разновидность задачи линейного программирования, в которой мы знаем, что все переменные могут принимать только целочисленные значения. Например, если переменные в нашей задаче — это количество людей или произведённых на заводе изделий, то они, разумеется, не могут быть дробными.\n",
    "\n",
    "> **Целочисленным линейным программированием (ЦЛП)** называется вариация задачи линейного программирования, когда все переменные — целые числа.\n",
    "\n",
    "Давайте рассмотрим алгоритм решения задачи линейного программирования на конкретном примере ↓\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 1***\n",
    "\n",
    "*Фабрика игрушек производит игрушки-антистресс и игрушки-вертушки.*\n",
    "\n",
    "*Для изготовления игрушки-антистресс необходимо потратить 2 доллара  и 3 часа, для изготовления игрушки-вертушки — 4 доллара и 2 часа.*\n",
    "\n",
    "*На этой неделе в бюджете у фабрики есть 220 долларов, и оплачено 150 трудочасов для производства указанных игрушек.*\n",
    "\n",
    "*Если одну игрушку-антистресс можно продать за 6 долларов, а игрушку-вертушку — за 7, то сколько экземпляров каждого товара необходимо произвести на этой неделе, чтобы максимизировать прибыль?*\n",
    "\n",
    "Такая задача идеально подходит для использования методов линейного программирования по следующим причинам:\n",
    "\n",
    "* все условия являются линейными;\n",
    "* значения переменных каким-то образом ограничены;\n",
    "* цель состоит в том, чтобы найти значения переменных, которые максимизируют некоторую величину.\n",
    "\n",
    "Обратите внимание, что производство каждой детали связано с затратами, как временными, так и финансовыми. Изготовление каждой игрушки-антистресс стоит 2 доллара, а изготовление каждой вертушки — 4 доллара. У фабрики есть только 220 долларов, чтобы потратить их на производство изделий. Отсюда возникает **ограничение на возможное количество изготовленных товаров**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обозначим за  количество произведённых игрушек-антистресс, за  — количество произведённых игрушек-вертушек Тогда это ограничение можно записать в виде следующего неравенства:\n",
    "\n",
    "$2 x+4 y \\leq 220$\n",
    "\n",
    "Также существует ограничение на то, сколько времени мы можем потратить на производство игрушек. На изготовление каждой игрушки-антистресс уходит 3 часа, а на изготовление каждой игрушки-вертушки — 2 часа. На этой неделе у фабрики есть только 150 рабочих часов, так что **производство ограничено по времени**. Это ограничение можно записать в виде неравенства:\n",
    "\n",
    "$3 x+2 y \\leq 150$\n",
    "\n",
    "К этим ограничениям мы также можем добавить ограничения из соображений здравого смысла. Невозможно произвести отрицательное количество игрушек, поэтому необходимо обозначить следующие ограничения:\n",
    "\n",
    "$\\begin{aligned} &x \\geq 0 \\\\ &y \\geq 0 \\end{aligned}$\n",
    "\n",
    "Итак, мы записали все необходимые ограничения. Они образуют систему неравенств:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 2 x+4 y & \\leq 220 \\\\ 3 x+2 y & \\leq 150 \\\\ x & \\geq 0 \\\\ y & \\geq 0  \\end{aligned}\\right.$\n",
    "\n",
    "Если [представить систему этих неравенств в графическом виде](http://mathprofi.ru/lineinye_neravenstva.html), получим многоугольник:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7fb295aca0458dc97f2ec9da18acf051/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_5_1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закрашенная область является областью допустимых решений этой задачи. **Наша цель** — найти внутри этого многоугольника такую точку, которая даст наилучшее решение нашей задачи (максимизацию или минимизацию целевой функции).\n",
    "\n",
    "Итак, мы поняли, что нам нужно найти максимальное значение прибыли (целевой функции) для точек внутри области допустимых значений, однако самой целевой функции у нас пока нет. Давайте составим её. На изготовление каждой игрушки-антистресс требуется 2 доллара, а продать её можно за 6. Получается, что чистая прибыль от продажи составляет 4 доллара. Чтобы изготовить игрушку-вертушку, мы потратим 4 доллара, а продадим её за 7. Значит, чистая прибыль для вертушки составляет 3 доллара. Исходя из этого, получаем целевую функцию для суммарной прибыли:\n",
    "\n",
    "$p(x, y)=4 x+3 y$\n",
    "\n",
    "Нам необходимо найти максимально возможное значение этой функции с учётом того, что точка, в которой оно будет достигаться, должна удовлетворять условиям системы, которую мы написали ранее.\n",
    "\n",
    "Для того чтобы решить задачу, выразим одну переменную через другую (так удобнее строить графики линейной функции в стандартной системе координат):\n",
    "\n",
    "$y=-\\frac{4}{3} x+\\frac{P}{3}$\n",
    "\n",
    "На графике ниже наша линия изображена красным цветом. Она может двигаться вверх и вниз в зависимости от значения $P$ (вы можете видеть три прямых — это три разных положения одной и той же прямой)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем найти такую точку, для которой значение $y$ будет наибольшим. Нас это интересует, так как мы хотим максимизировать значение $P$, а при максимально возможном $P$ прямая поднимется настолько высоко, насколько это возможно, и пересечение с точкой ординат тоже будет находиться максимально высоко.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/2ee65269b32c2501b4ddc4635080e546/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_5_2.png)\n",
    "\n",
    "Линия, которая максимизирует точку пересечения c осью ординат, проходит через точку $(20, 45)$ — это точка пересечения первых двух ограничений. Все остальные прямые, которые проходят выше, не проходят через область допустимых решений. Все остальные «нижние» прямые проходят более чем через одну точку в допустимой области и не максимизируют пересечение прямой с осью ординат, так как находятся ниже.\n",
    "\n",
    "Таким образом, получаем, что завод должен произвести 20 игрушек-антистресс и 45 игрушек-вертушек. Это даст прибыль в размере 215 долларов:\n",
    "\n",
    "$20*4 + 45* 3 = 215$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 2***\n",
    "\n",
    "*Фермер кормит своих коров специальной смесью. Для её изготовления он использует два вида корма: на 1 кг кукурузного корма приходится 100 г белка и 750 г крахмала, на 1 кг пшеничного — 150 г белка и 700 г крахмала.*\n",
    "\n",
    "*Каждой корове необходимо давать не более 7 кг корма в сутки, иначе у неё заболит живот и придётся тратить деньги на ветеринара. При этом, чтобы давать оптимально полезное и вкусное молоко, каждая корова должна ежедневно потреблять как минимум 650 г белка и 4000 г крахмала.*\n",
    "\n",
    "*Известно, что кукурузный корм стоит 0.4 доллара за 1 кг, а пшеничный — 0.45 долларов за 1 кг.*\n",
    "\n",
    "*Какая кормовая смесь будет минимизировать затраты и в то же время позволит получать качественное молоко?*\n",
    "\n",
    "Обозначим за $c$ количество кукурузного корма в смеси, а за $w$ — количество пшеничного. Тогда ограничения можно выразить следующим образом:\n",
    "\n",
    "$\\left\\{\\begin{aligned} 0.1 c+0.15 w & \\geq 0.65 \\\\ 0.75 c+0.7 w & \\geq 4 \\\\ c+w & \\leq 7 \\\\ c & \\geq 0 \\\\ w & \\geq 0 \\end{aligned}\\right.$\n",
    "\n",
    "Минимизировать мы будем целевую функцию, отражающую затраты на корм и имеющую следующий вид:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(c, w)=0.40 c+0.45 w$\n",
    "\n",
    "Снова изобразим условие задачи графически. Начнём с многоугольника области допустимых значений:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/a850978274d626911ea9204ab1e09b88/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_5_3.png)\n",
    "\n",
    "Получается, что нам необходимо найти точку, в которой пересекаются две прямые, обозначающие условия. Можно записать это следующим образом (точка пересечения — решение системы):\n",
    "\n",
    "$\\left\\{\\begin{array}{l} 0.1 c+0.15 w=0.65 \\\\ 0.75 c+0.7 w=4 \\end{array}\\right.$\n",
    "\n",
    "Найдя эту точку (для этого можно воспользоваться методом Гаусса или программированием), можно найти и итоговую стоимость смеси:\n",
    "\n",
    "$f(3.411,2.059)=\\$ 2.29$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.5\n",
    "\n",
    "Магазин спортивных товаров получает прибыль в размере 6 долларов с каждого проданного футбольного мяча и 5.5 долларов — с бейсбольного.\n",
    "\n",
    "Каждый месяц магазин продаёт от 35 до 45 футбольных мячей и от 40 до 55 бейсбольных.\n",
    "\n",
    "Известно, что в этом месяце у магазина есть в общей сложности 80 мячей.\n",
    "\n",
    "Какую максимальную прибыль в этом месяце может получить магазин от продажи мячей?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linprog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40., 40.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1,1], \n",
    "     [-1, 0],\n",
    "     [1, 0],\n",
    "     [0, -1],\n",
    "     [0, 1]\n",
    "     ]\n",
    "b = [80, -35, 45, -40, 55]\n",
    "\n",
    "f = [-6, -5.5]\n",
    "res = linprog(f, A, b)\n",
    "res.x.round()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 5.6\n",
    "\n",
    "На текстильной фабрике есть 750 метров хлопка и 1000 метров полиэстера.\n",
    "\n",
    "Для изготовления свитшота требуется 1 метр хлопка и 2 метра полиэстера, для изготовления рубашки — 1.5 метра хлопка и 1 метр полиэстера.\n",
    "\n",
    "Свитшот можно продать за 30 евро, а рубашку — за 24 евро.\n",
    "\n",
    "Какое суммарное количество свитшотов и рубашек максимизирует возможную прибыль?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([375., 250.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1,1.5], [2,1]]\n",
    "b = [750, 1000]\n",
    "f = [-30, -24]\n",
    "res = linprog(f, A, b)\n",
    "res.x.round()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем юните мы научимся решать задачи линейного программирования с использованием библиотек Python. Однако для того чтобы грамотно передавать условия в функции, необходимо научиться формулировать задачи линейного программирования в матричном виде. Давайте **рассмотрим несколько примеров в качестве подготовки** ↓\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 1***\n",
    "\n",
    "*Вы отвечаете за рекламу в компании.*\n",
    "\n",
    "*Затраты на рекламу в месяц не должны превышать 10 000 руб. Один показ рекламы в интернете стоит 1 рубль, а на телевидении — 90 рублей. При этом на телевидении нельзя купить больше 20 показов.*\n",
    "\n",
    "*Практика показывает, что 1 показ телерекламы приводит в среднем 300 клиентов, а 1 показ в интернете — 0.5 клиента.*\n",
    "\n",
    "*Вам необходимо привести как можно больше клиентов.*\n",
    "\n",
    "Обозначим за $x_1$ количество показов в интернете, за $x_2$ — количество показов на телевидении. Будем максимизировать число приведённых клиентов $0.5 x_1 + 300 x_2$.\n",
    "\n",
    "Составим задачу минимизации с ограничением на количество показов и затраты. Наша целевая функция примет следующий вид:\n",
    "\n",
    "Мы заменили максимизацию на минимизацию, так как готовим задачу для решения стандартным алгоритмом, а по умолчанию задача оптимизации сводится к минимизации. Поэтому перед применением функций Python формулируйте задачу именно в формате поиска минимума.\n",
    "\n",
    "Ограничения можно выразить так:\n",
    "\n",
    "$\\left\\{\\begin{array}{l}x_{2} \\leq 20 \\\\ x_{1}+90 x_{2} \\leq 10000\\end{array}\\right.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разумеется, будем помнить, что количество показов рекламы всегда неотрицательное.\n",
    "\n",
    "Представим наши данные в векторном виде. Пусть $c$ — это вектор приведённых клиентов:\n",
    "\n",
    "$c=\\left(\\begin{array}{c}-0.5 \\\\ -300\\end{array}\\right)$\n",
    "\n",
    "За $A$ и $b$ мы возьмём такие матрицы, чтобы с их помощью можно было представить систему ограничений:\n",
    "\n",
    "$A=\\left(\\begin{array}{cc}0 & 1 \\\\ 1 & 90\\end{array}\\right), \\ b=\\left(\\begin{array}{c}20 \\\\ 10000\\end{array}\\right)$\n",
    "\n",
    "Тогда наша задача формулируется следующим образом:\n",
    "\n",
    "$\\operatorname{min} c^{T} x$\n",
    "\n",
    "$A x \\leq b$\n",
    "\n",
    "Итак, у нас получилось перевести задачу на язык линейной алгебры. Решать такие задачи мы будем в следующем юните, а пока рассмотрим ещё один **пример посложнее** ↓"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Пример № 2***\n",
    "\n",
    "*Есть $n$ задач и $n$ человек, которые могут их выполнить.*\n",
    "\n",
    "*Каждая задача должна быть сделана одним человеком, и каждый должен сделать ровно одну задачу.*\n",
    "\n",
    "*Выполнение задачи $j$ человеком $i$ будет стоить $c_{ij}$.*\n",
    "\n",
    "*Вам необходимо сделать все задачи как можно дешевле.*\n",
    "\n",
    "Так как очевидно, что люди и задачи могут быть выражены только целыми числами, будем формулировать задачу как задачу целочисленного программирования.\n",
    "\n",
    "Пусть $x_{ij}=1$, если задачу $j$ выполнил человек $i$. Если работы выполнил кто-то другой, то $x_{ij}=0$.\n",
    "\n",
    "Распределение работ по людям мы можем представить в виде таблицы-матрицы, например, следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/9aee5b57d7a0c52c7abdaa00ba128d24/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/MATHML_md6_5_7_new.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь на пересечениях столбцов-работ и строк-людей указана стоимость работы, если её выполнит выбранный человек. Например, второй человек выполнит первую работу за 1 денежную единицу, вторую — за 5 денежных единиц, а третью — за 2 денежных единицы.\n",
    "\n",
    "Если мы выберем первого человека для выполнения первой работы, второго — для третьей и третьего — для второй (как и закрашено на рисунке выше), тогда в матрице распределения работ мы обозначим соответствующие элементы за 1, а остальные — за 0.\n",
    "\n",
    "Теперь сформулируем задачу. Минимизируем суммарную стоимость $c_{ij} \\cdot x_{ij}$:\n",
    "\n",
    "$\\min \\sum_{i, j} c_{i j} x_{i j}$\n",
    "\n",
    "Теперь запишем условия:\n",
    "\n",
    "1. $x$ равен либо $0$, либо $1$: $x_{i j} \\leq 1$.\n",
    "1. Каждый человек должен взять ровно одну задачу: $\\forall i: \\sum_{j} x_{i j}=1$.\n",
    "1. Каждую задачу должен взять ровно один человек: $\\forall j: \\sum_{i} x_{i j}=1$.\n",
    "\n",
    "Мы сформулировали задачи линейного программирования в чётком математическом виде. Самое время переходить к следующему юниту и учиться решать задачи с использованием программирования →"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ДОПОЛНИТЕЛЬНО\n",
    "\n",
    "Если вам интересно изучить более сложные, но очень известные задачи оптимизации, к которым применимо линейное программирование, рекомендуем прочитать следующие статьи:\n",
    "\n",
    "* [Задача коммивояжёра](https://habr.com/ru/post/560468/)\n",
    "* [Транспортная задача](https://habr.com/ru/post/573224/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950b5653ccfc34417735dd321d006fd482b31f7611416c3d8236dc5b17587d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
