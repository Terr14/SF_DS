{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ML-8. Продвинутые методы машинного обучения**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ В прошлом модуле мы научились важному этапу в разработке модели — поиску оптимальных гиперпараметров. Этот этап важен для получения лучшего качества модели. \n",
    "\n",
    "В этом модуле мы поговорим о способах ансамблирования и сбора в пайплайн как о дальнейшем шаге развёртывания модели. Дальнейшим этапом в освоении профессии дата-сайентиста станет погружение в тематику задач и основных методов решения задач **metric learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## АНСАМБЛИРОВАНИЕ МОДЕЛЕЙ\n",
    "\n",
    "При изучении Data Science идея ансамблирования впервые встречается при упоминании такой модели, как **случайный лес**. В данной модели обучаются базовые модели, представленные решающими деревьями,  предсказания которых впоследствии агрегируются некоторым образом, зависящим от задачи. В случае задачи регрессии берётся среднее либо средневзвешенное. В случае задачи классификации класс присваивается по принципу большинства. \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/67a412a1866884f5928f98e5781cfec3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dspr-ml-8-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот знакомый нам подход построения ансамбля называется **бэггингом** и позволяет улучшить качество предсказания. Однако существуют и другие техники ансамблирования, которым и посвятим часть модуля.\n",
    "\n",
    "## PIPELINE\n",
    "\n",
    "Как известно, любая задача классического машинного обучения сводится к этапам обработки данных с последующей генерацией признаков. Позднее признаки отбираются и подаются на модель, которую в свою очередь обучают и настраивают, находя оптимальные гиперпараметры. Качество модели проверяют с помощью методов валидации (например, **кросс-валидации**). \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/69e171db9504de109deb18b06242a056/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Процесс автоматического поэтапного выполнения манипуляций с данными, включающий в себя сбор, обработку, генерацию и отбор признаков, обучение модели с последующей её настройкой и проверкой качества называется **пайплайном**. \n",
    "\n",
    "Также  под пайплайном иногда подразумевают автоматизацию одного или нескольких этапов, названных ранее, например обработку данных. \n",
    "\n",
    "В этом модуле мы рассмотрим, для чего стоит использовать пайплайны, а также познакомимся с базовыми подходами по созданию пайплайнов на базе библиотеки scikit-learn.\n",
    "\n",
    "## METRIC LEARNING\n",
    "\n",
    "Любые подходы в машинном обучении, которые требуют измерения расстояния между объектами в выборке, являются подходами **metric learning**. Основными задачами, решаемыми подходами metric learning, наряду с классическими задачами обучения с учителем, являются задача **кластеризации** и задача **понижения размерности**. Также metric learning иногда используется в задачах восстановления данных по принципу нахождения ближайшего похоже объекта.\n",
    "\n",
    "В этом модуле мы научимся применять на практике основные методы и подходы metric learning, в частности **kNN, k-means**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ЦЕЛИ МОДУЛЯ\n",
    "\n",
    "Целями модуля являются:\n",
    "\n",
    "✔️ Научиться самостоятельно ансамблировать различными методами модели для повышения качества предсказаний.\n",
    "\n",
    "✔️ Освоить практические методы создания пайплайнов и создать свой.\n",
    "\n",
    "✔️ Применить подходы metric learning на практике."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Ансамблирование: бэггинг, случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Основным подходом для улучшения качества слабых и нестабильных моделей является обучение моделей для решения одной и той же поставленной задачи с последующим объединением и получением некоторого более сильного и стабильного консенсус-решения.  Как мы знаем, модели, используемые для ансамблирования, называются **базовыми**.\n",
    "\n",
    "[→ Скачайте ноутбук с кодом](https://lms.skillfactory.ru/assets/courseware/v1/7699e6679ca8749f3e5bc78cc40fb40e/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/ML-8._Ensembles.ipynb)\n",
    "\n",
    "Существует три основных способа построения ансамблей:\n",
    "\n",
    "**Бэггинг** — параллельно обучаем множество одинаковых моделей, а для предсказания берём среднее по предсказаниям каждой из моделей.\n",
    "\n",
    "**Бустинг** — последовательно обучаем множество одинаковых моделей, где каждая новая модель концентрируется на тех примерах, где предыдущая допустила ошибку.\n",
    "\n",
    "**Стекинг** — параллельно обучаем множество разных моделей, отправляем их результаты в финальную модель, и уже она принимает решение. \n",
    "\n",
    "> **Бэггинг (bagging)** — алгоритм построения ансамбля путём параллельного обучения множества независимых друг от друга моделей.\n",
    "\n",
    "Самым распространённым примером ансамбля типа бэггинг является уже знакомый нам **случайный лес** (Random Forest). \n",
    "\n",
    "Вам уже известно, что случайный лес является ансамблем решающих деревьев, в котором выборка выбирается посредством бутстрапа. Далее каждое решающее дерево обучается на случайной подвыборке из признакового пространства. Иными словами, случайный лес содержит в себе две случайности: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения отдельные решающие деревья объединяются в ансамбль. Ранее мы узнали, что в случайном лесу для задачи **классификации** ансамблирование происходит посредством большинства голосов (Majority Vote). Для задачи **регрессии** же ансамблирование происходит посредством усреднения результата предсказания каждой базовой модели (Averaging).\n",
    "\n",
    "Давайте рассмотрим это на примере. \n",
    "\n",
    "Для задачи регрессии ансамблирование решения 10 случайных деревьев есть не что иное, как среднее значение предсказаний данных моделей. Пускай вектор ответов моделей равен out = [0.5, 0.6, 0.2, 0.4, 0.9, 1.0, -0.5, 0.3, 0.2, 0.0]. \n",
    "\n",
    "Посчитаем среднее значение: \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/108d9322eaadc994332c3f9494b07670/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-50.png)\n",
    "\n",
    "Итак, среднее значение вектора out равно 0.36, что и является предсказанием случайного леса.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/42662ce6e6df7bda5ff3de6115a67abf/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✍️ Давайте проверим на практике, что случайный лес улучшает предсказание случайного дерева. \n",
    "\n",
    "Для этого возьмём [датасет по решению задачи на прогрессирование диабета](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset). Мы будем предсказывать уровень прогрессирования болезни относительно базового уровня (некоторая численная мера, насколько «сильно» болен пациент диабетом). Минимальное значение составляет 25, максимальное — 346. Далее посмотрим на распределение, чтобы убедиться, что это не многоклассовая классификация, данные нормированы и закодированы категориальным кодировщиком. \n",
    "\n",
    "Качество будем измерять по среднему квадрату ошибки (MSE) на кросс-валидации с точностью до второго знака после запятой. \n",
    "\n",
    "Для чистоты эксперимента возьмём решающее дерево с глубиной 10 (DecisionTreeRegressor) и случайный лес из 10 деревьев (параметр n_estimators) с глубиной 10 (RandomForestRegression). Предлагается исполнить код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019907</td>\n",
       "      <td>-0.017646</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068332</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005670</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>-0.025930</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>-0.009362</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031988</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>135.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356   \n",
       "3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "\n",
       "         s4        s5        s6  target  \n",
       "0 -0.002592  0.019907 -0.017646   151.0  \n",
       "1 -0.039493 -0.068332 -0.092204    75.0  \n",
       "2 -0.002592  0.002861 -0.025930   141.0  \n",
       "3  0.034309  0.022688 -0.009362   206.0  \n",
       "4 -0.002592 -0.031988 -0.046641   135.0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = load_diabetes(as_frame=True)\n",
    "X = data['frame']\n",
    "y = data['target']\n",
    " \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размерность обучающей выборки (296, 11)\n",
      "Размерность тестовой выборки (146, 11)\n",
      "Качество предсказания по MSE для решающего дерева 4.06\n",
      "Качество предсказания по MSE для случайного леса  1.84\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)\n",
    " \n",
    "print(f'Размерность обучающей выборки {X_train.shape}')\n",
    "print(f'Размерность тестовой выборки {X_test.shape}')\n",
    "#Размерность обучающей выборки (296, 11)\n",
    "#Размерность тестовой выборки (146, 11)\n",
    " \n",
    "regr1 = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "regr1.fit(X_train, y_train)\n",
    "##DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    " \n",
    "regr2 = RandomForestRegressor(max_depth=10, n_estimators=10, random_state=42)\n",
    "regr2.fit(X_train, y_train)\n",
    "##RandomForestRegressor(max_depth=10, n_estimators=10, random_state=42)\n",
    " \n",
    "y_pred1 = regr1.predict(X_test)\n",
    "y_pred2 = regr2.predict(X_test)\n",
    " \n",
    "print(f'Качество предсказания по MSE для решающего дерева {round(mean_squared_error(y_test, y_pred1),2)}')\n",
    "print(f'Качество предсказания по MSE для случайного леса  {round(mean_squared_error(y_test, y_pred2),2)}')\n",
    "\n",
    "#вывод: Качество предсказания по MSE для решающего дерева 4.09\n",
    "#Качество предсказания по MSE для случайного леса  1.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⭐️ На примере случайного леса мы убедились, что качество базовой модели улучшают даже простейшие методы ансамблирования моделей, основанные на бутстрапе, такие как усреднение (Averaging) и голосование большинством (Majority Vote).\n",
    "\n",
    "Давайте разберёмся, почему ансамблирование моделей улучшает качество. Дело в том, что идея ансамблирования основана на уменьшении разброса предсказаний модели.  \n",
    "\n",
    "Доказано, что дисперсия ансамбля типа бэггинг (variance) в  раз меньше, чем смещение отдельной базовой модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Доказательство**\n",
    "\n",
    "Выберем из нашей выборки $X$ бутстрапом $K$ раз выборку длиной $N$. Получим выборки $X_{1}, X_{2}, ..., X_{K}$. Обучим базовые модели $a(x)$ на данных подвыборках. Первую модель $a_{1}(x) = a(x,X_{1})$ обучим на первой выборке бутстрапа, вторую $a_{2}(x) = a(x,X_{2})$ — на второй и так далее. Выполнив данную процедуру  раз, мы получим предсказание как усреднение по всем обученным на подвыборках моделях.\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/eeae6ae8e03e184ff94a533a020c1a73/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-58.png)\n",
    "\n",
    "Теперь рассмотрим изменение смещения (bias) и разброса (variance) ансамблирования по отношению к базовым моделям.\n",
    "\n",
    "> **Смещение** (bias) есть не что иное, как математическое ожидание разности между истинными ответами y и предсказаниями ансамбля: \n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/0010efcbd8914782f96e32442230250f/asset-v1:SkillFactory+DST-3.0\n",
    "+28FEB2021+type@asset+block/dst-3-ml-8-51.png)\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c742c46100695e837ba46566856184cb/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-52.png)\n",
    "\n",
    "Вывод: смещение ансамбля равно смещению базовой модели ансамбля!\n",
    "\n",
    "Разброс (variance, обозначим далее как ) — это дисперсия ответов алгоритма:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/3d239a00c5d9d71aec09651f109a150f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-53.png)\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/c704097ba958bf98bb20e2aa7bbac971/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-54.png)\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bc1b9ab29110bd4e40c3d2b1f947fbf1/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-55.png)\n",
    "\n",
    "При условии некоррелированности базовых моделей, которая достигается за счёт обучения на бустрапе, последнее слагаемое равно нулю. Итого:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/1781bfac94b0757870c894169fbe2e0b/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-56.png)\n",
    "\n",
    "Тогда, зная, что модели не коррелированы, получаем:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/7c8d37f4978d49117ffa159ebaf2c527/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-57.png)\n",
    "\n",
    "**Вывод**: разброс ансамбля уменьшается в  раз по сравнению с разбросом базовой модели!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта методика ансамблирования применима не только к одинаковым моделям (например к решающим деревьям), но и к любым другим моделям. Главное — чтобы базовые модели были максимально нескоррелированы. Такую методику часто используют в соревновательном Data Science на различных хакатонах и Kaggle-контестах. \n",
    "\n",
    "⭐️ В этом юните мы разобрали и научились применять на практике методы, лежащие в основе ансамблирования в случайном лесу. О более сложных методах поговорим в следующих юнитах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.6\n",
    "\n",
    "Одним из самых важных параметров, который непосредственно отвечает за переобучение и недообучение в деревьях, является глубина дерева. Предлагается используя код из модуля попытаться добиться неообучения для решающего дерева и случайного леса. Для этого:\n",
    "\n",
    "Используя код из модуля, поставьте глубину деревьев в решающем дереве и случайном лесу, равную 2.\n",
    "\n",
    "В качестве ответа приведите MSE решающего дерева и случайного леса (по модулю), округлённую до второго знака после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество предсказания по MSE для решающего дерева 397.47\n",
      "Качество предсказания по MSE для случайного леса  303.12\n"
     ]
    }
   ],
   "source": [
    "regr3 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "regr3.fit(X_train, y_train)\n",
    " \n",
    "regr4 = RandomForestRegressor(max_depth=2, n_estimators=10, random_state=42)\n",
    "regr4.fit(X_train, y_train)\n",
    " \n",
    "y_pred3 = regr3.predict(X_test)\n",
    "y_pred4 = regr4.predict(X_test)\n",
    " \n",
    "print(f'Качество предсказания по MSE для решающего дерева {round(mean_squared_error(y_test, y_pred3),2)}')\n",
    "print(f'Качество предсказания по MSE для случайного леса  {round(mean_squared_error(y_test, y_pred4),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.7\n",
    "\n",
    "Известно, что при большой глубине деревья склонны к переобучению. В данном задании предлагается, используя код и прошлого задания, попытаться искусственно добиться переобучения модели решающего дерева и случайного леса.\n",
    "\n",
    "Используя код из модуля, поставьте глубину деревьев в решающем дереве и случайном лесу, равную 1000.\n",
    "\n",
    "В качестве ответа приведите MSE решающего дерева и случайного леса (по модулю), округлённую до второго знака после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество предсказания по MSE для решающего дерева 4.06\n",
      "Качество предсказания по MSE для случайного леса 1.84\n"
     ]
    }
   ],
   "source": [
    "regr5 = DecisionTreeRegressor(max_depth=1000, random_state=42)\n",
    "regr5.fit(X_train, y_train)\n",
    " \n",
    "regr6 = RandomForestRegressor(max_depth=1000, n_estimators=10, random_state=42)\n",
    "regr6.fit(X_train, y_train)\n",
    " \n",
    "y_pred5 = regr5.predict(X_test)\n",
    "y_pred6 = regr6.predict(X_test)\n",
    " \n",
    "print(f'Качество предсказания по MSE для решающего дерева {round(mean_squared_error(y_test, y_pred5),2)}')\n",
    "print(f'Качество предсказания по MSE для случайного леса {round(mean_squared_error(y_test, y_pred6),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ансамблирование: блендинг и стекинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Стекинг** (stacking) — алгоритм построения ансамбля, в котором параллельно и независимо друг от друга обучаются несколько базовых моделей (необязательно одной природы), а их предсказания используются для обучения **метамодели** (финальная модель) как факторы.\n",
    "\n",
    "Предсказания базовых алгоритмов называются **метапризнаками**. \n",
    "\n",
    "### БЛЕНДИНГ\n",
    "\n",
    "Простейшая реализация стекинга заключается в блендинге (blending). \n",
    "\n",
    "Схематично блендинг можно представить следующим образом:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/e6928c858c286dde36c590f57d4d6b66/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-5.png)\n",
    "\n",
    "Суть блендинга состоит в следующем: предположим у нас есть обучающая выборка $X$, которую мы делим пополам. Первая часть используется для обучения базовых моделей, а на второй базовые модели делают предсказания – **метапризнаки**, на которых уже и обучается в дальнейшем метамодель. \n",
    "\n",
    "**Недостатки блендинга** видны невооруженным глазом: ни базовые модели, ни метамодель не обучаются на полных данных. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СТЕКИНГ\n",
    "\n",
    "Для решения этой проблемы используется усовершенствованная модель блендинга, которая имеет полноценное название — **стекинг**. Идея борьбы с недостатком блендинга — использование **кросс-валидации**.\n",
    "\n",
    "Рассмотрим как обучается классический стекинг. Пусть у нас есть таблица с примерами X и ответами на них y. Количество признаков — $m$, количество наблюдений — $n$, количество моделей в стекинге — $K$.\n",
    "\n",
    "1. Обучающая выборка разбивается на $L$ равных частей, называемых **фолдами**. Например, для трёх фолдов ($L=3$) схематично это будет выглядеть следующим образом:\n",
    "\n",
    "    ![](https://lms.skillfactory.ru/assets/courseware/v1/a8d9de09d8f6a07ac8b04d84fa837454/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_1.png)\n",
    "\n",
    "2. Затем для каждой базовой модели эти фолды перебираются следующим образом: на каждом шаге фиксируются $L-1$ фолдов для обучения базовых моделей и один фолд для предсказания (в случае бинарной классификации каждая модель предсказывает вероятность принадлежности к классу 1, в случае мультиклассовой классификации — к каждому классу). В результате будет сформировано $L$ предсказаний, из которых формируется метапризнак $M_j$, где $j$ — номер модели:\n",
    "\n",
    "    ![](https://lms.skillfactory.ru/assets/courseware/v1/c3a923e9a53df27fe9098d747eda6f1f/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_2.png)\n",
    "\n",
    "    Такой подход к формированию метапризнаков позволяет избежать переобучения. Действительно, можно рассматривать $L-1$-фолд как обучающую выборку, а оставшийся — как тестовую. Таким образом, мы обучаемся на тренировочной выборке, но предсказания делаем для той выборки, которую ещё не видели.\n",
    "\n",
    "3. После того как мы проделаем шаг 2 для всех базовых моделей, мы получим новый набор данных, состоящий из $K$ метапризнаков — предсказаний каждой из моделей. Предсказания моделей будут использоваться в качестве метапризнаков, на которых будет обучена метамодель.\n",
    "\n",
    "    Пусть мы взяли три разных модели, т. е. $K=3$. Это будет выглядеть следующим образом:\n",
    "\n",
    "    ![](https://lms.skillfactory.ru/assets/courseware/v1/b7b2bde8db3159532ee0042b395858a6/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Кроме метафакторов, метамодель может использовать для своего обучения изначальные признаки из исходного набора данных.\n",
    "\n",
    "Давайте посмотрим, как работает алгоритм на конкретной таблице. Пусть у нас есть некоторый набор данных из четырёх признаков, характеризующих клиента (x_0, x_1, x_2 и x_3), и восемь наблюдений. На основе этих признаков необходимо предсказать бинарный целевой признак (y) покупки товара со значениями 1 (купил) и 0 (не купил). Будем использовать стекинг, состоящий из трёх различных моделей.\n",
    "\n",
    "Разбиваем выборку на четыре фолда, то есть в каждом фолде будет по две строки таблицы (обозначены цветом). Обучаем каждую модель на трёх из этих фолдов и делаем предсказание вероятности покупки для оставшегося.\n",
    "\n",
    "Из предсказаний будет сформировано три метапризнака (по одному на каждую базовую модель). Это будут предсказанные базовыми классификаторами вероятности покупки (вероятность принадлежности к классу 1).\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/de1cba156b331d27e229eec3ec107819/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формируем новый набор данных и отправляем его в метамодель, которая уже и делает финальное предсказание целевого признака покупки:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/476d2d3365cad4c4b9cf2738ef982ca4/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_5.png)\n",
    "\n",
    "Метамодель будет производить поиск зависимостей в данных и принимать решение уже на основе предсказанных вероятностей покупки, которые были получены на первом этапе. \n",
    "\n",
    "В общем случае, когда у нас есть $K$ моделей, общая схема стекинга будет иметь вид:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/bbe06be5289a83d9300f7d073f4ad468/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-8_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важно понимать, стекинг — это чистая эвристика, эффективность которой доказана только практическим применением. Стекинг использует тот же подход, что и нейронные сети: предсказания предыдущего этапа (слоя) используются в качестве признаков для следующего этапа (слоя).\n",
    "\n",
    "С точки зрения смещения и разброса, стекинг не имеет прямой математической интерпретации, так как не минимизирует напрямую ни ту, ни другую компоненту ошибки. Удачно работающий стекинг просто уменьшает ошибку, но гарантий уменьшения смещения или разброса нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть некоторые **рекомендации, как правильно строить стекинг**:\n",
    "\n",
    "* В качестве метамоделей лучше всего применять простые модели: например, для задачи регрессии — линейную регрессию, а для задачи классификации — логистическую регрессию.\n",
    "* В качестве базовых моделей лучшего всего использовать модели различной природы.\n",
    "\n",
    "Из всех ансамблевых методов стекинг применяется реже всего. Главная причина: так как используется много разных моделей, необходимо подбирать их внешние параметры (коэффициенты регуляризации, глубина деревьев, число деревьев, темп обучения и т. д.) в совокупности, а подбор огромного количества параметров очень затратен по времени (мы убедились в этом в модуле по подбору внешних параметров моделей).\n",
    "\n",
    "Вторая причина — в отличие от бэггинга и бустинга, для стекинга нет каких-то готовых решений, таких как случайный лес и градиентный бустинг над деревьями. Базовые модели нужно подбирать самому, а какие из них подойдут лучше всего — открытый вопрос.\n",
    "\n",
    "Но, несмотря на эти недостатки, при грамотном подходе опытные специалисты выигрывают соревнования на Kaggle благодаря стекингу. Хотя зачастую таких участников называют «читерами» (от англ. cheat — «жульничать, обманывать»), ведь часто они собирают чуть ли не все возможные ML-модели в стекинг, запускают на мощном сервере подбор внешних параметров и комбинации из этих моделей в стекинге получают заветные 1.5 % прироста качества модели. На Kaggle даже существует фраза — «настекали».\n",
    "\n",
    "В реальных условиях такой прирост значит мало, поэтому мы не будем концентрироваться на стекинге в нашем курсе, но пример разберём."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## СТЕКИНГ В SKLEARN\n",
    "\n",
    "Стекинг для задачи регрессии имеет реализацию в библиотеке scikit-learn в классе StackingRegressor, для задачи классификации — в классе StackingClassifier. На вход подаётся список базовых моделей (атрибут estimators) и метамодель (атрибут final_estimator).\n",
    "\n",
    "**Примечание**. Стоит понимать, что для задачи регрессии все базовые модели должны быть регрессорами, а для задачи классификации — классификаторами.  \n",
    "\n",
    "Попробуем на практике применить стекинг, используя реализацию из библиотеки sklearn. В качестве входных данных будем использовать данные про диабет, использованные ранее. Обратимся снова к коду и обучим модель на данных.\n",
    "\n",
    "Как и все ансамбли, модель стекинга находится в модуле ensemble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные параметры StackingRegressor:\n",
    "\n",
    "* estimators — список из кортежей базовых моделей в виде (str, model). Первым элементом в каждом кортеже идет строка с именем модели, вторым — собственно сама модель.\n",
    "* final_estimator — метамодель.\n",
    "* cv — количество фолдов, на которые делится выборка. По умолчанию используется пять фолдов.\n",
    "\n",
    "Будем строить стекинг на следующих моделях:\n",
    "\n",
    "* 'dt' — дерево решений;\n",
    "* 'lr' — ридж-регрессия, линейная модель регрессии с L2-регуляризацией;\n",
    "* случайный лес с количеством деревьев, равным 10, в качестве метамодели.\n",
    "\n",
    "**Примечание**. В данном случае мы рассматриваем **RidgeCV**, которая представляет собой ридж-регрессию со встроенной кросс-валидацией по методу **Leave-One-Out Cross-Validation**. Подробнее читайте по [ссылке](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n",
    "\n",
    "Создадим список кортежей в формате (\"наименование модели\", модель) из этих моделей, и назовем его estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список кортежей вида: (наименование модели, модель)\n",
    "estimators = [\n",
    "    ('lr', RidgeCV()),\n",
    "    ('dt',  DecisionTreeRegressor(random_state=42))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда список из базовых моделей готов, создадим объект класса StackingRegressor. Первым аргументом передаём список из базовых моделей. Будем использовать в качестве метамодели модель случайного леса. Для этого передаём её в параметр final_estimator. Остальные параметры оставим по умолчанию.\n",
    "\n",
    "Обучаем модель с помощью метода fit(), делаем предсказание классов с помощью метода predict(), а затем считаем метрики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;lr&#x27;, RidgeCV()),\n",
       "                              (&#x27;dt&#x27;, DecisionTreeRegressor(random_state=42))],\n",
       "                  final_estimator=RandomForestRegressor(n_estimators=10,\n",
       "                                                        random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;lr&#x27;, RidgeCV()),\n",
       "                              (&#x27;dt&#x27;, DecisionTreeRegressor(random_state=42))],\n",
       "                  final_estimator=RandomForestRegressor(n_estimators=10,\n",
       "                                                        random_state=42))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=10, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('lr', RidgeCV()),\n",
       "                              ('dt', DecisionTreeRegressor(random_state=42))],\n",
       "                  final_estimator=RandomForestRegressor(n_estimators=10,\n",
       "                                                        random_state=42))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создаем объект класса стекинг\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RandomForestRegressor(n_estimators=10,\n",
    "                                          random_state=42)\n",
    ")\n",
    " \n",
    "#Обучаем модель\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество предсказания по MSE для стекинга 0.82\n"
     ]
    }
   ],
   "source": [
    "y_pred_stack = reg.predict(X_test)\n",
    "print(f'Качество предсказания по MSE для стекинга {round(mean_squared_error(y_test, y_pred_stack),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотреть на метапризнаки можно с помощью метода **transform()**. Для этого в метод нужно передать матрицу наблюдений X. В результате вызова метода для всех объектов каждая из трёх моделей сделает предсказание вероятностей и вернёт матрицу из двух столбцов. Оформим её в виде DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meta_feature1</th>\n",
       "      <th>meta_feature2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.000000</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192.000001</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.000002</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81.000007</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>122.000005</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   meta_feature1  meta_feature2\n",
       "0     154.000000          154.0\n",
       "1     192.000001          192.0\n",
       "2     116.000002          116.0\n",
       "3      81.000007           81.0\n",
       "4     122.000005          122.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = reg.transform(X_train)\n",
    "#Создаем DataFrame\n",
    "meta_df = pd.DataFrame(\n",
    "    meta_data, #содержимое таблицы\n",
    "    columns=['meta_feature1', 'meta_feature2',] #название столбцов\n",
    ")\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑ Наша таблица метапризнаков, на которой происходит обучение метамодели случайного леса. \n",
    "\n",
    "Примечание. При желании к метапризнакам можно добавить столбцы из изначального набора данных и попробовать обучить модель на этом наборе данных.\n",
    "\n",
    " ⬇️ Мы рассмотрели основные принципы работы стекинга и его реализацию в sklearn. Попробуйте применить стекинг для решения следующей задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.5\n",
    "\n",
    "Для выполнения задания используйте набор данных о диабете, который представлен в ноутбуке с примерами.\n",
    "\n",
    "Постройте стекинг из следующих базовых моделей:\n",
    "\n",
    "* Ридж-регрессия (RidgeCV());\n",
    "* Линейная регрессия.\n",
    "\n",
    "В качестве метамодели используйте случайный лес с количеством деревьев 100, максимальной глубиной 10, все параметры для базовых моделей стандартные. Для всех алгоритмов параметр random_state=42.\n",
    "\n",
    "Сделайте предсказание целевой метки для тестового набора данных. Рассчитайте метрику MSE для набора данных и запишите её в качестве ответа с точностью до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем список кортежей вида: (наименование модели, модель)\n",
    "estimators = [\n",
    "    ('lr', RidgeCV()),\n",
    "    ('dt',  LinearRegression())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;lr&#x27;, RidgeCV()), (&#x27;dt&#x27;, LinearRegression())],\n",
       "                  final_estimator=RandomForestRegressor(max_depth=10,\n",
       "                                                        random_state=42))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;lr&#x27;, RidgeCV()), (&#x27;dt&#x27;, LinearRegression())],\n",
       "                  final_estimator=RandomForestRegressor(max_depth=10,\n",
       "                                                        random_state=42))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>dt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('lr', RidgeCV()), ('dt', LinearRegression())],\n",
       "                  final_estimator=RandomForestRegressor(max_depth=10,\n",
       "                                                        random_state=42))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создаем объект класса стекинг\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RandomForestRegressor(n_estimators=100,\n",
    "                                          max_depth=10,\n",
    "                                          random_state=42)\n",
    ")\n",
    " \n",
    "#Обучаем модель\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество предсказания по MSE для стекинга 0.27\n"
     ]
    }
   ],
   "source": [
    "y_pred_stack = reg.predict(X_test)\n",
    "print(f'Качество предсказания по MSE для стекинга {round(mean_squared_error(y_test, y_pred_stack),2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Ансамблирование: бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ Последней реализацией ансамблирования, которую мы рассмотрим, станет бустинг. Этот метод отличается от предыдущих своей структурой. \n",
    "\n",
    "> **Бустинг (boosting)** — это алгоритм построения ансамбля, основанный на последовательном построении слабых моделей, причём каждая новая модель пытается уменьшить ошибку предыдущей. После того как все модели обучены, они объединяются в композицию.\n",
    "\n",
    "Примечание. Под слабыми моделями мы подразумеваем модели, точность которых немногим выше, чем случайное угадывание. Как правило, это короткие деревья решений, они обладают слабой предсказательной способностью.\n",
    "\n",
    "Обратите внимание, что в бустинге базовые модели обучаются последовательно, а не параллельно, как в предыдущих методах, исправляя ошибки своего «предшественника»  и повышая качество всего ансамбля. \n",
    "\n",
    "Бустинг основан на вопросе, поднятом исследователями [М. Кернсом](https://en.wikipedia.org/wiki/Michael_Kearns_(computer_scientist)) и [Л. Вэлиантом](https://ru.wikipedia.org/wiki/%D0%92%D1%8D%D0%BB%D0%B8%D0%B0%D0%BD%D1%82,_%D0%9B%D0%B5%D1%81%D0%BB%D0%B8): «Может ли набор слабых обучающих алгоритмов создать сильный обучающий алгоритм?»\n",
    "\n",
    "В отличие от бэггинга, бустинг обучается на одном и том же наборе данных, без генерации дополнительных выборок. Однако в процессе обучения меняются так называемые **веса наблюдений**. Если слабая модель допустила ошибку на каких-то примерах, то значимость (вес) этих примеров увеличивается и на них концентрируется следующая за ней модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представить алгоритм бустинга можно следующей схемой:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/f57e9e2fc278d940a8a1074e62b904f3/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-7_1.png)\n",
    "\n",
    "На схеме  представлено 11 разных наблюдений трёх различных классов (красные, зелёные и синие шарики). После того как модель делает предсказания, мы смотрим, на каких объектах мы угадали класс верно, а на каких ошиблись. Для тех объектов, на которых мы допустили ошибку, мы задаём больший вес. Вес наблюдения обозначается интенсивностью цвета. Чем больше вес наблюдения, тем ярче его цвет.\n",
    "\n",
    "Так же как и бэггинг, бустинг предназначен для обучения моделей одного типа. То есть нельзя последовательно обучить 50 логистических регрессий, а затем 50 деревьев решения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждая модель создаётся для того, чтобы найти ошибки предыдущей. Сами по себе они решают задачу плохо, но стоит объединить их усилия, и мы получим супермодель.\n",
    "\n",
    "Очень наглядно будет выглядеть модель бустинга для логистической регрессии при обучении на двух признаках:\n",
    "\n",
    "![](https://lms.skillfactory.ru/assets/courseware/v1/46bb0f960392c6a39fcf52d6da588054/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst3-ml3-7_2.png)\n",
    "\n",
    "На рисунке изображено расположение двух линейно неразделимых классов. Нельзя провести одну такую плоскость, которая идеально решает задачу классификации. Раз нельзя одну, давайте проведём две.\n",
    "\n",
    "Возьмём точки, для которых первая логистическая регрессия совершила ошибку. Увеличим вес этих точек (их значимость). Далее построим такую разделяющую плоскость, которая в первую очередь обращает внимание на наблюдения, имеющие наибольший вес."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, объединим две модели в одну — получим модель-композицию, которая идеально решает задачу классификации!\n",
    "\n",
    "**Примечание**. Когда все модели из ансамбля обучены и составлена композиция из них, для того, чтобы совершить предсказание на новом объекте, необходимо «прогнать» характеристики объекта через все модели в той же последовательности, в которой они обучались, и объединить их результат.\n",
    "\n",
    "Если бэггинг создавался с целью уменьшить разброс модели, то **цель бустинга** — уменьшить смещение модели.\n",
    "\n",
    "Каждый следующий базовый алгоритм в бустинге обучается так, чтобы уменьшить общую ошибку всех своих предшественников. Как следствие, итоговая композиция будет иметь меньшее смещение, чем каждый отдельный базовый алгоритм (хотя уменьшение разброса также может происходить).\n",
    "\n",
    "В предельном случае модель может обучиться так, что не будет допускать ошибок вовсе. Однако мы знаем, что это не всегда хорошо, ведь в таком случае модель может полностью подстроиться под обучающий набор данных и переобучиться.\n",
    "\n",
    "Поскольку основная цель бустинга — уменьшение смещения, в качестве базовых моделей часто выбирают **алгоритмы с высоким смещением и небольшим разбросом**, например короткие деревья решений. У каждого из таких деревьев слабая предсказательная способность, но если их объединить, мы получим очень мощную модель. \n",
    "\n",
    "В этом юните мы постараемся затронуть основные шаги эволюции бустинга от первой успешной модели до современных модификаций. Начнём рассмотрение с самой первой модели бустинга — **адаптивного бустинга**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## АДАПТИВНЫЙ БУСТИНГ \n",
    "\n",
    "Первая реализация бустинга называлась AdaBoost. Это модель, которая подразумевает воплощение той самой идеи взвешивания объектов, которую мы рассмотрели выше. Алгоритм предполагает постоянную модификацию объектов выборки путём их взвешивания, причём веса обновляются специальным образом: каждая новая модель из ансамбля обучается на взвешенных данных и обращает большее внимание на ошибки своих предшественников.\n",
    "\n",
    "Так как алгоритм является несовершенным и в дальнейшем получил свое развитие, мы не будем подробно останавливаться на его работе. Однако приведем краткое описание работы алгоритма на примере задачи **бинарной классификации**. \n",
    "\n",
    "Пусть у нас есть набор данных $X$, в котором $N$ объектов размерности $m$ (вектора в признаковом пространстве размера $M$) и метки класса $y \\in  {-1,1}$, где -1 и 1 — метки отрицательного и положительного класса соответственно.\n",
    "\n",
    "Будем строить ансамбль из  абстрактных базовых моделей — классификаторов. Обозначим их как  (это могут быть логистические регрессии/деревья решений или что-то ещё).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Перед обучением базовой модели мы инициализируем веса объектов обучающей выборки следующим образом: $w_j = \\frac{1}{N}, \\, j=1,2,\\dots,N$\n",
    "2. Для всех $i$ от $1$ до $K$:\n",
    "\n",
    "    1. Обучить базовую модель $a_{i}(x)$ с учётом весов объектов $w_{j}$.\n",
    "\n",
    "    2. Вычислить ошибку классификатора $a_{i}(x)$, обозначим её за :\n",
    "\n",
    "\n",
    "\n",
    "        Примечание. Здесь выражение $[y_{j} \\neq a_{i}(x_{j})]$ — это знакомая нам по модулю классификации индикаторная функция. Она равна 1, если ответ $y_i$ не совпал с предсказанием базовой модели $a_{i}(x_{j})$, и 0 — в противном случае.\n",
    "\n",
    "    3. Тогда вес предсказаний данного классификатора (мера «вклада» предсказаний -ой модели в общий ансамбль) вычисляется по формуле: $n_{i} = \\frac{1}{2}ln (\\frac{1 - e_{i}}{e_{i}})$\n",
    "\n",
    "        Формула веса становится нулевой только при **случайном угадывании**, то есть когда классификатор ошибается в половине меток — работает ровно так же, как и подбрасывание монетки. Однако остальные будут вносить вес в итоговую модель с положительным или отрицательным знаком (в этом легко убедиться, подставив вероятность ошибки, отличную от 0.5). Тем самым мы исключаем возможность вклада случайных классификаторов в результирующую модель.\n",
    "\n",
    "    4. Обновляем веса объектов в выборке. Для тех объектов, на которых мы допустили ошибку, вес увеличивается; для тех объектов, для которых наш ансамбль предсказал верный ответ — не изменяется. Формула обновления весов:\n",
    "\n",
    "        ![](https://lms.skillfactory.ru/assets/courseware/v1/8fd37ebf163d92d4f96f358e6b37a414/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-60.png)\n",
    "\n",
    "        Видно, что если классификация была произведена верно для объекта $x_j$, то степень экспоненты будет равна 0, а значит множитель $e^{-n_{j}[y_{j}\\neq a_{i}(x_{j}]}=1$ и вес $w_{j}$ не изменится. \n",
    "\n",
    "        Для того, чтобы привести все веса объектов к единому масштабу от 0 до 1, производится их нормировка путем деления каждого веса  на сумму всех весов:\n",
    "\n",
    "        ![](https://lms.skillfactory.ru/assets/courseware/v1/a5a60869b39154a53ff4da9ec785b530/asset-v1:SkillFactory+DST-3.0+28FEB2021+type@asset+block/dst-3-ml-8-61.png)\n",
    "\n",
    "    5. После обучения каждой базовой модели предсказания ансамбля строятся как сумма из предсказаний базовых моделей, взятых с весом $\\eta_{i}$:  \n",
    "        \n",
    "        $f(x) = sign (\\sum_{i = 1}^{N} \\eta_{i}a_{i}(x))$.\n",
    "\n",
    "        **Примечание**. Функция $sign$ — функция взятия знака, принимает значение -1, если аргумент функции отрицательный, 0 — если аргумент функции нулевой, и 1 — если аргумент функции положительный.\n",
    "\n",
    "        $sign(x) = \\left\\{\\begin{matrix} 1,x > 0 \\\\ 0, x = 0 \\\\ -1, x < 0 \\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В чём плюсы такого алгоритма?**\n",
    "\n",
    "✔️ Он прост. Обратите внимание: все математические операции — школьный курс математики, о высшей математике даже не идёт речи. Операции просты в реализации и не требуют вычисления производных, умножений матриц и прочих сложных математических конструкций.\n",
    "\n",
    "✔️ Накладные расходы бустинга минимальны. Время построения определяется временем построения базовых моделей.\n",
    "\n",
    "✔️ Показывает хорошую обобщающую способность.\n",
    "\n",
    "✔️ Имеет возможность идентификации шумовых объектов.\n",
    "\n",
    "**Но в чём минусы?**\n",
    "\n",
    "⛔️ Жадное добавление алгоритмов приводит к неоптимальности композиции.\n",
    "\n",
    "⛔️ Склонен к переобучению при наличии шума в данных.\n",
    "\n",
    "⛔️ Алгоритм является эвристикой, и «взвешивание» объектов, на котором он основан, не подкреплено математическим обоснованием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## АДАПТИВНЫЙ БУСТИНГ В SKLEARN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "950b5653ccfc34417735dd321d006fd482b31f7611416c3d8236dc5b17587d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
